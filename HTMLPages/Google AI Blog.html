<!doctype html>
<html class="v2 list-page" dir="ltr" itemscope itemtype="http://schema.org/Blog" lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:b="http://www.google.com/2005/gml/b" xmlns:data="http://www.google.com/2005/gml/data" xmlns:expr="http://www.google.com/2005/gml/expr"> 
 <head> 
  <link href="https://www.blogger.com/static/v1/widgets/1667664774-css_bundle_v2.css" rel="stylesheet" type="text/css"> 
  <title>
Google AI Blog
</title> 
  <meta content="width=device-width, height=device-height, minimum-scale=1.0, initial-scale=1.0, user-scalable=0" name="viewport"> 
  <meta content="IE=Edge" http-equiv="X-UA-Compatible"> 
  <meta content="Google AI Blog" property="og:title"> 
  <meta content="en_US" property="og:locale"> 
  <meta content="http://ai.googleblog.com/" property="og:url"> 
  <meta content="Google AI Blog" property="og:site_name"> <!-- Twitter Card properties --> 
  <meta content="Google AI Blog" property="og:title"> 
  <meta content="summary" name="twitter:card"> 
  <meta content="@googleai" name="twitter:creator"> 
  <link href="https://fonts.googleapis.com/css?family=Roboto:400italic,400,500,500italic,700,700italic" rel="stylesheet" type="text/css"> 
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet"> 
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js" type="text/javascript"></script> <!-- End --> 
  <style id="page-skin-1" type="text/css"><!--
/*
<Group description="Header Color" selector="header">
<Variable name="header.background.color" description="Header Background"
type="color" default="#ffffff"/>
</Group>
*/
.header-outer {
border-bottom: 1px solid #e0e0e0;
background: #ffffff;
}
html, .Label h2, #sidebar .rss a, .BlogArchive h2, .FollowByEmail h2.title, .widget .post h2 {
font-family: Roboto, sans-serif;
}
.plusfollowers h2.title, .post h2.title, .widget h2.title {
font-family: Roboto, sans-serif;
}
.widget-item-control {
height: 100%;
}
.widget.Header, #header {
position: relative;
height: 100%;
width: 100%;
}
}
.widget.Header .header-logo1 {
float: left;
margin-right: 15px;
padding-right: 15px;
border-right: 1px solid #ddd;
}
.header-title h1 {
color: rgba(39, 37, 37, 0.69);
display: inline-block;
font-size: 36px;
font-family: Roboto, sans-serif;
font-weight: normal;
line-height: 50px;
vertical-align: top;
margin-left: -23px;
}
.header-inner {
background-repeat: no-repeat;
background-position: right 0px;
}
.post-author,
.byline-author {
font-size: 14px;
font-weight: normal;
color: #757575;
color: rgba(0,0,0,.54);
}
.post-content .img-border {
border: 1px solid rgb(235, 235, 235);
padding: 4px;
}
.header-title a {
text-decoration: none !important;
}
pre {
border: 1px solid #bbbbbb;
margin-top: 1em 0 0 0;
padding: 0.99em;
overflow-x: auto;
overflow-y: auto;
}
pre, code {
font-size: 9pt;
background-color: #fafafa;
line-height: 125%;
font-family: monospace;
}
pre, code {
color: #060;
font: 13px/1.54 "courier new",courier,monospace;
}
.header-left .header-logo1 {
width: 128px !important;
}
.header-desc {
line-height: 20px;
margin-top: -22px;
margin-left: 17px;
}
.fb-custom img, .twitter-custom img, .gplus-share img {
cursor: pointer;
opacity: 0.54;
}
.fb-custom img:hover, .twitter-custom img:hover, .gplus-share img:hover {
opacity: 0.87;
}
.fb-like {
width: 80px;
}
.post .share {
float: right;
}
#twitter-share{
border: #CCC solid 1px;
border-radius: 3px;
background-image: -webkit-linear-gradient(top,#ffffff,#dedede);
}
.twitter-follow {
background: url(//3.bp.blogspot.com/-M7uPAxKEeh4/WKrvV1ujKCI/AAAAAAAATZE/cdHhTldtvk4q4ad1Me1XDIgQD9Aul09CACK4B/s1600/twitter-bird.png) no-repeat left center;
padding-left: 18px;
font: normal normal normal 11px/18px 'Helvetica Neue',Arial,sans-serif;
font-weight: bold;
text-shadow: 0 1px 0 rgba(255,255,255,.5);
cursor: pointer;
margin-bottom: 10px;
}
.twitter-fb {
padding-top: 2px;
}
.fb-follow-button  {
background: -webkit-linear-gradient(#4c69ba, #3b55a0);
background: -moz-linear-gradient(#4c69ba, #3b55a0);
background: linear-gradient(#4c69ba, #3b55a0);
border-radius: 2px;
height: 18px;
padding: 4px 0 0 3px;
width: 57px;
border: #4c69ba solid 1px;
}
.fb-follow-button a {
text-decoration: none !important;
text-shadow: 0 -1px 0 #354c8c;
text-align: center;
white-space: nowrap;
font-size: 11px;
color: white;
vertical-align: top;
}
.fb-follow-button a:visited {
color: white;
}
.fb-follow {
padding: 0px 5px 3px 0px;
width: 14px;
vertical-align: bottom;
}
.gplus-wrapper {
margin-top: 3px;
display: inline-block;
vertical-align: top;
}
.twitter-custom, .gplus-share {
margin-right: 12px;
}
.fb-follow-button{
margin: 10px auto;
}
sub, sup {
line-height: 0;
}
/** CUSTOM CODE **/
.post-content td {
width: inherit;
}
--></style> 
  <style id="template-skin-1" type="text/css"><!--
.header-outer {
clear: both;
}
.header-inner {
margin: auto;
padding: 0px;
}
.footer-outer {
background: #f5f5f5;
clear: both;
margin: 0;
}
.footer-inner {
margin: auto;
padding: 0px;
}
.footer-inner-2 {
/* Account for right hand column elasticity. */
max-width: calc(100% - 248px);
}
.google-footer-outer {
clear: both;
}
.cols-wrapper, .google-footer-outer, .footer-inner, .header-inner {
max-width: 978px;
margin-left: auto;
margin-right: auto;
}
.cols-wrapper {
margin: auto;
clear: both;
margin-top: 60px;
margin-bottom: 60px;
overflow: hidden;
}
.col-main-wrapper {
float: left;
width: 100%;
}
.col-main {
margin-right: 278px;
max-width: 660px;
}
.col-right {
float: right;
width: 248px;
margin-left: -278px;
}
/* Tweaks for layout mode. */
body#layout .google-footer-outer {
display: none;
}
body#layout .header-outer, body#layout .footer-outer {
background: none;
}
body#layout .header-inner {
height: initial;
}
body#layout .cols-wrapper {
margin-top: initial;
margin-bottom: initial;
}
--></style> <!-- start all head --> 
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> 
  <meta content="blogger" name="generator"> 
  <link href="//ai.googleblog.com/favicon.ico" rel="icon" type="image/x-icon"> 
  <link href="http://ai.googleblog.com/" rel="canonical"> 
  <link rel="alternate" type="application/atom+xml" title="Google AI Blog - Atom" href="http://ai.googleblog.com/feeds/posts/default"> 
  <link rel="alternate" type="application/rss+xml" title="Google AI Blog - RSS" href="http://ai.googleblog.com/feeds/posts/default?alt=rss"> 
  <link rel="service.post" type="application/atom+xml" title="Google AI Blog - Atom" href="https://www.blogger.com/feeds/8474926331452026626/posts/default"> <!--[if IE]><script type="text/javascript" src="https://www.blogger.com/static/v1/jsbin/403901366-ieretrofit.js"></script>
<![endif]--> 
  <meta content="http://ai.googleblog.com/" property="og:url"> 
  <meta content="Google AI Blog" property="og:title"> 
  <meta content="The latest from Google Research" property="og:description"> <!--[if IE]> <script> (function() { var html5 = ("abbr,article,aside,audio,canvas,datalist,details," + "figure,footer,header,hgroup,mark,menu,meter,nav,output," + "progress,section,time,video").split(','); for (var i = 0; i < html5.length; i++) { document.createElement(html5[i]); } try { document.execCommand('BackgroundImageCache', false, true); } catch(e) {} })(); </script> <![endif]--> <!-- end all head --> 
  <base target="_self"> 
  <style>
      html {
        font-family: Roboto, sans-serif;
        -moz-osx-font-smoothing: grayscale;
        -webkit-font-smoothing: antialiased;
      }
      body {
        padding: 0;
        /* This ensures that the scroll bar is always present, which is needed */
        /* because content render happens after page load; otherwise the header */
        /* would "bounce" in-between states. */
        min-height: 150%;
      }
      h2 {
        font-size: 16px;
      }
      h1, h2, h3, h4, h5 {
        line-height: 2em;
      }
      html, h4, h5, h6 {
        font-size: 14px;
      }
      a, a:visited {
        color: #4184F3;
        text-decoration: none;
      }
      a:focus, a:hover, a:active {
        text-decoration: none;
      }
      .Header {
        margin-top: 15px;
      }
      /*.Header h1 {
        font-size: 32px;
        font-weight: 300;
        line-height: 32px;
        height: 42px;
      }*/
      .header-inner .Header .titlewrapper {
        padding: 0;
        margin-top: 30px;
      }
      .header-inner .Header .descriptionwrapper {
        padding: 0;
        margin: 0;
      }
      .cols-wrapper {
        margin-top: 56px;
      }
      .header-outer, .cols-wrapper, .footer-outer, .google-footer-outer {
        padding: 0 60px;
      }
      .header-inner {
        height: 256px;
        position: relative;
      }
      html, .header-inner a {
        color: #212121;
        color: rgba(0,0,0,.87);
      }
      .header-inner .google-logo {
        display: inline-block;
        background-size: contain;
        z-index: 1;
        height: 100px;
        overflow: hidden;
        margin-top: -23px;
        margin-right: 8px;
      }
      .header-left {
        position: absolute;
        top: 50%;
        -webkit-transform: translateY(-50%);
        transform: translateY(-50%);
        margin-top: 2px;
        width: 100%;
        margin-left: -15px;
      }
      .google-logo {
        margin-left: -4px;
      }
      .google-logo img{
        height: 250px;
    	margin-top: -79px;
      }
      #google-footer {
        position: relative;
        font-size: 13px;
        list-style: none;
        text-align: right;
      }
      #google-footer a {
        color: #444;
      }
      #google-footer ul {
        margin: 0;
        padding: 0;
        height: 144px;
        line-height: 144px;
      }
      #google-footer ul li {
        display: inline;
      }
      #google-footer ul li:before {
        color: #999;
        content: "\00b7";
        font-weight: bold;
        margin: 5px;
      }
      #google-footer ul li:first-child:before {
        content: '';
      }
      #google-footer .google-logo-dark {
        left: 0;
        margin-top: -16px;
        position: absolute;
        top: 50%;
      }
      /** Sitemap links. **/
      .footer-inner-2 {
        font-size: 14px;
        padding-top: 42px;
        padding-bottom: 74px;
      }
      .footer-inner-2 .HTML h2 {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-size: 14px;
        font-weight: 500;
        padding-left: 0;
        margin: 10px 0;
      }
      .footer-inner-2 .HTML ul {
        font-weight: normal;
        list-style: none;
        padding-left: 0;
      }
      .footer-inner-2 .HTML li {
        line-height: 24px;
        padding: 0;
      }
      .footer-inner-2 li a {
        color: rgba(65,132,243,.87);
      }
      /** Archive widget. **/
      .BlogArchive {
        font-size: 13px;
        font-weight: normal;
      }
      .BlogArchive .widget-content {
        display: none;
      }
      .BlogArchive h2, .Label h2 {
        color: #4184F3;
        text-decoration: none;
      }
      .BlogArchive .hierarchy li {
        display: inline-block;
      }
      /* Specificity needed here to override widget CSS defaults. */
      .BlogArchive #ArchiveList ul li, .BlogArchive #ArchiveList ul ul li {
        margin: 0;
        padding-left: 0;
        text-indent: 0;
      }
      .BlogArchive .intervalToggle {
        cursor: pointer;
      }
      .BlogArchive .expanded .intervalToggle .new-toggle {
        -ms-transform: rotate(180deg);
        transform: rotate(180deg);
      }
      .BlogArchive .new-toggle {
        float: right;
        padding-top: 3px;
        opacity: 0.87;
      }
      #ArchiveList {
        text-transform: uppercase;
      }
      #ArchiveList .expanded > ul:last-child {
        margin-bottom: 16px;
      }
      #ArchiveList .archivedate {
        width: 100%;
      }
      /* Months */
      .BlogArchive .items {
        max-width: 150px;
        margin-left: -4px;
      }
      .BlogArchive .expanded .items {
        margin-bottom: 10px;
        overflow: hidden;
      }
      .BlogArchive .items > ul {
        float: left;
        height: 32px;
      }
      .BlogArchive .items a {
        padding: 0 4px;
      }
      .Label {
        font-size: 13px;
        font-weight: normal;
      }
      .sidebar-icon {
        display: inline-block;
        width: 24px;
        height: 24px;
        vertical-align: middle;
        margin-right: 12px;
        margin-top: -1px
      }
      .Label a {
        margin-right: 4px;
      }
      .Label .widget-content {
        display: none;
      }
      .FollowByEmail {
        font-size: 13px;
        font-weight: normal;
      }
      .FollowByEmail h2 {
        background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAALCAYAAACZIGYHAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAUBJREFUeNrMkSGLAlEUhb+ZB4JFi8mx2cz+ApvhRUGTcUCrNqNJDYIi+DO0GUwmQXDK2DSIoGgZcSaIjDrzwrK4ssvChj1w0733O+fdp+m6PozH4yQSCfb7Pa7r8pOi0SjJZBLP8zgej4gAIMvlMuPxmADIYrHger1+C6lUKmo+NJ/NZojb7SZDWiwWo1qtks1msW2bw+HwZdkwDHq9HvV6nel0SqvVYrvdIh6Ph3Qch+VyqRYLhQJSSjRNw7IsfN9XgGKxSLfbJZfL0e/3aTabrFYr7vc7IujLcOh8PqunrNdr0uk0pVKJVCpFJBJRgEajweVyod1uMxgM2O12BAGUgRbU8DV2JpOhVquRz+cRQii3+XxOp9NRN3jVR5LPOp1OjEYjlSL8hclkgmmabDabt4d+m+S30vkD/R/IU4ABAPTZgnZdmG/PAAAAAElFTkSuQmCC");
        background-repeat: no-repeat;
        background-position: 0 50%;
        text-indent: 30px;
      }
      .FollowByEmail .widget-content {
        display: none;
      }
      .searchBox input {
        border: 1px solid #eee;
        color: #212121;
        color: rgba(0,0,0,.87);
        font-size: 14px;
        padding: 8px 8px 8px 40px;
        width: 164px;
        font-family: Roboto, sans-serif;
        background: url("https://www.gstatic.com/images/icons/material/system/1x/search_grey600_24dp.png") 8px center no-repeat;
      }
      .searchBox ::-webkit-input-placeholder { /* WebKit, Blink, Edge */
        color:    rgba(0,0,0,.54);
      }
      .searchBox :-moz-placeholder { /* Mozilla Firefox 4 to 18 */
        color:    #000;
        opacity:  0.54;
      }
      .searchBox ::-moz-placeholder { /* Mozilla Firefox 19+ */
        color:    #000;
        opacity:  0.54;
      }
      .searchBox :-ms-input-placeholder { /* Internet Explorer 10-11 */
        color:    #757575;
      }
      .widget-item-control {
        margin-top: 0px;
      }
      .section {
        margin: 0;
        padding: 0;
      }
      #sidebar-top {
        border: 1px solid #eee;
      }
      #sidebar-top > div {
        margin: 16px 0;
      }
      .widget ul {
        line-height: 1.6;
      }
      /*main post*/
      .post {
		display: inline-block;
        margin-bottom:30px;
      }
      #main .post .title {
        margin: 0;
      }
      #main .post .title a {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-weight: normal;
        font-size: 24px;
      }
      #main .post .title a:hover {
        text-decoration:none;
        color:#4184F3;
      }
      .message,  #main .post .post-header {
        margin: 0;
        padding: 0;
      }
      #main .post .post-header .caption, #main .post .post-header .labels-caption,  #main .post .post-footer .caption, #main .post .post-footer .labels-caption {
        color: #444;
        font-weight: 500;
      }
      #main .tr-caption-container td {
        text-align: center;
      }
      #main .post .tr-caption {
        color: #757575;
        color: rgba(0,0,0,.54);
        display: block;
        padding-bottom: 20px;
		line-height: 1.5;
      }
      #main .post .tr-caption-container {
        line-height: 24px;
        padding: 4px 0;
        text-align: center;
      }
      #main .post .post-header .published{
        font-size:11px;
        font-weight:bold;
      }
      .post-header .publishdate {
        font-size: 17px;
        font-weight:normal;
        color: #757575;
        color: rgba(0,0,0,.54);
      }
      #main .post .post-footer{
        font-size:12px;
        padding-bottom: 21px;
      }
      .label-footer {
        margin-bottom: 12px;
        margin-top: 12px;
      }
      .comment-img {
        margin-right: 16px;
        opacity: 0.54;
        vertical-align: middle;
      }
      #main .post .post-header .published {
        margin-bottom: 10px;
        margin-top: -2px;
      }
      .post .post-content {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-size: 15px;
        margin: 5px 0 36px 0;
        line-height: 20px;
      }
      .post-body .post-content ul, .post-body .post-content ol {
        margin: 16px 0;
        padding: 0 48px;
      }
      .post-summary {
        display: none;
      }
      /* Another old-style caption. */
      .post-content div i, .post-content div + i {
        font-size: 14px;
        font-style: normal;
        color: #757575;
        color: rgba(0,0,0,.54);
        display: block;
        line-height: 24px;
        margin-bottom: 16px;
        text-align: left;
      }
      /* Another old-style caption (with link) */
      .post-content a > i {
        color: #4184F3 !important;
      }
      /* Old-style captions for images. */
      .post-content .separator + div:not(.separator) {
        margin-top: -16px;
      }
      /* Capture section headers. */
      .post-content br + br + b, .post-content .space + .space + b, .post-content .separator + b {
        display: inline-block;
      }
      /*.post-content li {
        line-height: 1.5;
      }*/
      /* Override all post images/videos to left align. */
      .post-content .separator, .post-content > div {
        text-align: center;
      }
      .post-content .separator > a, .post-content .separator > span {
        margin-left: 0 !important;
      }
      .post-content img {
        max-width: 100%;
        height: auto;
        width: auto;
      }
      .post-content .tr-caption-container img {
        margin-bottom: 12px;
      }
      .post-content iframe, .post-content embed {
        max-width: 100%;
      }
      .post-content .carousel-container {
        margin-bottom: 48px;
      }
      #main .post-content b {
        font-weight: 500;
      }
      /* These are the main paragraph spacing tweaks. */
      #main .post-content br {
        /*content: ' ';*/
        display: block;
        padding: 4px;
      }
      .post-content .space {
        display: block;
        height: 8px;
      }
      .post-content iframe + .space, .post-content iframe + br {
        padding: 0 !important;
      }
      #main .post .jump-link {
        margin-bottom:10px;
      }
      .post-content img, .post-content iframe {
        margin: 15px 0 20px 0;
      }
      .post-content > img:first-child, .post-content > iframe:first-child {
        margin-top: 0;
      }
      .col-right .section {
        padding: 0 16px;
      }
      #aside {
        background:#fff;
        border:1px solid #eee;
        border-top: 0;
      }
      #aside .widget {
        margin:0;
      }
      #aside .widget h2, #ArchiveList .toggle + a.post-count-link {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-weight: 400 !important;
        margin: 0;
      }
      #ArchiveList .toggle {
        float: right;
      }
      #ArchiveList .toggle .material-icons {
        padding-top: 4px;
      }
      #sidebar .tab {
        cursor: pointer;
      }
      #sidebar .tab .arrow {
        display: inline-block;
        float: right;
      }
      #sidebar .tab .icon {
        display: inline-block;
        vertical-align: top;
        height: 24px;
        width: 24px;
        margin-right: 13px;
        margin-left: -1px;
        margin-top: 1px;
        color: #757575;
        color: rgba(0,0,0,.54);
      }
      #sidebar .widget-content > :first-child {
        padding-top: 8px;
      }
      #sidebar .active .tab .arrow {
        -ms-transform: rotate(180deg);
        transform: rotate(180deg);
      }
      #sidebar .arrow {
        color: #757575;
        color: rgba(0,0,0,.54);
      }
      #sidebar .widget h2 {
        font-size: 14px;
        line-height: 24px;
        display: inline-block;
      }
      #sidebar .widget .BlogArchive {
        padding-bottom: 8px;
      }
      #sidebar .widget {
        border-bottom: 1px solid #eee;
        box-shadow: 0px 1px 0 white;
        margin-bottom: 0;
        padding: 14px 0;
        min-height: 20px;
      }
      #sidebar .widget:last-child {
        border-bottom: none;
        box-shadow: none;
        margin-bottom: 0;
      }
      #sidebar ul {
        margin: 0;
        padding: 0;
      }
      #sidebar ul li {
        list-style:none;
        padding:0;
      }
      #sidebar ul li a {
        line-height: 32px;
      }
      #sidebar .archive {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAYCAYAAADzoH0MAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAE1JREFUeNpiNDY23s9AAWBioBCwYBM8c+YMVsUmJibEGYBNMS5DaeMFfDYSZQA2v9I3FrB5AZeriI4FmnrBccCT8mhmGs1MwyAzAQQYAKEWG9zm9QFEAAAAAElFTkSuQmCC");
        height: 24px;
        line-height: 24px;
        padding-left: 30px;
      }
      #sidebar .labels {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAARCAYAAAA7bUf6AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAUxJREFUeNpiNDY23s9AAMycOfM7UF05kHkZmzwTMkdSUhKrIcXFxZy3bt3qBjIN8RrS09PDsHnzZjCNDr58+cKQlpbGDjSoHcg1w2oIyAUODg5gARCNzUVIBrUCuVYYhjx//pzhwIEDYAEQDeJjA1CDWIAGNQK59jBxRuSABbkAlwHIgIeHh2HWrFn/1NTU2oDcvSgBS4wBSC5iArqoCsj1YGIgEyAZVMoEchqlBjEB/cZAiUHg2AEGznpKDAImxOeM////B4VLKtBvEUCngZ1ILKivr3/u6+ubBzJAGZQ9gC5aQoqLgAY8BhkAZL4BuQQkxgXE34A4BuiiZEIuAhrwEGhAEZD5DpzYoIaA2UAM4kQADUrHZRDUgAIg8wO2XAwzbQXQa5OweQ1owB10AyA6gS7BgX1u3ry5397eHow3bdo0EyjGi00tQIABANPgyAH1q1eaAAAAAElFTkSuQmCC");
        height: 20px;
        line-height: 20px;
        padding-left: 30px;
      }
      #sidebar .rss a {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAASCAYAAABWzo5XAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAX5JREFUeNqsVDGSgkAQHL2rIiIikohIc/EBRkbwAIwuwgfwAXiAD9AHSI7kEkECRCb6AIyINDLx7K0aa6kT7uq0q7YYtnZ7umdnt7darXbr9Zpegeu61DNNc0dvwCcH4/GYJpMJnc9nOhwOVJbl/4hAAokMECZJQtvt9k+kH7qufyEYDAakqqqYxFdRFBqNRmTbNg2HQ0rTlK7XayvR0xqBdDqdkuM4dE/0ULhYLOh4PHYrknG5XGi/31MYhuL/nkwonM1mlGUZ1XXdrsiyLGEDhY7juJEZ1u5tIixDGdYhmYw+B7CAzPP5nDabjdgIAgCksMX1832/3drtdqPT6SQWapomiGEFNkDEdpDMMAzK81ys/7XYy+XyoQgq2WoURSIJ2iIIgp/WZCCTvFm2wgeAU31aI3Q2GhIDMeB53qPYPIcm5VrxXIOIOxsDMStjVawAc1VViRgN22lNBiuQN3GR+SY07hpOoStmFQAKXRRFY93bnpG+fONfedi+BRgAbkS8Fxp7QQIAAAAASUVORK5CYII=");
      }
      #sidebar .subscription a {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAALCAYAAACZIGYHAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAUBJREFUeNrMkSGLAlEUhb+ZB4JFi8mx2cz+ApvhRUGTcUCrNqNJDYIi+DO0GUwmQXDK2DSIoGgZcSaIjDrzwrK4ssvChj1w0733O+fdp+m6PozH4yQSCfb7Pa7r8pOi0SjJZBLP8zgej4gAIMvlMuPxmADIYrHger1+C6lUKmo+NJ/NZojb7SZDWiwWo1qtks1msW2bw+HwZdkwDHq9HvV6nel0SqvVYrvdIh6Ph3Qch+VyqRYLhQJSSjRNw7IsfN9XgGKxSLfbJZfL0e/3aTabrFYr7vc7IujLcOh8PqunrNdr0uk0pVKJVCpFJBJRgEajweVyod1uMxgM2O12BAGUgRbU8DV2JpOhVquRz+cRQii3+XxOp9NRN3jVR5LPOp1OjEYjlSL8hclkgmmabDabt4d+m+S30vkD/R/IU4ABAPTZgnZdmG/PAAAAAElFTkSuQmCC");
      }
      #sidebar-bottom {
        background: #f5f5f5;
        border-top:1px solid #eee;
      }
      #sidebar-bottom .widget {
        border-bottom: 1px solid #e0e0e0;
        padding: 15px 0;
        text-align: center;
      }
      #sidebar-bottom > div:last-child {
        border-bottom: 0;
      }
      #sidebar-bottom .text {
        line-height: 20px;
      }
      /* Home, forward, and backward pagination. */
      .blog-pager {
        border-top : 1px #e0e0e0 solid;
        padding-top: 10px;
        margin-top: 15px;
        text-align: right !important;
      }
      #blog-pager {
        margin-botom: 0;
        margin-top: -14px;
        padding: 16px 0 0 0;
      }
      #blog-pager a {
        display: inline-block;
      }
      .blog-pager i.disabled {
        opacity: 0.2 !important;
      }
      .blog-pager i {
        color: black;
        margin-left: 16px;
        opacity: 0.54;
      }
      .blog-pager i:hover, .blog-pager i:active {
        opacity: 0.87;
      }
      #blog-pager-older-link, #blog-pager-newer-link {
        float: none;
      }
      .gplus-profile {
        background-color: #fafafa;
        border: 1px solid #eee;
        overflow: hidden;
        width: 212px;
      }
      .gplus-profile-inner {
        margin-left: -1px;
        margin-top: -1px;
      }
      /* Sidebar follow buttons. */
      .followgooglewrapper {
        padding: 12px 0 0 0;
      }
      .loading {
        visibility: hidden;
      }
      .detail-page .post-footer .cmt_iframe_holder {
        padding-top: 40px !important;
      }
      /** Desktop **/
      @media (max-width: 900px) {
        .col-right {
          display: none;
        }
        .col-main {
          margin-right: 0;
          min-width: initial;
        }
        .footer-outer {
          display: none;
        }
        .cols-wrapper {
          min-width: initial;
        }
        .google-footer-outer {
          background-color: #f5f5f5;
        }
      }
      /** Tablet **/
      @media (max-width: 712px) {
        .header-outer, .cols-wrapper, .footer-outer, .google-footer-outer {
          padding: 0 40px;
        }
      }
      /* An extra breakpoint accommodating for long blog titles. */
      @media (max-width: 600px) {
        .header-left {
          height: 100%;
          top: inherit;
          margin-top: 0;
          -webkit-transform: initial;
          transform: initial;
        }
        .header-title {
          margin-top: 88px;
        }
        .header-inner .google-logo {
          height: 40px;
          margin-top: 3px;
        }
        .header-inner .google-logo img {
          height: 200px;
        }
        .header-title h2 {
          font-size: 32px;
          line-height: 40px;
        }
		.header-title h1{
          font-size: 32px;
          line-height: 50px;
		}
        .header-desc {
          bottom: 94px;
          position: absolute;
          margin-left: 14px;
        }
      }
      /** Mobile/small desktop window; also landscape. **/
      @media (max-width: 480px), (max-height: 480px) {
        .header-outer, .cols-wrapper, .footer-outer, .google-footer-outer {
          padding: 0 16px;
        }
        .cols-wrapper {
          margin-top: 0;
        }
        .post-header .publishdate, .post .post-content {
          font-size: 16px;
        }
        .post .post-content {
          line-height: 28px;
          margin-bottom: 30px;
        }
        .post {
          margin-top: 30px;
        }
        .byline-author {
          display: block;
          font-size: 12px;
          line-height: 24px;
          margin-top: 6px;
        }
        #main .post .title a {
          font-weight: 500;
          color: #4c4c4c;
          color: rgba(0,0,0,.70);
        }
        #main .post .post-header {
          padding-bottom: 12px;
        }
        #main .post .post-header .published {
          margin-bottom: -8px;
          margin-top: 3px;
        }
        .post .read-more {
          display: block;
          margin-top: 14px;
        }
        .post .tr-caption {
          font-size: 12px;
        }
        #main .post .title a {
          font-size: 20px;
          line-height: 30px;
        }
        .post-content iframe {
          /* iframe won't keep aspect ratio when scaled down. */
          max-height: 240px;
        }
        .post-content .separator img, .post-content .tr-caption-container img, .post-content iframe {
          max-width: inherit;
          width: calc(100% + 32px);
        }
        /*.post-content table, .post-content td {
          width: 100%;
        }*/
        #blog-pager {
          margin: 0;
          padding: 16px 0;
        }
        /** List page tweaks. **/
        .list-page .post-original {
          display: none;
        }
        .list-page .post-summary {
          display: block;
        }
        .list-page .comment-container {
          display: none;
        }
        .list-page #blog-pager {
          padding-top: 0;
          border: 0;
          margin-top: -8px;
        }
        .list-page .label-footer {
          display: none;
        }
        .list-page #main .post .post-footer {
          border-bottom: 1px solid #eee;
          margin: -16px 0 0 0;
          padding: 0 0 20px 0;
        }
        .list-page .post .share {
          display: none;
        }
        /** Detail page tweaks. **/
        .detail-page .post-footer .cmt_iframe_holder {
          padding-top: 32px !important;
        }
        .detail-page .label-footer {
          margin-bottom: 0;
        }
        .detail-page #main .post .post-footer {
          padding-bottom: 0;
        }
        .detail-page #comments {
          display: none;
        }
      }
      [data-about-pullquote], [data-is-preview], [data-about-syndication] {
        display: none;
      }
    </style> 
  <noscript> 
   <style>
        .loading { visibility: visible }</style> 
  </noscript> 
  <script type="text/javascript">
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-961555-69', 'auto', 'blogger');
      ga('blogger.send', 'pageview');
    </script> 
  <link href="https://www.blogger.com/dyn-css/authorization.css?targetBlogID=8474926331452026626&amp;zx=534f5331-5d86-4749-b6e6-6fed4a9badbf" media="none" onload="if(media!='all')media='all'" rel="stylesheet">
  <noscript>
   <link href="https://www.blogger.com/dyn-css/authorization.css?targetBlogID=8474926331452026626&amp;zx=534f5331-5d86-4749-b6e6-6fed4a9badbf" rel="stylesheet">
  </noscript> 
  <meta name="google-adsense-platform-account" content="ca-host-pub-1556223355139109"> 
  <meta name="google-adsense-platform-domain" content="blogspot.com"> 
 </head> 
 <body> 
  <script type="text/javascript">
      //<![CDATA[
      var axel = Math.random() + "";
      var a = axel * 10000000000000;
      document.write('<iframe src="https://2542116.fls.doubleclick.net/activityi;src=2542116;type=gblog;cat=googl0;ord=ord=' + a + '?" width="1" height="1" frameborder="0" style="display:none"></iframe>');
      //]]>
    </script> 
  <noscript> 
   <img alt="" height="1" src="https://ad.doubleclick.net/ddm/activity/src=2542116;type=gblog;cat=googl0;ord=1?" width="1"> 
  </noscript> <!-- Header --> 
  <div class="header-outer"> 
   <div class="header-inner"> 
    <div class="section" id="header">
     <div class="widget Header" data-version="1" id="Header1"> 
      <div class="header-left"> 
       <div class="header-title"> <a class="google-logo" href="http://ai.googleblog.com/"> <img height="100" src="//2.bp.blogspot.com/-qRz-hnwUdY4/WulXSQ6Rv4I/AAAAAAAATvQ/shk7KsphA0c3E3nUMsDVASqYaH0PhLPNwCK4BGAYYCw/s1600/GoogleAI_logo_horizontal_color_rgb.png"> </a> <a href="/."> <h1> Blog </h1> </a> 
       </div> 
       <div class="header-desc">
         The latest from Google Research 
       </div> 
      </div> 
     </div>
    </div> 
   </div> 
  </div> <!-- all content wrapper start --> 
  <div class="cols-wrapper loading"> 
   <div class="col-main-wrapper"> 
    <div class="col-main"> 
     <div class="section" id="main">
      <div class="widget Blog" data-version="1" id="Blog1"> 
       <div class="post" data-id="6359395602733952589" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/11/permutation-invariant-neural-networks.html" itemprop="url" title="Permutation-Invariant Neural Networks for Reinforcement Learning"> Permutation-Invariant Neural Networks for Reinforcement Learning </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Thursday, November 18, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Posted by David Ha, Staff Research Scientist and Yujin Tang, Research Software Engineer, Google Research, Tokyo</span>
  
  <img src="https://blogger.googleusercontent.com/img/a/AVvXsEhG9U9ZdT3YvEGaVkOO15yWOXly9pwDWHafSv2yqd56jYi77LK_uEsRI13NikY7VfA2-DN1WAC7thrgufs0Ew0YHuZoYvkGw3VqcJXJQVt7jfmDFw8i4P6cBT7xH3cF2djo14Ph4BnFvJ9ZP5VDNubXd2GKsgSy1mnkFio214-rHOXk3xVdlly8dz4R8Q=s460" style="display: none;" />

  
<p>
<i>&#8220;The brain is able to use information coming from the skin as if it were coming from the eyes. We don&#8217;t see with the eyes or hear with the ears, these are just the receptors, seeing and hearing in fact goes on in the brain.&#8221;</i>
</p><p style="text-align: right;">
- <a href="https://en.wikipedia.org/wiki/Paul_Bach-y-Rita">Paul Bach-y-Rita</a><a href="#1" name="top1"><span class="Apple-style-span" style="font-size: small;"><sup><small>1</small></sup></span></a>
</p>
  
<p>
People have the amazing ability to use one sensory modality (e.g., touch) to supply environmental information normally gathered by another sense (e.g., vision). This adaptive ability, called <a href="https://en.wikipedia.org/wiki/Sensory_substitution">sensory substitution</a>, is a phenomenon well-known to neuroscience. While difficult adaptations &#8212; such as adjusting to seeing things <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010945217301314">upside-down</a>, learning to ride a <a href="https://ed.ted.com/best_of_web/bf2mRAfC">&#8220;backwards&#8221; bicycle</a>, or learning to &#8220;see&#8221; by interpreting visual information emitted from a grid of electrodes placed on one&#8217;s tongue &#8212; require anywhere from weeks, months or even years to attain mastery, people are able to eventually adjust to sensory substitutions.
</p>

<table align="center" cellpadding="0" cellspacing="4" class="tr-caption-container">
  <tbody><tr>
    <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEifk5jKD0BQctRKuEgUtcLjvH1IYPAtB20iO5FtuIKIR3jS1lLfvcySaXp6kGoWEHPZRi2QwQzs8VRPCK8gYBUTJmz9V2cwrFoMPFPbq4cqg5GT6LIbmGI7VzF2ivOGTPwNLXpyF86GuRL8mgqcafCjatn-y9Gv0FRG9XL-r9b5pZJ5cW2xb_AQ4eeiPA=s999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="605" data-original-width="999" height="388" src="https://blogger.googleusercontent.com/img/a/AVvXsEifk5jKD0BQctRKuEgUtcLjvH1IYPAtB20iO5FtuIKIR3jS1lLfvcySaXp6kGoWEHPZRi2QwQzs8VRPCK8gYBUTJmz9V2cwrFoMPFPbq4cqg5GT6LIbmGI7VzF2ivOGTPwNLXpyF86GuRL8mgqcafCjatn-y9Gv0FRG9XL-r9b5pZJ5cW2xb_AQ4eeiPA=w640-h388" width="640" /></a></td>
  <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEg7JNrWWO4-Odvsg3i_EM58Muc2Lh0Psy39zK7UHKB6l6BrBE7irh5ijNWtf8jJ19BiYKBP63IT8aGKBNQARHnepVphl_zmSFYiui8FX9ybrky267SqWX2zJ8AaPMt0MX2OYPZ2OvEDFS2Ky1qtub51kInh7NtP4HDKcwMp40faYuOwqkgWGMfQjBuYEQ=s379" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="379" data-original-width="321" height="400" src="https://blogger.googleusercontent.com/img/a/AVvXsEg7JNrWWO4-Odvsg3i_EM58Muc2Lh0Psy39zK7UHKB6l6BrBE7irh5ijNWtf8jJ19BiYKBP63IT8aGKBNQARHnepVphl_zmSFYiui8FX9ybrky267SqWX2zJ8AaPMt0MX2OYPZ2OvEDFS2Ky1qtub51kInh7NtP4HDKcwMp40faYuOwqkgWGMfQjBuYEQ=w339-h400" width="339" /></a></td></tr><!--<tr><td class="tr-caption" style="text-align: center;"></td>
    </tr>-->
  </tbody></table>
<table align="center" cellpadding="-4" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;">Examples of Sensory Substitution. <b>Left</b>: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0006899301026671">Tongue Display Unit</a> (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0006899301026671">Maris and Bach-y-Rita, 2001</a>; Image: <a href="https://www.sciencedirect.com/science/article/pii/S1026309811001702#f000020">Kaczmarek, 2011</a>). <b>Right</b>: &#8220;Upside down goggles&#8221; initially conceived by Erismann and Kohler in 1931. (Image <a href="https://en.wikipedia.org/wiki/Upside_down_goggles">Wikipedia</a>).</td></tr></tbody></table>

<p>
In contrast, most neural networks are not able to adapt to sensory substitutions <em>at all</em>. For instance, most <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> (RL) agents require their inputs to be in a pre-specified format, or else they will fail. They expect fixed-size inputs and assume that each element of the input carries a precise meaning, such as the pixel intensity at a specified location, or state information, like position or velocity. In popular RL benchmark tasks (e.g., <a href="https://pybullet.org/">Ant</a> or <a href="https://github.com/google/brain-tokyo-workshop/tree/master/learntopredict/cartpole">Cart-pole</a>), an agent trained using current <a href="https://github.com/DLR-RM/stable-baselines3">RL algorithms</a> will fail if its sensory inputs are changed or if the agent is fed additional noisy inputs that are unrelated to the task at hand. 
</p>
  
<p>
In &#8220;<a href="https://attentionneuron.github.io/">The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning</a>&#8221;, a <a href="https://arxiv.org/abs/2109.02869">spotlight paper</a> at<a href="https://neurips.cc/"> NeurIPS 2021</a>, we explore <em>permutation invariant</em> neural network agents, which require each of their sensory neurons (receptors that receive sensory inputs from the environment) to figure out the meaning and context of its input signal, rather than explicitly assuming a fixed meaning. Our experiments show that such agents are robust to observations that contain additional redundant or noisy information, and to observations that are corrupt and incomplete.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container">
  <tbody><tr>
    <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEix_fWzEojotm-HAlw6PCMx9tcRH3rWLWLavBgoRegcHUUr-UfZwkNWvP47m4LQiHob1vVAYLEUBts4C3VQyW6413I_8dFhy0XQZROieavtB63eYAE4Ki7vBwiuBzDQdc78MWjxR76tdj5r_CBGZphsJfUPoUiXD1sO5hZ_NGl7hY8SPhZXjv43oCethw=s480" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="320" data-original-width="480" height="426" src="https://blogger.googleusercontent.com/img/a/AVvXsEix_fWzEojotm-HAlw6PCMx9tcRH3rWLWLavBgoRegcHUUr-UfZwkNWvP47m4LQiHob1vVAYLEUBts4C3VQyW6413I_8dFhy0XQZROieavtB63eYAE4Ki7vBwiuBzDQdc78MWjxR76tdj5r_CBGZphsJfUPoUiXD1sO5hZ_NGl7hY8SPhZXjv43oCethw=w640-h426" width="640" /></a></td>
    <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiFvxoniZCUitKtPZMgWX9Bj5ytMl-mvthB6sR7hw3iBvf6myLGNBEiKoi_MjV2zC5kiwWEte0_LhZzhEYyJzyS7DPj_EnnSbw55_KcJQwleavrs2poXEyFA5A_n0VKuFWLlV08XFLSpiQxaoI3IT80jfzYOHvAorXfF5ApV8IBJud2SXkTnO12gb9X9g=s772" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="772" data-original-width="720" height="400" src="https://blogger.googleusercontent.com/img/a/AVvXsEiFvxoniZCUitKtPZMgWX9Bj5ytMl-mvthB6sR7hw3iBvf6myLGNBEiKoi_MjV2zC5kiwWEte0_LhZzhEYyJzyS7DPj_EnnSbw55_KcJQwleavrs2poXEyFA5A_n0VKuFWLlV08XFLSpiQxaoI3IT80jfzYOHvAorXfF5ApV8IBJud2SXkTnO12gb9X9g=w373-h400" width="373" /></a></td>
    </tr>
  </tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;"> <em>Permutation invariant</em> reinforcement learning agents adapting to sensory substitutions. <b>Left</b>: The ordering of the ant&#8217;s 28 observations are randomly shuffled every 200 time-steps. Unlike the standard policy, our policy is not affected by the suddenly permuted inputs. <b>Right</b>: Cart-pole agent given many redundant noisy inputs (Interactive <a href="https://attentionneuron.github.io/#cartpole_demo_special">web-demo</a>).</td></tr></tbody></table>


<p>
In addition to adapting to sensory substitutions in state-observation environments (like the ant and cart-pole examples), we show that these agents can also adapt to sensory substitutions in complex visual-observation environments (such as a <a href="https://gym.openai.com/envs/CarRacing-v0/">CarRacing</a> game that uses only pixel observations) and can perform when the stream of input images is constantly being reshuffled:
</p>


<table align="center" cellpadding="0" cellspacing="4" class="tr-caption-container">
  <tbody><tr>
    <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgIJk9mVzUhwo7aFu2u4rpQpVOaHClC8IKhoTAA0GJG_2PdKFBg3ZJN5Tf7wMgMD4hI08benjmbKYsfnPuEAo3pjqmjgJbSHl2eMgYknfGFuwEcNTPzuelQr_LYMct6MP23RKt62AqdjIEDLqbH1tOX6r3rjBn-fnmrSmSpjLHhNGyQu0QPN1OBABT1FA=s320" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="160" data-original-width="320" height="320" src="https://blogger.googleusercontent.com/img/a/AVvXsEgIJk9mVzUhwo7aFu2u4rpQpVOaHClC8IKhoTAA0GJG_2PdKFBg3ZJN5Tf7wMgMD4hI08benjmbKYsfnPuEAo3pjqmjgJbSHl2eMgYknfGFuwEcNTPzuelQr_LYMct6MP23RKt62AqdjIEDLqbH1tOX6r3rjBn-fnmrSmSpjLHhNGyQu0QPN1OBABT1FA=w640-h320" width="640" /></a></td>
  <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgtub_tV-ycwMMyvpfPeaUAJ2QpcKzpOuGKuGwBw1z_hYXpF49jDvJGcq6VhV0vJgr-rGFx_YhdXF6BiYTidTo0q_ok_6bcy6vkU-9FLf3G80BVKGa7R3R8HzTeZCHGmIyWC2Gj4LWDLwaHcAEKZ1ykoHqVvfkeTvDQmdA8c_WDJ5vPJmoK5LoWp9maYQ=s320" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="160" data-original-width="320" height="320" src="https://blogger.googleusercontent.com/img/a/AVvXsEgtub_tV-ycwMMyvpfPeaUAJ2QpcKzpOuGKuGwBw1z_hYXpF49jDvJGcq6VhV0vJgr-rGFx_YhdXF6BiYTidTo0q_ok_6bcy6vkU-9FLf3G80BVKGa7R3R8HzTeZCHGmIyWC2Gj4LWDLwaHcAEKZ1ykoHqVvfkeTvDQmdA8c_WDJ5vPJmoK5LoWp9maYQ=w640-h320" width="640" /></a></td>
        </tr>
  </tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;">We partition the visual input from CarRacing into a 2D grid of small patches, and shuffled their ordering. Without any additional training, our agent still performs even when the original training background (<b>left</b>) is replaced with new images (<b>right</b>).</td></tr></tbody></table>

<p>
<b>Method</b>
<br />
Our approach takes observations from the environment at each time-step and feeds each element of the observation into distinct, but identical neural networks (called &#8220;sensory neurons&#8221;), each with no fixed relationship with one another. Each sensory neuron integrates over time information from only their particular sensory input channel. Because each sensory neuron receives only a small part of the full picture, they need to <a href="https://en.wikipedia.org/wiki/Self-organization">self-organize</a> through communication in order for a global coherent behavior to <a href="https://en.wikipedia.org/wiki/Emergence">emerge</a>.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgjYRu83PENPLvdGRoF4fVc9Re-zl20f538Jkm3cU8NO7HtRs7yllXoWkKLMTeciDn2JPxEkGC1KMDbDscFIadqJMb72UaXyv6jfJr65e7UkrEmqznEJwQfsF2JBrwV6qoAyECFVphHrbjY_1cLml3jkEsNLt1e8Fs4ieW5CSn5hW383iJ_G5HmpN_lMA=s1209" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="376" data-original-width="1209" height="200" src="https://blogger.googleusercontent.com/img/a/AVvXsEgjYRu83PENPLvdGRoF4fVc9Re-zl20f538Jkm3cU8NO7HtRs7yllXoWkKLMTeciDn2JPxEkGC1KMDbDscFIadqJMb72UaXyv6jfJr65e7UkrEmqznEJwQfsF2JBrwV6qoAyECFVphHrbjY_1cLml3jkEsNLt1e8Fs4ieW5CSn5hW383iJ_G5HmpN_lMA=w640-h200" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><b>Illustration of observation segmentation.</b>We segment each input into elements, which are then fed to independent sensory neurons. For non-vision tasks where the inputs are usually 1D vectors, each element is a scalar. For vision tasks, we crop each input image into non-overlapping patches.</td></tr></tbody></table>
  
  
<p>
We encourage neurons to communicate with each other by training them to broadcast messages. While receiving information locally, each individual sensory neuron also continually broadcasts an output message at each time-step. These messages are consolidated and combined into an output vector, called the <em>global latent code</em>, using an attention mechanism similar to that applied in the <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer</a> architecture. A policy network then uses the global latent code to produce the action that the agent will use to interact with the environment. This action is also fed back into each sensory neuron in the next time-step, closing the communication loop.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj16icG3UKM4awNSfZ_4imo2AmoT6vtqJsv-9tgLWHLeAE854CHVT4M1oo9kDzleAQu4dFkl-siRW5okjYdpjzCJRg4CqA1uFT_wuWP5wPbDp8K6Ed-Ed4aZhhOxKqFF7Kux8VAcEIYfr6t3auizYi-c2Lc6km729LI9Y6FaU26G3tRxf41iVeUONv-6g=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="794" data-original-width="1999" height="254" src="https://blogger.googleusercontent.com/img/a/AVvXsEj16icG3UKM4awNSfZ_4imo2AmoT6vtqJsv-9tgLWHLeAE854CHVT4M1oo9kDzleAQu4dFkl-siRW5okjYdpjzCJRg4CqA1uFT_wuWP5wPbDp8K6Ed-Ed4aZhhOxKqFF7Kux8VAcEIYfr6t3auizYi-c2Lc6km729LI9Y6FaU26G3tRxf41iVeUONv-6g=w640-h254" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><b>Overview of the permutation-invariant RL method</b>. We first feed each individual observation (o<sub>t</sub>) into a particular sensory neuron (along with the agent&#8217;s previous action, a<sub>t-1</sub>). Each neuron then produces and broadcasts a message independently, and an attention mechanism summarizes them into a global latent code (m<sub><em>t</em></sub>) that is given to the agent's downstream policy network (?) to produce the agent&#8217;s action a<sub>t</sub>.</td></tr></tbody></table>

<p>
Why is this system permutation invariant? Each sensory neuron is an identical neural network that is not confined to only process information from one particular sensory input. In fact, in our setup, the inputs to each sensory neuron are not defined. Instead, each neuron must<em> figure out</em> the meaning of its input signal by paying attention to the inputs received by the other sensory neurons, rather than explicitly assuming a fixed meaning. This encourages the agent to process the entire input as an <a href="https://arxiv.org/abs/1810.00825">unordered set</a>, making the system to be permutation invariant to its input. Furthermore, in principle, the agent can use as many sensory neurons as required, thus enabling it to process observations of arbitrary length. Both of these properties will help the agent adapt to sensory substitutions.
</p>


<p>
<b>Results</b>
<br />
We demonstrate the robustness and flexibility of this approach in simpler, state-observation environments, where the observations the agent receives as inputs are low-dimensional vectors holding information about the agent&#8217;s states, such as the position or velocity of its components. The agent in the popular <a href="https://pybullet.org/">Ant</a> locomotion task has a total of 28 inputs with information that includes positions and velocities. We shuffle the order of the input vector several times during a trial and show that the agent is rapidly able to adapt and is still able to walk forward.
</p>

<p>
In <a href="https://github.com/google/brain-tokyo-workshop/tree/master/learntopredict/cartpole">cart-pole</a>, the agent&#8217;s goal is to swing up a cart-pole mounted at the center of the cart and balance it upright. Normally the agent sees only five inputs, but we modify the cartpole environment to provide 15 shuffled input signals, 10 of which are pure noise, and the remainder of which are the actual observations from the environment. The agent is still able to perform the task, demonstrating the system&#8217;s capacity to work with a large number of inputs and attend only to channels it deems useful. Such flexibility may find useful applications for processing a large unspecified number of signals, most of which are noise, from ill-defined systems.
</p>

<p>
We also apply this approach to high-dimensional vision-based environments where the observation is a stream of pixel images. Here, we investigate screen-shuffled versions of vision-based RL environments, where each observation frame is divided into a grid of patches, and like a puzzle, the agent must process the patches in a shuffled order to determine a course of action to take. To demonstrate our approach on vision-based tasks, we created a shuffled version of Atari Pong.
</p>

<table align="center" cellpadding="0" cellspacing="4" class="tr-caption-container">
  <tbody><tr>
    <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiKBu-1Z1G3ifq2FCMze1WmGK46SbQ9uMZ9lG2-j_EGTJlCud5SLT184tL26Hy6eEYnm7JLLx0wSYoWnANt9pKQfM7DHRD8AsLdCEJhrZFOtRDmAR5eGhokx9NzJIRFsxnei2nbtXdAjgG_tKIz-7nNswMSOCFCIy0W6BBFHGzMYTJ5kB8oULRa58Wzqg=s400" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="200" data-original-width="400" height="160" src="https://blogger.googleusercontent.com/img/a/AVvXsEiKBu-1Z1G3ifq2FCMze1WmGK46SbQ9uMZ9lG2-j_EGTJlCud5SLT184tL26Hy6eEYnm7JLLx0wSYoWnANt9pKQfM7DHRD8AsLdCEJhrZFOtRDmAR5eGhokx9NzJIRFsxnei2nbtXdAjgG_tKIz-7nNswMSOCFCIy0W6BBFHGzMYTJ5kB8oULRa58Wzqg=w320-h160" width="320" /></a></td>
  <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEh5NGBTcDhYcqnZ3MSx7Gqk-zxe_tF2ecz-LCv4aFliXePCs6yrs4xWmLwwFx2X85CaxPzgFoYZCErfJHN6fBWCAm04bccNFD9OdAaB5tc32a_DAQr4gblOO4orpYijAlZLYmOyvzyNUmWggbxxq5iNAe63dH0bdf-wd0qy8w33eIS3_QGQBhtZPtL5zA=s320" style="margin-left: auto; margin-right: auto; text-align: center;"><img border="0" data-original-height="200" data-original-width="320" height="200" src="https://blogger.googleusercontent.com/img/a/AVvXsEh5NGBTcDhYcqnZ3MSx7Gqk-zxe_tF2ecz-LCv4aFliXePCs6yrs4xWmLwwFx2X85CaxPzgFoYZCErfJHN6fBWCAm04bccNFD9OdAaB5tc32a_DAQr4gblOO4orpYijAlZLYmOyvzyNUmWggbxxq5iNAe63dH0bdf-wd0qy8w33eIS3_QGQBhtZPtL5zA=w400-h200" width="460" /></a></td>
        </tr>
  </tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;">Shuffled Pong results. <b>Left</b>: Pong agent trained to play using only 30% of the patches matches performance of Atari opponent. <b>Right</b>: Without extra training, when we give the agent more puzzle pieces, its performance increases.
</td></tr></tbody></table>
    
<p>
Here the agent&#8217;s input is a variable-length list of patches, so unlike typical RL agents, the agent only gets to &#8220;see&#8221; a subset of patches from the screen. In the puzzle pong experiment, we pass to the agent a random sample of patches across the screen, which are then fixed through the remainder of the game. We find that we can discard 70% of the patches (at these fixed-random locations) and still train the agent to perform well against the built-in Atari opponent. Interestingly, if we then reveal additional information to the agent (e.g., allowing it access to more image patches), its performance increases, even without additional training. When the agent receives all the patches, in shuffled order, it wins 100% of the time, achieving the same result with agents that are trained while seeing the entire screen.
</p>

<p>
We find that imposing additional difficulty during training by using unordered observations has additional benefits, such as improving generalization to unseen variations of the task, like when the background of the <a href="https://gym.openai.com/envs/CarRacing-v0/">CarRacing</a> training environment is replaced with a novel image.
</p>

<table align="center" cellpadding="0" cellspacing="4" class="tr-caption-container">
  <tbody><tr>
    <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj3O--5ecgaxzIvwpr6hPMfMxPicXgGLgXhUIQdWO0rbY11kF4i-DFb3W60CdKRn8vBsRcGzl3DMyaUiMRivmKxBF-AfMBVMudq9tmqZAyFYiXzA_YAjS396GqHlzp4HSP5drzLU7JwQDVXMwFMYigxnqDEuqYTkixnI8B53YhModqWv1qGyrMsVBvn9g=s320" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="160" data-original-width="320" height="320" src="https://blogger.googleusercontent.com/img/a/AVvXsEj3O--5ecgaxzIvwpr6hPMfMxPicXgGLgXhUIQdWO0rbY11kF4i-DFb3W60CdKRn8vBsRcGzl3DMyaUiMRivmKxBF-AfMBVMudq9tmqZAyFYiXzA_YAjS396GqHlzp4HSP5drzLU7JwQDVXMwFMYigxnqDEuqYTkixnI8B53YhModqWv1qGyrMsVBvn9g=w640-h320" width="640" /></a></td>
    <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEipvDmr-aRFOVk5e8NfxyIagQei5y6MJNTuAoZskOHETQfRwxxUbhAKgeWmAwQHKAf_0Q6iiDbNP5Mq7A4UdOzFH0FyqTq7Scz1Q91ve29w24GeI1qtFu0aaMuh5vpHvo9szPSDMM9Fn3-YrZiVtESPFPGJVOVmHpcSGFBT-Ec3masJW9dDx1_fAK8TUw=s320" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="160" data-original-width="320" height="320" src="https://blogger.googleusercontent.com/img/a/AVvXsEipvDmr-aRFOVk5e8NfxyIagQei5y6MJNTuAoZskOHETQfRwxxUbhAKgeWmAwQHKAf_0Q6iiDbNP5Mq7A4UdOzFH0FyqTq7Scz1Q91ve29w24GeI1qtFu0aaMuh5vpHvo9szPSDMM9Fn3-YrZiVtESPFPGJVOVmHpcSGFBT-Ec3masJW9dDx1_fAK8TUw=w640-h320" width="640" /></a></td>
           </tr>
  </tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;">Shuffled CarRacing results. The agent has learned to focus its attention (indicated by the highlighted patches) on the road boundaries. <b>Left:</b> Training environment. <b>Right:</b> Test environment with new background.
</td></tr></tbody></table> 
    
<p>
<b>Conclusion</b>
<br />
The permutation invariant neural network agents presented here can handle ill-defined, varying observation spaces. Our agents are robust to observations that contain redundant or noisy information, or observations that are corrupt and incomplete. We believe that permutation invariant systems open up numerous possibilities in reinforcement learning.
</p>

<p>
If you&#8217;re interested to learn more about this work, we invite readers to read our <a href="https://attentionneuron.github.io/">interactive article</a> (<a href="https://arxiv.org/abs/2109.02869">pdf</a> version) or watch our <a href="https://youtu.be/7nTlXhx0CZI">video</a>. We also released <a href="https://github.com/google/brain-tokyo-workshop">code</a> to reproduce our experiments.
</p>

<!--Footnotes themselves at the bottom.-->

<hr width="80%" /><span class="Apple-style-span" style="font-size: x-small;"><br />
  <a name="1"><sup>1</sup></a>Quoted in <a href="https://en.wikipedia.org/wiki/Livewired_(book)">Livewired</a>, by David Eagleman.<a href="#top1"> &nbsp;<sup>&#8617;<br /></sup></a></span>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Posted by David Ha, Staff Research Scientist and Yujin Tang, Research Software Engineer, Google Research, Tokyo</span> 
           <img src="https://blogger.googleusercontent.com/img/a/AVvXsEhG9U9ZdT3YvEGaVkOO15yWOXly9pwDWHafSv2yqd56jYi77LK_uEsRI13NikY7VfA2-DN1WAC7thrgufs0Ew0YHuZoYvkGw3VqcJXJQVt7jfmDFw8i4P6cBT7xH3cF2djo14Ph4BnFvJ9ZP5VDNubXd2GKsgSy1mnkFio214-rHOXk3xVdlly8dz4R8Q=s460" style="display: none;"> 
           <p> <i>The brain is able to use information coming from the skin as if it were coming from the eyes. We dont see with the eyes or hear with the ears, these are just the receptors, seeing and hearing in fact goes on in the brain.</i> </p>
           <p style="text-align: right;"> - <a href="https://en.wikipedia.org/wiki/Paul_Bach-y-Rita">Paul Bach-y-Rita</a><a href="#1" name="top1"><span class="Apple-style-span" style="font-size: small;"><sup><small>1</small></sup></span></a> </p> 
           <p> People have the amazing ability to use one sensory modality (e.g., touch) to supply environmental information normally gathered by another sense (e.g., vision). This adaptive ability, called <a href="https://en.wikipedia.org/wiki/Sensory_substitution">sensory substitution</a>, is a phenomenon well-known to neuroscience. While difficult adaptations  such as adjusting to seeing things <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010945217301314">upside-down</a>, learning to ride a <a href="https://ed.ted.com/best_of_web/bf2mRAfC">backwards bicycle</a>, or learning to see by interpreting visual information emitted from a grid of electrodes placed on ones tongue  require anywhere from weeks, months or even years to attain mastery, people are able to eventually adjust to sensory substitutions. </p> 
           <table align="center" cellpadding="0" cellspacing="4" class="tr-caption-container"> 
            <tbody>
             <tr> 
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEifk5jKD0BQctRKuEgUtcLjvH1IYPAtB20iO5FtuIKIR3jS1lLfvcySaXp6kGoWEHPZRi2QwQzs8VRPCK8gYBUTJmz9V2cwrFoMPFPbq4cqg5GT6LIbmGI7VzF2ivOGTPwNLXpyF86GuRL8mgqcafCjatn-y9Gv0FRG9XL-r9b5pZJ5cW2xb_AQ4eeiPA=s999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="605" data-original-width="999" height="388" src="https://blogger.googleusercontent.com/img/a/AVvXsEifk5jKD0BQctRKuEgUtcLjvH1IYPAtB20iO5FtuIKIR3jS1lLfvcySaXp6kGoWEHPZRi2QwQzs8VRPCK8gYBUTJmz9V2cwrFoMPFPbq4cqg5GT6LIbmGI7VzF2ivOGTPwNLXpyF86GuRL8mgqcafCjatn-y9Gv0FRG9XL-r9b5pZJ5cW2xb_AQ4eeiPA=w640-h388" width="640"></a></td> 
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEg7JNrWWO4-Odvsg3i_EM58Muc2Lh0Psy39zK7UHKB6l6BrBE7irh5ijNWtf8jJ19BiYKBP63IT8aGKBNQARHnepVphl_zmSFYiui8FX9ybrky267SqWX2zJ8AaPMt0MX2OYPZ2OvEDFS2Ky1qtub51kInh7NtP4HDKcwMp40faYuOwqkgWGMfQjBuYEQ=s379" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="379" data-original-width="321" height="400" src="https://blogger.googleusercontent.com/img/a/AVvXsEg7JNrWWO4-Odvsg3i_EM58Muc2Lh0Psy39zK7UHKB6l6BrBE7irh5ijNWtf8jJ19BiYKBP63IT8aGKBNQARHnepVphl_zmSFYiui8FX9ybrky267SqWX2zJ8AaPMt0MX2OYPZ2OvEDFS2Ky1qtub51kInh7NtP4HDKcwMp40faYuOwqkgWGMfQjBuYEQ=w339-h400" width="339"></a></td>
             </tr><!--<tr><td class="tr-caption" style="text-align: center;"></td>
    </tr>--> 
            </tbody>
           </table> 
           <table align="center" cellpadding="-4" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td class="tr-caption" style="text-align: center;">Examples of Sensory Substitution. <b>Left</b>: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0006899301026671">Tongue Display Unit</a> (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0006899301026671">Maris and Bach-y-Rita, 2001</a>; Image: <a href="https://www.sciencedirect.com/science/article/pii/S1026309811001702#f000020">Kaczmarek, 2011</a>). <b>Right</b>: Upside down goggles initially conceived by Erismann and Kohler in 1931. (Image <a href="https://en.wikipedia.org/wiki/Upside_down_goggles">Wikipedia</a>).</td>
             </tr>
            </tbody>
           </table> 
           <p> In contrast, most neural networks are not able to adapt to sensory substitutions <em>at all</em>. For instance, most <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> (RL) agents require their inputs to be in a pre-specified format, or else they will fail. They expect fixed-size inputs and assume that each element of the input carries a precise meaning, such as the pixel intensity at a specified location, or state information, like position or velocity. In popular RL benchmark tasks (e.g., <a href="https://pybullet.org/">Ant</a> or <a href="https://github.com/google/brain-tokyo-workshop/tree/master/learntopredict/cartpole">Cart-pole</a>), an agent trained using current <a href="https://github.com/DLR-RM/stable-baselines3">RL algorithms</a> will fail if its sensory inputs are changed or if the agent is fed additional noisy inputs that are unrelated to the task at hand. </p> 
           <p> In <a href="https://attentionneuron.github.io/">The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning</a>, a <a href="https://arxiv.org/abs/2109.02869">spotlight paper</a> at<a href="https://neurips.cc/"> NeurIPS 2021</a>, we explore <em>permutation invariant</em> neural network agents, which require each of their sensory neurons (receptors that receive sensory inputs from the environment) to figure out the meaning and context of its input signal, rather than explicitly assuming a fixed meaning. Our experiments show that such agents are robust to observations that contain additional redundant or noisy information, and to observations that are corrupt and incomplete. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container"> 
            <tbody>
             <tr> 
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEix_fWzEojotm-HAlw6PCMx9tcRH3rWLWLavBgoRegcHUUr-UfZwkNWvP47m4LQiHob1vVAYLEUBts4C3VQyW6413I_8dFhy0XQZROieavtB63eYAE4Ki7vBwiuBzDQdc78MWjxR76tdj5r_CBGZphsJfUPoUiXD1sO5hZ_NGl7hY8SPhZXjv43oCethw=s480" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="320" data-original-width="480" height="426" src="https://blogger.googleusercontent.com/img/a/AVvXsEix_fWzEojotm-HAlw6PCMx9tcRH3rWLWLavBgoRegcHUUr-UfZwkNWvP47m4LQiHob1vVAYLEUBts4C3VQyW6413I_8dFhy0XQZROieavtB63eYAE4Ki7vBwiuBzDQdc78MWjxR76tdj5r_CBGZphsJfUPoUiXD1sO5hZ_NGl7hY8SPhZXjv43oCethw=w640-h426" width="640"></a></td> 
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiFvxoniZCUitKtPZMgWX9Bj5ytMl-mvthB6sR7hw3iBvf6myLGNBEiKoi_MjV2zC5kiwWEte0_LhZzhEYyJzyS7DPj_EnnSbw55_KcJQwleavrs2poXEyFA5A_n0VKuFWLlV08XFLSpiQxaoI3IT80jfzYOHvAorXfF5ApV8IBJud2SXkTnO12gb9X9g=s772" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="772" data-original-width="720" height="400" src="https://blogger.googleusercontent.com/img/a/AVvXsEiFvxoniZCUitKtPZMgWX9Bj5ytMl-mvthB6sR7hw3iBvf6myLGNBEiKoi_MjV2zC5kiwWEte0_LhZzhEYyJzyS7DPj_EnnSbw55_KcJQwleavrs2poXEyFA5A_n0VKuFWLlV08XFLSpiQxaoI3IT80jfzYOHvAorXfF5ApV8IBJud2SXkTnO12gb9X9g=w373-h400" width="373"></a></td> 
             </tr> 
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td class="tr-caption" style="text-align: center;"> <em>Permutation invariant</em> reinforcement learning agents adapting to sensory substitutions. <b>Left</b>: The ordering of the ants 28 observations are randomly shuffled every 200 time-steps. Unlike the standard policy, our policy is not affected by the suddenly permuted inputs. <b>Right</b>: Cart-pole agent given many redundant noisy inputs (Interactive <a href="https://attentionneuron.github.io/#cartpole_demo_special">web-demo</a>).</td>
             </tr>
            </tbody>
           </table> 
           <p> In addition to adapting to sensory substitutions in state-observation environments (like the ant and cart-pole examples), we show that these agents can also adapt to sensory substitutions in complex visual-observation environments (such as a <a href="https://gym.openai.com/envs/CarRacing-v0/">CarRacing</a> game that uses only pixel observations) and can perform when the stream of input images is constantly being reshuffled: </p> 
           <table align="center" cellpadding="0" cellspacing="4" class="tr-caption-container"> 
            <tbody>
             <tr> 
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgIJk9mVzUhwo7aFu2u4rpQpVOaHClC8IKhoTAA0GJG_2PdKFBg3ZJN5Tf7wMgMD4hI08benjmbKYsfnPuEAo3pjqmjgJbSHl2eMgYknfGFuwEcNTPzuelQr_LYMct6MP23RKt62AqdjIEDLqbH1tOX6r3rjBn-fnmrSmSpjLHhNGyQu0QPN1OBABT1FA=s320" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="160" data-original-width="320" height="320" src="https://blogger.googleusercontent.com/img/a/AVvXsEgIJk9mVzUhwo7aFu2u4rpQpVOaHClC8IKhoTAA0GJG_2PdKFBg3ZJN5Tf7wMgMD4hI08benjmbKYsfnPuEAo3pjqmjgJbSHl2eMgYknfGFuwEcNTPzuelQr_LYMct6MP23RKt62AqdjIEDLqbH1tOX6r3rjBn-fnmrSmSpjLHhNGyQu0QPN1OBABT1FA=w640-h320" width="640"></a></td> 
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgtub_tV-ycwMMyvpfPeaUAJ2QpcKzpOuGKuGwBw1z_hYXpF49jDvJGcq6VhV0vJgr-rGFx_YhdXF6BiYTidTo0q_ok_6bcy6vkU-9FLf3G80BVKGa7R3R8HzTeZCHGmIyWC2Gj4LWDLwaHcAEKZ1ykoHqVvfkeTvDQmdA8c_WDJ5vPJmoK5LoWp9maYQ=s320" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="160" data-original-width="320" height="320" src="https://blogger.googleusercontent.com/img/a/AVvXsEgtub_tV-ycwMMyvpfPeaUAJ2QpcKzpOuGKuGwBw1z_hYXpF49jDvJGcq6VhV0vJgr-rGFx_YhdXF6BiYTidTo0q_ok_6bcy6vkU-9FLf3G80BVKGa7R3R8HzTeZCHGmIyWC2Gj4LWDLwaHcAEKZ1ykoHqVvfkeTvDQmdA8c_WDJ5vPJmoK5LoWp9maYQ=w640-h320" width="640"></a></td> 
             </tr> 
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td class="tr-caption" style="text-align: center;">We partition the visual input from CarRacing into a 2D grid of small patches, and shuffled their ordering. Without any additional training, our agent still performs even when the original training background (<b>left</b>) is replaced with new images (<b>right</b>).</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Method</b> <br> Our approach takes observations from the environment at each time-step and feeds each element of the observation into distinct, but identical neural networks (called sensory neurons), each with no fixed relationship with one another. Each sensory neuron integrates over time information from only their particular sensory input channel. Because each sensory neuron receives only a small part of the full picture, they need to <a href="https://en.wikipedia.org/wiki/Self-organization">self-organize</a> through communication in order for a global coherent behavior to <a href="https://en.wikipedia.org/wiki/Emergence">emerge</a>. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgjYRu83PENPLvdGRoF4fVc9Re-zl20f538Jkm3cU8NO7HtRs7yllXoWkKLMTeciDn2JPxEkGC1KMDbDscFIadqJMb72UaXyv6jfJr65e7UkrEmqznEJwQfsF2JBrwV6qoAyECFVphHrbjY_1cLml3jkEsNLt1e8Fs4ieW5CSn5hW383iJ_G5HmpN_lMA=s1209" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="376" data-original-width="1209" height="200" src="https://blogger.googleusercontent.com/img/a/AVvXsEgjYRu83PENPLvdGRoF4fVc9Re-zl20f538Jkm3cU8NO7HtRs7yllXoWkKLMTeciDn2JPxEkGC1KMDbDscFIadqJMb72UaXyv6jfJr65e7UkrEmqznEJwQfsF2JBrwV6qoAyECFVphHrbjY_1cLml3jkEsNLt1e8Fs4ieW5CSn5hW383iJ_G5HmpN_lMA=w640-h200" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;"><b>Illustration of observation segmentation.</b>We segment each input into elements, which are then fed to independent sensory neurons. For non-vision tasks where the inputs are usually 1D vectors, each element is a scalar. For vision tasks, we crop each input image into non-overlapping patches.</td>
             </tr>
            </tbody>
           </table> 
           <p> We encourage neurons to communicate with each other by training them to broadcast messages. While receiving information locally, each individual sensory neuron also continually broadcasts an output message at each time-step. These messages are consolidated and combined into an output vector, called the <em>global latent code</em>, using an attention mechanism similar to that applied in the <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer</a> architecture. A policy network then uses the global latent code to produce the action that the agent will use to interact with the environment. This action is also fed back into each sensory neuron in the next time-step, closing the communication loop. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj16icG3UKM4awNSfZ_4imo2AmoT6vtqJsv-9tgLWHLeAE854CHVT4M1oo9kDzleAQu4dFkl-siRW5okjYdpjzCJRg4CqA1uFT_wuWP5wPbDp8K6Ed-Ed4aZhhOxKqFF7Kux8VAcEIYfr6t3auizYi-c2Lc6km729LI9Y6FaU26G3tRxf41iVeUONv-6g=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="794" data-original-width="1999" height="254" src="https://blogger.googleusercontent.com/img/a/AVvXsEj16icG3UKM4awNSfZ_4imo2AmoT6vtqJsv-9tgLWHLeAE854CHVT4M1oo9kDzleAQu4dFkl-siRW5okjYdpjzCJRg4CqA1uFT_wuWP5wPbDp8K6Ed-Ed4aZhhOxKqFF7Kux8VAcEIYfr6t3auizYi-c2Lc6km729LI9Y6FaU26G3tRxf41iVeUONv-6g=w640-h254" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;"><b>Overview of the permutation-invariant RL method</b>. We first feed each individual observation (o<sub>t</sub>) into a particular sensory neuron (along with the agents previous action, a<sub>t-1</sub>). Each neuron then produces and broadcasts a message independently, and an attention mechanism summarizes them into a global latent code (m<sub><em>t</em></sub>) that is given to the agent's downstream policy network (?) to produce the agents action a<sub>t</sub>.</td>
             </tr>
            </tbody>
           </table> 
           <p> Why is this system permutation invariant? Each sensory neuron is an identical neural network that is not confined to only process information from one particular sensory input. In fact, in our setup, the inputs to each sensory neuron are not defined. Instead, each neuron must<em> figure out</em> the meaning of its input signal by paying attention to the inputs received by the other sensory neurons, rather than explicitly assuming a fixed meaning. This encourages the agent to process the entire input as an <a href="https://arxiv.org/abs/1810.00825">unordered set</a>, making the system to be permutation invariant to its input. Furthermore, in principle, the agent can use as many sensory neurons as required, thus enabling it to process observations of arbitrary length. Both of these properties will help the agent adapt to sensory substitutions. </p> 
           <p> <b>Results</b> <br> We demonstrate the robustness and flexibility of this approach in simpler, state-observation environments, where the observations the agent receives as inputs are low-dimensional vectors holding information about the agents states, such as the position or velocity of its components. The agent in the popular <a href="https://pybullet.org/">Ant</a> locomotion task has a total of 28 inputs with information that includes positions and velocities. We shuffle the order of the input vector several times during a trial and show that the agent is rapidly able to adapt and is still able to walk forward. </p> 
           <p> In <a href="https://github.com/google/brain-tokyo-workshop/tree/master/learntopredict/cartpole">cart-pole</a>, the agents goal is to swing up a cart-pole mounted at the center of the cart and balance it upright. Normally the agent sees only five inputs, but we modify the cartpole environment to provide 15 shuffled input signals, 10 of which are pure noise, and the remainder of which are the actual observations from the environment. The agent is still able to perform the task, demonstrating the systems capacity to work with a large number of inputs and attend only to channels it deems useful. Such flexibility may find useful applications for processing a large unspecified number of signals, most of which are noise, from ill-defined systems. </p> 
           <p> We also apply this approach to high-dimensional vision-based environments where the observation is a stream of pixel images. Here, we investigate screen-shuffled versions of vision-based RL environments, where each observation frame is divided into a grid of patches, and like a puzzle, the agent must process the patches in a shuffled order to determine a course of action to take. To demonstrate our approach on vision-based tasks, we created a shuffled version of Atari Pong. </p> 
           <table align="center" cellpadding="0" cellspacing="4" class="tr-caption-container"> 
            <tbody>
             <tr> 
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiKBu-1Z1G3ifq2FCMze1WmGK46SbQ9uMZ9lG2-j_EGTJlCud5SLT184tL26Hy6eEYnm7JLLx0wSYoWnANt9pKQfM7DHRD8AsLdCEJhrZFOtRDmAR5eGhokx9NzJIRFsxnei2nbtXdAjgG_tKIz-7nNswMSOCFCIy0W6BBFHGzMYTJ5kB8oULRa58Wzqg=s400" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="200" data-original-width="400" height="160" src="https://blogger.googleusercontent.com/img/a/AVvXsEiKBu-1Z1G3ifq2FCMze1WmGK46SbQ9uMZ9lG2-j_EGTJlCud5SLT184tL26Hy6eEYnm7JLLx0wSYoWnANt9pKQfM7DHRD8AsLdCEJhrZFOtRDmAR5eGhokx9NzJIRFsxnei2nbtXdAjgG_tKIz-7nNswMSOCFCIy0W6BBFHGzMYTJ5kB8oULRa58Wzqg=w320-h160" width="320"></a></td> 
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEh5NGBTcDhYcqnZ3MSx7Gqk-zxe_tF2ecz-LCv4aFliXePCs6yrs4xWmLwwFx2X85CaxPzgFoYZCErfJHN6fBWCAm04bccNFD9OdAaB5tc32a_DAQr4gblOO4orpYijAlZLYmOyvzyNUmWggbxxq5iNAe63dH0bdf-wd0qy8w33eIS3_QGQBhtZPtL5zA=s320" style="margin-left: auto; margin-right: auto; text-align: center;"><img border="0" data-original-height="200" data-original-width="320" height="200" src="https://blogger.googleusercontent.com/img/a/AVvXsEh5NGBTcDhYcqnZ3MSx7Gqk-zxe_tF2ecz-LCv4aFliXePCs6yrs4xWmLwwFx2X85CaxPzgFoYZCErfJHN6fBWCAm04bccNFD9OdAaB5tc32a_DAQr4gblOO4orpYijAlZLYmOyvzyNUmWggbxxq5iNAe63dH0bdf-wd0qy8w33eIS3_QGQBhtZPtL5zA=w400-h200" width="460"></a></td> 
             </tr> 
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td class="tr-caption" style="text-align: center;">Shuffled Pong results. <b>Left</b>: Pong agent trained to play using only 30% of the patches matches performance of Atari opponent. <b>Right</b>: Without extra training, when we give the agent more puzzle pieces, its performance increases. </td>
             </tr>
            </tbody>
           </table> 
           <p> Here the agents input is a variable-length list of patches, so unlike typical RL agents, the agent only gets to see a subset of patches from the screen. In the puzzle pong experiment, we pass to the agent a random sample of patches across the screen, which are then fixed through the remainder of the game. We find that we can discard 70% of the patches (at these fixed-random locations) and still train the agent to perform well against the built-in Atari opponent. Interestingly, if we then reveal additional information to the agent (e.g., allowing it access to more image patches), its performance increases, even without additional training. When the agent receives all the patches, in shuffled order, it wins 100% of the time, achieving the same result with agents that are trained while seeing the entire screen. </p> 
           <p> We find that imposing additional difficulty during training by using unordered observations has additional benefits, such as improving generalization to unseen variations of the task, like when the background of the <a href="https://gym.openai.com/envs/CarRacing-v0/">CarRacing</a> training environment is replaced with a novel image. </p> 
           <table align="center" cellpadding="0" cellspacing="4" class="tr-caption-container"> 
            <tbody>
             <tr> 
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj3O--5ecgaxzIvwpr6hPMfMxPicXgGLgXhUIQdWO0rbY11kF4i-DFb3W60CdKRn8vBsRcGzl3DMyaUiMRivmKxBF-AfMBVMudq9tmqZAyFYiXzA_YAjS396GqHlzp4HSP5drzLU7JwQDVXMwFMYigxnqDEuqYTkixnI8B53YhModqWv1qGyrMsVBvn9g=s320" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="160" data-original-width="320" height="320" src="https://blogger.googleusercontent.com/img/a/AVvXsEj3O--5ecgaxzIvwpr6hPMfMxPicXgGLgXhUIQdWO0rbY11kF4i-DFb3W60CdKRn8vBsRcGzl3DMyaUiMRivmKxBF-AfMBVMudq9tmqZAyFYiXzA_YAjS396GqHlzp4HSP5drzLU7JwQDVXMwFMYigxnqDEuqYTkixnI8B53YhModqWv1qGyrMsVBvn9g=w640-h320" width="640"></a></td> 
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEipvDmr-aRFOVk5e8NfxyIagQei5y6MJNTuAoZskOHETQfRwxxUbhAKgeWmAwQHKAf_0Q6iiDbNP5Mq7A4UdOzFH0FyqTq7Scz1Q91ve29w24GeI1qtFu0aaMuh5vpHvo9szPSDMM9Fn3-YrZiVtESPFPGJVOVmHpcSGFBT-Ec3masJW9dDx1_fAK8TUw=s320" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="160" data-original-width="320" height="320" src="https://blogger.googleusercontent.com/img/a/AVvXsEipvDmr-aRFOVk5e8NfxyIagQei5y6MJNTuAoZskOHETQfRwxxUbhAKgeWmAwQHKAf_0Q6iiDbNP5Mq7A4UdOzFH0FyqTq7Scz1Q91ve29w24GeI1qtFu0aaMuh5vpHvo9szPSDMM9Fn3-YrZiVtESPFPGJVOVmHpcSGFBT-Ec3masJW9dDx1_fAK8TUw=w640-h320" width="640"></a></td> 
             </tr> 
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td class="tr-caption" style="text-align: center;">Shuffled CarRacing results. The agent has learned to focus its attention (indicated by the highlighted patches) on the road boundaries. <b>Left:</b> Training environment. <b>Right:</b> Test environment with new background. </td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Conclusion</b> <br> The permutation invariant neural network agents presented here can handle ill-defined, varying observation spaces. Our agents are robust to observations that contain redundant or noisy information, or observations that are corrupt and incomplete. We believe that permutation invariant systems open up numerous possibilities in reinforcement learning. </p> 
           <p> If youre interested to learn more about this work, we invite readers to read our <a href="https://attentionneuron.github.io/">interactive article</a> (<a href="https://arxiv.org/abs/2109.02869">pdf</a> version) or watch our <a href="https://youtu.be/7nTlXhx0CZI">video</a>. We also released <a href="https://github.com/google/brain-tokyo-workshop">code</a> to reproduce our experiments. </p> <!--Footnotes themselves at the bottom.--> 
           <hr width="80%"><span class="Apple-style-span" style="font-size: x-small;"><br> <a name="1"><sup>1</sup></a>Quoted in <a href="https://en.wikipedia.org/wiki/Livewired_(book)">Livewired</a>, by David Eagleman.<a href="#top1"> &nbsp;<sup>?<br></sup></a></span> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Permutation-Invariant Neural Networks for Reinforcement Learning&amp;url=http://ai.googleblog.com/2021/11/permutation-invariant-neural-networks.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/11/permutation-invariant-neural-networks.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="post" data-id="4380850543713879249" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/11/predicting-text-readability-from.html" itemprop="url" title="Predicting Text Readability from Scrolling Interactions"> Predicting Text Readability from Scrolling Interactions </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Wednesday, November 17, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Posted by Sian Gooding, Intern, Google Research</span>


<p>
Illiteracy affects at least 773 million people globally, both young and old. For these individuals, reading information from unfamiliar sources or on unfamiliar topics can be extremely difficult. Unfortunately, these inequalities have been further magnified by the global pandemic as a result of unequal access to education in reading and writing. In fact, <a href="https://en.unesco.org/news/100-million-more-children-under-minimum-reading-proficiency-level-due-covid-19-unesco-convenes">UNESCO reports</a> that over 100 million children are falling behind the minimum proficiency level in reading due to COVID-related school closures. 
</p>

<p>
With increasing world-wide access to technology, reading on a device, such as a tablet or phone, has largely taken the place of traditional formats. This provides a unique opportunity to observe reading interactions, e.g., how a reader scrolls through a text, which can inform our understanding of what can make text difficult to read. This understanding is crucial when designing educational applications for low-proficiency readers and language learners, because it can be used to match learners with appropriately leveled texts as well as to support readers in understanding texts beyond their reading level. 
</p>

<p>
In &#8220;<a href="https://aclanthology.org/2021.conll-1.30.pdf">Predicting Text Readability from Scrolling Interactions</a>&#8221;, presented at <a href="https://conll.org">CoNLL 2021</a>, we show that data from on-device reading interactions can be used to predict how readable a text is. This novel approach provides insights into subjective readability &#8212; whether an individual reader has found a text accessible &#8212; and demonstrates that existing readability models can be improved by including feedback from scroll-based reading interactions. In order to encourage research in this area and to help enable more personalized tools for language learning and text simplification, we are releasing the <a href="https://github.com/siangooding/readability_scroll">dataset of reading interactions</a> generated from our scrolling behavior&#8211;based readability assessment of English-language texts.
</p>

<p>
<b>Understanding Text Difficulty</b>
<br />
There are multiple aspects of a text that impact how difficult it is to read, including the vocabulary level, the syntactic structure, and overall coherence. Traditional machine learning approaches to measure readability have exclusively relied on such linguistic features. However, using these features alone does not work well for online content, because such content often contains abbreviations, emojis, broken text, and short passages, which detrimentally impact the performance of readability models. 
</p>

<p>
To address this, we investigated whether aggregate data about the reading interactions of a group can be used to predict how difficult a text is, as well as how reading interactions may differ based on a readers&#8217; understanding. When reading on a device, readers typically interact with text by scrolling in a vertical fashion, which we hypothesize can be used as a coarse proxy for reading comprehension. With this in mind, we recruited 518 paid participants and asked them to read English-language texts of different difficulty levels. We recorded the reading interactions by measuring different features of the participants&#8217; scrolling behavior, such as the speed, acceleration and number of times areas of text were revisited. We then used this information to produce a set of features for a readability classifier. 
</p>

<p>
<b>Predicting Text Difficulty from Scrolling Behavior</b>
<br />
We investigated which types of scrolling behaviors were most impacted by text difficulty and tested the significance using <a href="https://en.wikipedia.org/wiki/Generalized_linear_mixed_model">linear mixed effect models</a>. In our set up, we have repeated measures, as multiple participants read the same texts and each participant reads more than one text. Using linear mixed-effect models gives us a higher confidence that the differences in interactions we are observing are because of the text difficulty, and not other random effects.
</p>

<p>
Our results showed that multiple reading behaviors differed significantly based on the text level, for example, the average, maximum and minimum acceleration of scrolling. We found the most significant features to be the total read time and the maximum reading speeds. 
</p>

<p>
We then used these features as inputs to a machine learning algorithm. We designed and trained a <a href="https://en.wikipedia.org/wiki/Support-vector_machine">support vector machine</a> (i.e., a binary classifier) to predict whether a text is either advanced or elementary based only on scrolling behaviors as individuals interacted with it. The dataset on which the model was trained contains 60 articles, each of which were read by an average of 17 participants. From these interactions we produced aggregate features by taking the mean of the significant measures across participants.</p>&nbsp;<div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgzjXLmrd92vXR5_Mnm9_3eUtjgYkdDCdtNAmKB_0U6eiIdLdzWStNzmI1-tW1a2lO4YrnSgCvoltYCDypdEmbk8MXI_ur2WjIdnTR5YzUeUdqbTg4JbVQB7HMxfSqZM3AfaGXDNGu3Hxm0XzaJqz3xNUFkn0Ll3XR7iiADx2rWlpulHH2-UH5K5mXoRw=s743" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="287" data-original-width="743" height="248" src="https://blogger.googleusercontent.com/img/a/AVvXsEgzjXLmrd92vXR5_Mnm9_3eUtjgYkdDCdtNAmKB_0U6eiIdLdzWStNzmI1-tW1a2lO4YrnSgCvoltYCDypdEmbk8MXI_ur2WjIdnTR5YzUeUdqbTg4JbVQB7HMxfSqZM3AfaGXDNGu3Hxm0XzaJqz3xNUFkn0Ll3XR7iiADx2rWlpulHH2-UH5K5mXoRw=w640-h248" width="640" /></a></div>


<p>
We measured the accuracy of the approach using a metric called <a href="https://en.wikipedia.org/wiki/F-score">f-score</a>, which measures how accurate the model is at classifying a text as either &#8220;easy&#8221; or &#8220;difficult&#8221; (where 1.0 reflects perfect classification accuracy). We are able to achieve an f-score of 0.77 on this task, using interaction features alone. This is the first work to show that it is possible to predict the readability of a text using only interaction features.
</p>

<p>
<b>Improving Readability Models</b>
<br />
In order to demonstrate the value of applying readability measures from scrolling behaviors to existing readability models, we integrated scroll-based features into the state-of-the-art automated readability assessment tool, which was released as part of the <a href="https://aclanthology.org/W18-0535.pdf">OneStopEnglish corpus</a>. We found that the addition of interaction features improves the f-score of this model from 0.84 to 0.88. In addition, we were able to significantly outperform this system by using interaction information with simple vocabulary features, such as the number of words in the text, achieving an impressive f-score of 0.96.
</p>

<p>
In our study, we recorded comprehension scores to evaluate the understanding and readability of text for <em>individuals</em>. Participants were asked three questions per article to assess the reader&#8217;s understanding of what they had read. The interaction features of an individual&#8217;s scrolling behavior was represented as a high dimensional vector. To explore this data, we visualized the reading interaction features for each participant using <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-distributed stochastic neighbor embeddings</a>, which is a statistical method for visualizing high-dimensional data. The results revealed clusters in the comprehension score based on how well individuals understood the text. This shows that there is implicit information in reading interactions about the likelihood that an <em>individual</em> has understood a given text. We refer to this phenomenon as <em>subjective readability</em>. This information can be very useful for educational applications or for simplifying online content. 
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEigNWwBya_aso27v93XdH78lSEnqfmVt8CynXTR0yCDi0KWaU4VQrrJ2LZzwXzqPmdODs5xo9RSdyBuENvlbJV_nnC6T8KH-RH4ukNzJItXUArpLGLUu85ZTdjVXien58Yo1FGYfFz0BhM8N8e6uhE7PdS5DXz61u1Lc4MYupwSTvaI6v1LgGTncMrCYw=s319" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="319" data-original-width="309" height="640" src="https://blogger.googleusercontent.com/img/a/AVvXsEigNWwBya_aso27v93XdH78lSEnqfmVt8CynXTR0yCDi0KWaU4VQrrJ2LZzwXzqPmdODs5xo9RSdyBuENvlbJV_nnC6T8KH-RH4ukNzJItXUArpLGLUu85ZTdjVXien58Yo1FGYfFz0BhM8N8e6uhE7PdS5DXz61u1Lc4MYupwSTvaI6v1LgGTncMrCYw=w619-h640" width="619" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Plot showing t-SNE projection of scroll interactions in 2-dimensions. The color of each data point corresponds to the comprehension score. Clusters of comprehension scores indicate that there are correlations between reading behaviors and comprehension.</td></tr></tbody></table>



<p>
Finally, we investigated the extent to which reading interactions vary across audiences. We compared the average scrolling speed across different reader groups, covering reading proficiency and the reader&#8217;s first language. We found that the speed distribution varies depending on the proficiency and first language of the audience. This supports the case that first language and proficiency alter the reading behaviors of audiences, which allows us to contextualize the reading behavior of groups and better understand which areas of text may be harder for them to read. 
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiCGREKJMugy98QnGllBEwvkhXZdaqfeF4A2xI_wb3qVFZ5Rf_Rf1szc6lmCa8w2dcKJEKEdY74SBRRjFlEf01pbBPvIFj4QIetdnJCIwUFvfEy6OzNg9gsq30mYVOajwaCdDoQYEhXSA0uB0bHKNnTn1Ww1vnkBgqnwEAjyvh1Hp2I-6v6XjxaSoN_PA=s2048" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1436" data-original-width="2048" height="280" src="https://blogger.googleusercontent.com/img/a/AVvXsEiCGREKJMugy98QnGllBEwvkhXZdaqfeF4A2xI_wb3qVFZ5Rf_Rf1szc6lmCa8w2dcKJEKEdY74SBRRjFlEf01pbBPvIFj4QIetdnJCIwUFvfEy6OzNg9gsq30mYVOajwaCdDoQYEhXSA0uB0bHKNnTn1Ww1vnkBgqnwEAjyvh1Hp2I-6v6XjxaSoN_PA=w400-h280" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Histogram">Histogram</a> showing the average speeds of scrolling (in vertical pixels per millisecond) across readers of different proficiency levels (beginner, intermediate and advanced), with lines showing the smoothed trend for each group. A higher average scroll speed indicates faster reading times. For example, a more challenging text that corresponds to slower scroll speeds by advanced readers is associated with higher scroll speeds by beginners because they engage with the text only superficially.</td></tr></tbody></table>

<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjBdZs-63kpDNovZrpORQLq99zBKFfIyWC1sOd-XefXPPtotBeAgBiRMGGpzuq3RHxwr1OTSlni-PGV41mOvl34m5GBq7VP898eIAtH1X0mhW5-VnP8ONmD7aOfp8JAGf-hkGbiknvwqd1pBMBAmMewTgyW_bahYYaQB_JJ2CKkUwQtiozosV487-swnQ=s2048" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1428" data-original-width="1999" height="286" src="https://blogger.googleusercontent.com/img/a/AVvXsEjBdZs-63kpDNovZrpORQLq99zBKFfIyWC1sOd-XefXPPtotBeAgBiRMGGpzuq3RHxwr1OTSlni-PGV41mOvl34m5GBq7VP898eIAtH1X0mhW5-VnP8ONmD7aOfp8JAGf-hkGbiknvwqd1pBMBAmMewTgyW_bahYYaQB_JJ2CKkUwQtiozosV487-swnQ=w400-h286" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Histogram showing the average speeds of scrolling (in vertical pixels per millisecond) across audiences by first language of the readers, Tamil or English, with lines showing the smoothed trend for each group. A higher average scroll speed indicates faster reading times. Dark blue bars are where the histograms overlap.</td></tr></tbody></table>

<p>
<b>Conclusion</b>
<br />
This work is the first to show that reading interactions, such as scrolling behavior, can be used to predict the readability of text, which can yield numerous benefits. Such measures are language agnostic, unobtrusive, and robust to noisy text. Implicit user feedback allows insight into readability at an individual level, thereby allowing for a more inclusive and personalisable assessment of text difficulty. Furthermore, being able to judge the subjective readability of text benefits language learning and educational apps. We conducted a 518 participant study to investigate the impact of text readability on reading interactions and are releasing a <a href="https://github.com/siangooding/readability_scroll">novel dataset of the associated reading interactions</a>. We confirm that there are statistically significant differences in the way that readers interact with advanced and elementary texts, and that the comprehension scores of individuals correlate with specific measures of scrolling interaction. For more information our <a href="https://www.youtube.com/watch?v=Y0Q7cF1f2tw">conference presentation</a> is available to view. 
</p>

<p>
<b>Acknowledgements</b>
<br />
<em>We thank our collaborators Yevgeni Berzak, Tony Mak and Matt Sharifi, as well as Dmitry Lagun and Blaise Aguera y Arcas for their helpful feedback on the paper.</em>
</p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Posted by Sian Gooding, Intern, Google Research</span> 
           <p> Illiteracy affects at least 773 million people globally, both young and old. For these individuals, reading information from unfamiliar sources or on unfamiliar topics can be extremely difficult. Unfortunately, these inequalities have been further magnified by the global pandemic as a result of unequal access to education in reading and writing. In fact, <a href="https://en.unesco.org/news/100-million-more-children-under-minimum-reading-proficiency-level-due-covid-19-unesco-convenes">UNESCO reports</a> that over 100 million children are falling behind the minimum proficiency level in reading due to COVID-related school closures. </p> 
           <p> With increasing world-wide access to technology, reading on a device, such as a tablet or phone, has largely taken the place of traditional formats. This provides a unique opportunity to observe reading interactions, e.g., how a reader scrolls through a text, which can inform our understanding of what can make text difficult to read. This understanding is crucial when designing educational applications for low-proficiency readers and language learners, because it can be used to match learners with appropriately leveled texts as well as to support readers in understanding texts beyond their reading level. </p> 
           <p> In <a href="https://aclanthology.org/2021.conll-1.30.pdf">Predicting Text Readability from Scrolling Interactions</a>, presented at <a href="https://conll.org">CoNLL 2021</a>, we show that data from on-device reading interactions can be used to predict how readable a text is. This novel approach provides insights into subjective readability  whether an individual reader has found a text accessible  and demonstrates that existing readability models can be improved by including feedback from scroll-based reading interactions. In order to encourage research in this area and to help enable more personalized tools for language learning and text simplification, we are releasing the <a href="https://github.com/siangooding/readability_scroll">dataset of reading interactions</a> generated from our scrolling behaviorbased readability assessment of English-language texts. </p> 
           <p> <b>Understanding Text Difficulty</b> <br> There are multiple aspects of a text that impact how difficult it is to read, including the vocabulary level, the syntactic structure, and overall coherence. Traditional machine learning approaches to measure readability have exclusively relied on such linguistic features. However, using these features alone does not work well for online content, because such content often contains abbreviations, emojis, broken text, and short passages, which detrimentally impact the performance of readability models. </p> 
           <p> To address this, we investigated whether aggregate data about the reading interactions of a group can be used to predict how difficult a text is, as well as how reading interactions may differ based on a readers understanding. When reading on a device, readers typically interact with text by scrolling in a vertical fashion, which we hypothesize can be used as a coarse proxy for reading comprehension. With this in mind, we recruited 518 paid participants and asked them to read English-language texts of different difficulty levels. We recorded the reading interactions by measuring different features of the participants scrolling behavior, such as the speed, acceleration and number of times areas of text were revisited. We then used this information to produce a set of features for a readability classifier. </p> 
           <p> <b>Predicting Text Difficulty from Scrolling Behavior</b> <br> We investigated which types of scrolling behaviors were most impacted by text difficulty and tested the significance using <a href="https://en.wikipedia.org/wiki/Generalized_linear_mixed_model">linear mixed effect models</a>. In our set up, we have repeated measures, as multiple participants read the same texts and each participant reads more than one text. Using linear mixed-effect models gives us a higher confidence that the differences in interactions we are observing are because of the text difficulty, and not other random effects. </p> 
           <p> Our results showed that multiple reading behaviors differed significantly based on the text level, for example, the average, maximum and minimum acceleration of scrolling. We found the most significant features to be the total read time and the maximum reading speeds. </p> 
           <p> We then used these features as inputs to a machine learning algorithm. We designed and trained a <a href="https://en.wikipedia.org/wiki/Support-vector_machine">support vector machine</a> (i.e., a binary classifier) to predict whether a text is either advanced or elementary based only on scrolling behaviors as individuals interacted with it. The dataset on which the model was trained contains 60 articles, each of which were read by an average of 17 participants. From these interactions we produced aggregate features by taking the mean of the significant measures across participants.</p>&nbsp;
           <div class="separator" style="clear: both; text-align: center;">
            <a href="https://blogger.googleusercontent.com/img/a/AVvXsEgzjXLmrd92vXR5_Mnm9_3eUtjgYkdDCdtNAmKB_0U6eiIdLdzWStNzmI1-tW1a2lO4YrnSgCvoltYCDypdEmbk8MXI_ur2WjIdnTR5YzUeUdqbTg4JbVQB7HMxfSqZM3AfaGXDNGu3Hxm0XzaJqz3xNUFkn0Ll3XR7iiADx2rWlpulHH2-UH5K5mXoRw=s743" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="287" data-original-width="743" height="248" src="https://blogger.googleusercontent.com/img/a/AVvXsEgzjXLmrd92vXR5_Mnm9_3eUtjgYkdDCdtNAmKB_0U6eiIdLdzWStNzmI1-tW1a2lO4YrnSgCvoltYCDypdEmbk8MXI_ur2WjIdnTR5YzUeUdqbTg4JbVQB7HMxfSqZM3AfaGXDNGu3Hxm0XzaJqz3xNUFkn0Ll3XR7iiADx2rWlpulHH2-UH5K5mXoRw=w640-h248" width="640"></a>
           </div> 
           <p> We measured the accuracy of the approach using a metric called <a href="https://en.wikipedia.org/wiki/F-score">f-score</a>, which measures how accurate the model is at classifying a text as either easy or difficult (where 1.0 reflects perfect classification accuracy). We are able to achieve an f-score of 0.77 on this task, using interaction features alone. This is the first work to show that it is possible to predict the readability of a text using only interaction features. </p> 
           <p> <b>Improving Readability Models</b> <br> In order to demonstrate the value of applying readability measures from scrolling behaviors to existing readability models, we integrated scroll-based features into the state-of-the-art automated readability assessment tool, which was released as part of the <a href="https://aclanthology.org/W18-0535.pdf">OneStopEnglish corpus</a>. We found that the addition of interaction features improves the f-score of this model from 0.84 to 0.88. In addition, we were able to significantly outperform this system by using interaction information with simple vocabulary features, such as the number of words in the text, achieving an impressive f-score of 0.96. </p> 
           <p> In our study, we recorded comprehension scores to evaluate the understanding and readability of text for <em>individuals</em>. Participants were asked three questions per article to assess the readers understanding of what they had read. The interaction features of an individuals scrolling behavior was represented as a high dimensional vector. To explore this data, we visualized the reading interaction features for each participant using <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-distributed stochastic neighbor embeddings</a>, which is a statistical method for visualizing high-dimensional data. The results revealed clusters in the comprehension score based on how well individuals understood the text. This shows that there is implicit information in reading interactions about the likelihood that an <em>individual</em> has understood a given text. We refer to this phenomenon as <em>subjective readability</em>. This information can be very useful for educational applications or for simplifying online content. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEigNWwBya_aso27v93XdH78lSEnqfmVt8CynXTR0yCDi0KWaU4VQrrJ2LZzwXzqPmdODs5xo9RSdyBuENvlbJV_nnC6T8KH-RH4ukNzJItXUArpLGLUu85ZTdjVXien58Yo1FGYfFz0BhM8N8e6uhE7PdS5DXz61u1Lc4MYupwSTvaI6v1LgGTncMrCYw=s319" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="319" data-original-width="309" height="640" src="https://blogger.googleusercontent.com/img/a/AVvXsEigNWwBya_aso27v93XdH78lSEnqfmVt8CynXTR0yCDi0KWaU4VQrrJ2LZzwXzqPmdODs5xo9RSdyBuENvlbJV_nnC6T8KH-RH4ukNzJItXUArpLGLUu85ZTdjVXien58Yo1FGYfFz0BhM8N8e6uhE7PdS5DXz61u1Lc4MYupwSTvaI6v1LgGTncMrCYw=w619-h640" width="619"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Plot showing t-SNE projection of scroll interactions in 2-dimensions. The color of each data point corresponds to the comprehension score. Clusters of comprehension scores indicate that there are correlations between reading behaviors and comprehension.</td>
             </tr>
            </tbody>
           </table> 
           <p> Finally, we investigated the extent to which reading interactions vary across audiences. We compared the average scrolling speed across different reader groups, covering reading proficiency and the readers first language. We found that the speed distribution varies depending on the proficiency and first language of the audience. This supports the case that first language and proficiency alter the reading behaviors of audiences, which allows us to contextualize the reading behavior of groups and better understand which areas of text may be harder for them to read. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiCGREKJMugy98QnGllBEwvkhXZdaqfeF4A2xI_wb3qVFZ5Rf_Rf1szc6lmCa8w2dcKJEKEdY74SBRRjFlEf01pbBPvIFj4QIetdnJCIwUFvfEy6OzNg9gsq30mYVOajwaCdDoQYEhXSA0uB0bHKNnTn1Ww1vnkBgqnwEAjyvh1Hp2I-6v6XjxaSoN_PA=s2048" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1436" data-original-width="2048" height="280" src="https://blogger.googleusercontent.com/img/a/AVvXsEiCGREKJMugy98QnGllBEwvkhXZdaqfeF4A2xI_wb3qVFZ5Rf_Rf1szc6lmCa8w2dcKJEKEdY74SBRRjFlEf01pbBPvIFj4QIetdnJCIwUFvfEy6OzNg9gsq30mYVOajwaCdDoQYEhXSA0uB0bHKNnTn1Ww1vnkBgqnwEAjyvh1Hp2I-6v6XjxaSoN_PA=w400-h280" width="400"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Histogram">Histogram</a> showing the average speeds of scrolling (in vertical pixels per millisecond) across readers of different proficiency levels (beginner, intermediate and advanced), with lines showing the smoothed trend for each group. A higher average scroll speed indicates faster reading times. For example, a more challenging text that corresponds to slower scroll speeds by advanced readers is associated with higher scroll speeds by beginners because they engage with the text only superficially.</td>
             </tr>
            </tbody>
           </table> 
           <br>
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjBdZs-63kpDNovZrpORQLq99zBKFfIyWC1sOd-XefXPPtotBeAgBiRMGGpzuq3RHxwr1OTSlni-PGV41mOvl34m5GBq7VP898eIAtH1X0mhW5-VnP8ONmD7aOfp8JAGf-hkGbiknvwqd1pBMBAmMewTgyW_bahYYaQB_JJ2CKkUwQtiozosV487-swnQ=s2048" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1428" data-original-width="1999" height="286" src="https://blogger.googleusercontent.com/img/a/AVvXsEjBdZs-63kpDNovZrpORQLq99zBKFfIyWC1sOd-XefXPPtotBeAgBiRMGGpzuq3RHxwr1OTSlni-PGV41mOvl34m5GBq7VP898eIAtH1X0mhW5-VnP8ONmD7aOfp8JAGf-hkGbiknvwqd1pBMBAmMewTgyW_bahYYaQB_JJ2CKkUwQtiozosV487-swnQ=w400-h286" width="400"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Histogram showing the average speeds of scrolling (in vertical pixels per millisecond) across audiences by first language of the readers, Tamil or English, with lines showing the smoothed trend for each group. A higher average scroll speed indicates faster reading times. Dark blue bars are where the histograms overlap.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Conclusion</b> <br> This work is the first to show that reading interactions, such as scrolling behavior, can be used to predict the readability of text, which can yield numerous benefits. Such measures are language agnostic, unobtrusive, and robust to noisy text. Implicit user feedback allows insight into readability at an individual level, thereby allowing for a more inclusive and personalisable assessment of text difficulty. Furthermore, being able to judge the subjective readability of text benefits language learning and educational apps. We conducted a 518 participant study to investigate the impact of text readability on reading interactions and are releasing a <a href="https://github.com/siangooding/readability_scroll">novel dataset of the associated reading interactions</a>. We confirm that there are statistically significant differences in the way that readers interact with advanced and elementary texts, and that the comprehension scores of individuals correlate with specific measures of scrolling interaction. For more information our <a href="https://www.youtube.com/watch?v=Y0Q7cF1f2tw">conference presentation</a> is available to view. </p> 
           <p> <b>Acknowledgements</b> <br> <em>We thank our collaborators Yevgeni Berzak, Tony Mak and Matt Sharifi, as well as Dmitry Lagun and Blaise Aguera y Arcas for their helpful feedback on the paper.</em> </p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Predicting Text Readability from Scrolling Interactions&amp;url=http://ai.googleblog.com/2021/11/predicting-text-readability-from.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/11/predicting-text-readability-from.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="post" data-id="3747664622032574221" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/11/rliable-towards-reliable-evaluation.html" itemprop="url" title="RLiable: Towards Reliable Evaluation &amp; Reporting in Reinforcement Learning"> RLiable: Towards Reliable Evaluation &amp; Reporting in Reinforcement Learning </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Wednesday, November 17, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Posted by Rishabh Agarwal, Research Scientist and Pablo Samuel Castro, Staff Software Engineer, Google Research, Brain Team 
</span>

<p>

<a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement learning</a> (RL) is an area of <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> that focuses on learning from experiences to solve decision making tasks. While the field of RL has made great progress, resulting in impressive empirical results on complex tasks, such as <a href="https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html">playing video games</a>, <a href="http://rdcu.be/cbBRc">flying stratospheric balloons</a> and <a href="https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html">designing hardware chips</a>, it is <a href="https://arxiv.org/abs/1709.06560">becoming</a> <a href="https://arxiv.org/abs/1806.08295">increasingly</a> <a href="https://arxiv.org/abs/1803.07055">apparent</a> that the current standards for empirical evaluation might give a false sense of fast scientific progress while slowing it down.
</p>

<p>

To that end, in &#8220;<a href="https://agarwl.github.io/rliable">Deep RL at the Edge of the Statistical Precipice</a>&#8221;, accepted as an <a href="https://openreview.net/forum?id=uqv8-U4lKBe">oral presentation</a> at <a href="https://openreview.net/group?id=NeurIPS.cc/2021/Conference">NeurIPS 2021</a>, we discuss how <a href="https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty">statistical uncertainty</a> of results needs to be considered, especially when using only a few training runs, in order for evaluation in deep RL to be reliable.  Specifically, the predominant practice of reporting <a href="https://en.wikipedia.org/wiki/Point_estimation">point estimates</a> ignores this uncertainty and hinders reproducibility of results. Related to this, tables with per-task scores, as are commonly reported, can be overwhelming beyond a few tasks and often omit standard deviations. Furthermore, simple performance metrics like the mean can be dominated by a few outlier tasks, while the median score would remain unaffected even if up to half of the tasks had performance scores of zero. Thus, to increase the field's confidence in reported results with a handful of runs, we propose various statistical tools, including stratified bootstrap confidence intervals, performance profiles, and better metrics, such as interquartile mean and probability of improvement. To help researchers incorporate these tools, we also release an easy-to-use Python library <a href="https://github.com/google-research/rliable">RLiable</a> with a <a href="https://colab.research.google.com/drive/1a0pSD-1tWhMmeJeeoyZM1A-HCW3yf1xR?usp=sharing">quickstart colab</a>.
</p>

<p>
<b>Statistical Uncertainty in RL Evaluation</b>
<br />
Empirical research in RL relies on evaluating performance on a diverse suite of tasks, such as <a href="https://www.google.com/search?q=Atari+2600+games">Atari 2600 video games</a>, to assess progress.  Published results on deep RL benchmarks typically compare <a href="https://en.wikipedia.org/wiki/Point_estimation">point estimates</a> of the mean and median scores aggregated across tasks. These scores are typically relative to some defined baseline and optimal performance (e.g., random agent and &#8220;average&#8221; human performance on Atari games, respectively) so as to make scores comparable across different tasks. 
</p>


<p>
In most RL experiments, there is randomness in the scores obtained from different training runs, so reporting only point estimates does not reveal whether similar results would be obtained with new independent runs. A small number of training runs, coupled with the <a href="https://arxiv.org/abs/1912.05663">high</a> <a href="https://arxiv.org/abs/1904.06312">variability</a> <a href="https://arxiv.org/abs/1909.03772">in</a> performance of deep RL algorithms, often leads to large statistical uncertainty in such point estimates. 
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEigMVfUgUJ35fOsr0HsN5J7mW6n18c3JuynEJFdxsvIHGm3zFmCtpTumI3PxlQHXF2t_zgoyUBnmQKmiS6xVtyhRwFLO1SynUVQiEN0t2KbYZuigRs42QMMcy1kRFc6HJGpft-HHS_sVUSG65R853eHvBy7wyTDddaRePGh-_YKgCR4yfnhmhEwYjjp8w=s1200" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="740" data-original-width="1200" height="394" src="https://blogger.googleusercontent.com/img/a/AVvXsEigMVfUgUJ35fOsr0HsN5J7mW6n18c3JuynEJFdxsvIHGm3zFmCtpTumI3PxlQHXF2t_zgoyUBnmQKmiS6xVtyhRwFLO1SynUVQiEN0t2KbYZuigRs42QMMcy1kRFc6HJGpft-HHS_sVUSG65R853eHvBy7wyTDddaRePGh-_YKgCR4yfnhmhEwYjjp8w=w640-h394" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The distribution of median human normalized scores on the <a href="https://arxiv.org/abs/1903.00374">Atari 100k</a> benchmark, which contains 26 games, for five recently published algorithms, <a href="https://arxiv.org/abs/1906.05243">DER</a>, <a href="https://arxiv.org/abs/2003.10181">OTR</a>, <a href="https://arxiv.org/abs/2004.04136">CURL</a>, two variants of <a href="https://arxiv.org/abs/2004.13649">DrQ</a>, and <a href="https://arxiv.org/abs/2007.05929">SPR</a>. The reported point estimates of median scores based on a few runs in publications, as shown by dashed lines, do not provide information about the variability in median scores and typically overestimate (e.g., CURL, SPR, DrQ) or underestimate (e.g., DER) the expected median, which can result in erroneous conclusions.</td></tr></tbody></table>




<p>
As benchmarks become increasingly more complex, evaluating more than a few runs will be increasingly demanding due to the increased compute and data needed to solve such tasks. For example, five runs on 50 Atari games for 200 million frames takes 1000+ GPU days.  Thus, evaluating more runs is not a feasible solution for reducing statistical uncertainty on computationally demanding benchmarks. While <a href="https://arxiv.org/abs/1709.06560">prior</a> <a href="https://arxiv.org/abs/1904.06979">work</a> has recommended <a href="https://en.wikipedia.org/wiki/Statistical_significance">statistical significance</a> tests as a solution, such tests are <a href="https://www.nature.com/articles/d41586-019-00857-9">dichotomous in nature</a> (either &#8220;significant&#8221; or &#8220;not significant&#8221;), so they often lack the granularity needed to yield meaningful insights and are <a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913">widely misinterpreted</a>. 
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhcuKWT01-AvyHEUoCc-_NGaeHtFnCnD1uoeU9Nn-NCrMW6jwIoZ4sBkSIHcK9GJy5wKeVmG4xJdkXv1vZg17_yIkc4VcrxzzJ1C7-SuqtaqOqcKktAPGpGBiMrTgKAWN_8nE61Sw9LgA71u_R8R9OG-NUb_bw6LxyaygyfK88mHpOEEuWrEQsoL06v3w=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="846" data-original-width="1999" height="270" src="https://blogger.googleusercontent.com/img/a/AVvXsEhcuKWT01-AvyHEUoCc-_NGaeHtFnCnD1uoeU9Nn-NCrMW6jwIoZ4sBkSIHcK9GJy5wKeVmG4xJdkXv1vZg17_yIkc4VcrxzzJ1C7-SuqtaqOqcKktAPGpGBiMrTgKAWN_8nE61Sw9LgA71u_R8R9OG-NUb_bw6LxyaygyfK88mHpOEEuWrEQsoL06v3w=w640-h270" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Number of runs in RL papers over the years. Beginning with the <a href="https://arxiv.org/abs/1207.4708">Arcade Learning Environment</a> (ALE), the shift toward computationally-demanding benchmarks has led to the practice of evaluating only a handful of runs per task, increasing the statistical uncertainty in point estimates. </td></tr></tbody></table>



<p>
<b>Tools for Reliable Evaluation</b>
<br />
Any aggregate metric based on a finite number of runs is a <a href="https://en.wikipedia.org/wiki/Random_variable">random variable</a>, so to take this into account, we advocate for reporting <em>stratified <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrap</a> <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence intervals</a></em> (CIs), which predict the likely values of aggregate metrics if the same experiment were repeated with different runs. These CIs allow us to understand the statistical uncertainty and reproducibility of results. Such CIs use the scores on combined runs across tasks. For example, evaluating 3 runs each on <a href="https://arxiv.org/abs/1903.00374">Atari 100k</a>, which contains 26 tasks, results in 78 sample scores for uncertainty estimation. 
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgLuKXv3mpjh9fpzosblqurTrARuBshMYmX0SXPjvFUxSujQ6j_cm7ljLzRpwkPuwseysygp9f_bwtusH9Av-hs-32UP5VZmscajQOIV6wE_o4bDjzOpl0NLBTN2SN2fhxV1jHljDbFFhIIZx1z_e4uo-gir1BjlCv_5Jkk6cKCRr-70TppuoCfqiOKNg=s1450" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1000" data-original-width="1450" height="442" src="https://blogger.googleusercontent.com/img/a/AVvXsEgLuKXv3mpjh9fpzosblqurTrARuBshMYmX0SXPjvFUxSujQ6j_cm7ljLzRpwkPuwseysygp9f_bwtusH9Av-hs-32UP5VZmscajQOIV6wE_o4bDjzOpl0NLBTN2SN2fhxV1jHljDbFFhIIZx1z_e4uo-gir1BjlCv_5Jkk6cKCRr-70TppuoCfqiOKNg=w640-h442" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">In each task, colored balls denote scores on different runs.  To compute statified bootstrap CIs using the <a href="https://www.uvm.edu/~statdhtx/StatPages/Randomization%20Tests/ResamplingWithR/BootstMeans/bootstrapping_means.html">percentile method</a>, bootstrap samples are created by randomly sampling scores <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)#Replacement_of_selected_units">with replacement</a> proportionately from each task. Then, the distribution of aggregate scores on these samples is the bootstrapping distribution, whose spread around the center gives us the confidence interval.</td></tr></tbody></table>



<p>
Most deep RL algorithms often perform better on some tasks and training runs, but aggregate performance metrics can conceal this variability, as shown below. 
</p>



    
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj5r_gWAU9Up1vXocAE8X9mVEsjQR42FEn-CCahp_6xy0Fx4ILyDKIm-kX7IZMQ9dsKmS4iptXJWAAR5dC3jlAVP3_7abzxaMg8Kgz9Jg-Y8l-Fhgz81pLz5uFdpFDwk7-YbOEhe9LPds1UaILP8Yg-79WjOEgQgspRuI46WaxHEY45W8tccFKc51ZnUA=s827" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="332" data-original-width="827" height="256" src="https://blogger.googleusercontent.com/img/a/AVvXsEj5r_gWAU9Up1vXocAE8X9mVEsjQR42FEn-CCahp_6xy0Fx4ILyDKIm-kX7IZMQ9dsKmS4iptXJWAAR5dC3jlAVP3_7abzxaMg8Kgz9Jg-Y8l-Fhgz81pLz5uFdpFDwk7-YbOEhe9LPds1UaILP8Yg-79WjOEgQgspRuI46WaxHEY45W8tccFKc51ZnUA=w640-h256" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Data with varied appearance but identical aggregate statistics. Source: <a href="https://dl.acm.org/doi/10.1145/3025453.3025912">Same Stats, Different Graphs.</a></td></tr></tbody></table>


<p>
Instead, we recommend <a href="http://www.argmin.net/2018/03/26/performance-profiles/">performance profiles</a>, which are typically used for <a href="https://arxiv.org/abs/cs/0102001">comparing solve times</a> of optimization software. These profiles plot the score distribution across all runs and tasks with uncertainty estimates using stratified bootstrap <a href="https://en.wikipedia.org/wiki/Confidence_and_prediction_bands">confidence bands</a>. These plots show the total runs across all tasks that obtain a score above a threshold (?) as a function of the threshold.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj5-ndlBjLrGM93AmJoygLeSSbYv0zzRxPYEva4-_Qtx9j5ylZ-elLgSV86MOFaZycuhhdhSAMfBtbhDiE05wygKFrIM-TqNiC-s6X_G_rbSmbIEQOFtBgDqHzVY4h3hv1hPZQZyNwxMH2HTOO6Jfk7qrJXHONvmM4qxUgzUg_G8c2B8nH79nCT3qMb6Q=s1275" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="754" data-original-width="1275" height="378" src="https://blogger.googleusercontent.com/img/a/AVvXsEj5-ndlBjLrGM93AmJoygLeSSbYv0zzRxPYEva4-_Qtx9j5ylZ-elLgSV86MOFaZycuhhdhSAMfBtbhDiE05wygKFrIM-TqNiC-s6X_G_rbSmbIEQOFtBgDqHzVY4h3hv1hPZQZyNwxMH2HTOO6Jfk7qrJXHONvmM4qxUgzUg_G8c2B8nH79nCT3qMb6Q=w640-h378" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Performance profiles correspond to the <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function#Complementary_cumulative_distribution_function_(tail_distribution)">empirical tail distribution</a> of scores on runs combined across all tasks. Shaded regions show 95% stratified bootstrap confidence bands.</td></tr></tbody></table>



<p>
Such profiles allow for qualitative comparisons at a glance. For example, the curve for one algorithm above another means that one algorithm <a href="https://en.wikipedia.org/wiki/Stochastic_dominance">is better than</a> the other. We can also read any score percentile, e.g., the profiles intersect y = 0.5 (dotted line above) at the median score. Furthermore, the area under the profile corresponds to the mean score.
</p>

<p>
While performance profiles are useful for qualitative comparisons, algorithms rarely outperform other algorithms on all tasks and thus their profiles often intersect, so finer quantitative comparisons require aggregate performance metrics. However, existing metrics have limitations: (1) a single high performing task may dominate the task mean score, while (2) the task median is unaffected by zero scores on nearly half of the tasks and requires a large number of training runs for small statistical uncertainty.  To address the above limitations, we recommend two alternatives based on <a href="https://en.wikipedia.org/wiki/Robust_statistics">robust statistics</a>: the <a href="https://en.wikipedia.org/wiki/Interquartile_mean">interquartile mean</a> (IQM) and the optimality gap, both of which can be read as areas under the performance profile, below. 
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhub3TF6zXB8n95OVgQMFmgqpZGFOgelDYKroLohF_D6HylH7jQvIAJujREC62uuve0E72WTL3KVk96sRWV2qmL0dNH6ndbBEn36Rw9Ipqcw67R8lSQAgl0Dz1Ow4jForkYOd6OGvp5vaVIbrjF1AIZ27Zty3TytoHAry9A2lBGCEYzuObGnZxfxsd1fg=s823" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="589" data-original-width="823" height="286" src="https://blogger.googleusercontent.com/img/a/AVvXsEhub3TF6zXB8n95OVgQMFmgqpZGFOgelDYKroLohF_D6HylH7jQvIAJujREC62uuve0E72WTL3KVk96sRWV2qmL0dNH6ndbBEn36Rw9Ipqcw67R8lSQAgl0Dz1Ow4jForkYOd6OGvp5vaVIbrjF1AIZ27Zty3TytoHAry9A2lBGCEYzuObGnZxfxsd1fg=w400-h286" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">IQM (red) corresponds to the area under the performance profile, shown in blue, between the 25 and 75 percentile scores on the x-axis. Optimality gap (yellow) corresponds to the area between the profile and horizontal line at y = 1 (human performance), for scores less than 1.</td></tr></tbody></table>



<p>
As an alternative to median and mean, <a href="https://araffin.github.io/post/rliable/">IQM corresponds to</a> the mean score of the middle 50% of the runs combined across all tasks. It is more robust to outliers than mean, a better indicator of overall performance than median, and results in smaller CIs, and so, requires fewer runs to claim improvements. Another alternative to mean, optimality gap measures how far an algorithm is from optimal performance. 
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiQopOFqAXUMe7IK6MxF3UpJmcmCMxUjNI6IpTh9Y5tgw5MUm_8tQzUxCR2_L_QKfTSeH-zZ0T50xclA9dLbxGwQDA7UIMnOB2UXFucIPfiUszSQECvUbdjlhSw_87laU1vDSqB4A_GUn6gdEELIxhapJ7QMIoIjWZkleLe4Gd9ViO73KPX9nMlMayNpA=s1600" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="600" data-original-width="1600" height="240" src="https://blogger.googleusercontent.com/img/a/AVvXsEiQopOFqAXUMe7IK6MxF3UpJmcmCMxUjNI6IpTh9Y5tgw5MUm_8tQzUxCR2_L_QKfTSeH-zZ0T50xclA9dLbxGwQDA7UIMnOB2UXFucIPfiUszSQECvUbdjlhSw_87laU1vDSqB4A_GUn6gdEELIxhapJ7QMIoIjWZkleLe4Gd9ViO73KPX9nMlMayNpA=w640-h240" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">IQM discards the lowest 25% and highest 25% of the combined scores (colored balls) and computes the mean of the remaining 50% scores.</td></tr></tbody></table>

<p>

For directly comparing two algorithms, another metric to consider is the average probability of improvement, which describes how <a href="https://araffin.github.io/post/rliable/">likely an improvement over baseline is</a>, regardless of its size. This metric is computed using the <a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test">Mann-Whitney U-statistic</a>, averaged across tasks.
</p>


<p>
<b>Re-evaluating Evaluation</b>
<br />
Using the above tools for evaluation, we revisit performance evaluations of existing algorithms on widely used RL benchmarks, revealing inconsistencies in prior evaluation. For example, in the <a href="https://arxiv.org/abs/1207.4708">Arcade Learning Environment</a> (ALE), a widely recognized RL benchmark, the performance ranking of algorithms changes depending on the choice of aggregate metric. Since performance profiles capture the full picture, they often illustrate why such inconsistencies exist. 


</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjYao7gqPWhvLrPHngLHa-ZQn3UEQY5oQf3rN5MmhhQS5uZV9olWr8Xwb8oAFvbX4rLMVcyn2Xbaw__pKohd3ZcWaERAN4VkZ-nTRZ2B5tAmYuaF853fNKPId3O_LYIKjinbHO8lxZOT2ZJYgx_JvnZ4PRIKEvKocCFBMSufh0o3QwEhUAH4UMYsmXWRg=s680" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="294" data-original-width="680" height="276" src="https://blogger.googleusercontent.com/img/a/AVvXsEjYao7gqPWhvLrPHngLHa-ZQn3UEQY5oQf3rN5MmhhQS5uZV9olWr8Xwb8oAFvbX4rLMVcyn2Xbaw__pKohd3ZcWaERAN4VkZ-nTRZ2B5tAmYuaF853fNKPId3O_LYIKjinbHO8lxZOT2ZJYgx_JvnZ4PRIKEvKocCFBMSufh0o3QwEhUAH4UMYsmXWRg=w640-h276" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Median (<b>left</b>) and IQM (<b>right</b>) human normalized scores on the ALE as a function of the number of environment frames seen during training. IQM results in significantly smaller CIs than median scores.</td></tr></tbody></table>



<p>
On <a href="https://github.com/deepmind/dm_control">DM Control</a>, a popular continuous control benchmark, there are large overlaps in 95% CIs of mean normalized scores for most algorithms.
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgoDdiFCyskMHZZHrvhrx1Uwq1RZQtr4ImdLOcD6WJmTf0-yuo4r2eeHEo6ZpUzvGfRS0tyc3Rf1gmWK3GXzkMAyKJyZDjRZ2tYdRIQzKvZFcxsp-aUJtX47R2p9-g2wgLOFqNkR_6i-fxvpoUp4RHP3Entb4HlDLOMWjOj7lVA06_oQKlcKhhKPSrYYQ=s1159" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="430" data-original-width="1159" height="238" src="https://blogger.googleusercontent.com/img/a/AVvXsEgoDdiFCyskMHZZHrvhrx1Uwq1RZQtr4ImdLOcD6WJmTf0-yuo4r2eeHEo6ZpUzvGfRS0tyc3Rf1gmWK3GXzkMAyKJyZDjRZ2tYdRIQzKvZFcxsp-aUJtX47R2p9-g2wgLOFqNkR_6i-fxvpoUp4RHP3Entb4HlDLOMWjOj7lVA06_oQKlcKhhKPSrYYQ=w640-h238" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">DM Control Suite results, averaged across six tasks, on the 100k and 500k step benchmark. Since scores are normalized using maximum performance, mean scores correspond to one minus the optimality gap. The ordering of the algorithms is based on their claimed relative performance &#8212; all algorithms except Dreamer claimed improvement over at least one algorithm placed below them. Shaded regions show 95% CIs.</td></tr></tbody></table>



<p>
Finally, on <a href="https://openai.com/blog/procgen-benchmark/">Procgen</a>, a benchmark for evaluating <a href="https://ai.googleblog.com/2021/09/improving-generalization-in.html">generalization in RL</a>, the average probability of improvement shows that some claimed improvements are only 50-70% likely, suggesting that some reported improvements could be spurious.
</p>


    

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEg5xeMyvQsANks82WbfH2LdD9TZsscjgNHiFN9FHimFXpSzdjuH-5ppOXwe9HW4rpoafVjdvU28VFb6VtVQqvjYT2credILikAoR4CFnRN8JdJNUihNDcSNTRPy8YwXukWi2BwoRX68ABK7rTW9J6LsN2i4hE5NwRPhgdUwtddeXEh6hyACOpV7vZHhug=s811" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="386" data-original-width="811" height="304" src="https://blogger.googleusercontent.com/img/a/AVvXsEg5xeMyvQsANks82WbfH2LdD9TZsscjgNHiFN9FHimFXpSzdjuH-5ppOXwe9HW4rpoafVjdvU28VFb6VtVQqvjYT2credILikAoR4CFnRN8JdJNUihNDcSNTRPy8YwXukWi2BwoRX68ABK7rTW9J6LsN2i4hE5NwRPhgdUwtddeXEh6hyACOpV7vZHhug=w640-h304" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Each row shows the probability that the algorithm X on the left outperforms algorithm Y on the right, given that X was claimed to be better than Y. Shaded region denotes 95% stratified bootstrap CIs.</td></tr></tbody></table>



<p>
<b>Conclusion</b>
<br />
Our findings on widely-used deep RL benchmarks show that statistical issues can have a large influence on previously reported results. In this work, we take a fresh look at evaluation to improve the interpretation of reported results and standardize experimental reporting. We&#8217;d like to emphasize the importance of published papers providing results for all runs to allow for future statistical analyses. To build confidence in your results, please check out our open-source library <a href="https://github.com/google-research/rliable">RLiable</a> and the <a href="https://colab.research.google.com/drive/1a0pSD-1tWhMmeJeeoyZM1A-HCW3yf1xR?usp=sharing">quickstart colab</a>. 
</p>


<p>
<b><em>Acknowledgments</em></b>
<br />
<em>This work was done in collaboration with Max Schwarzer, Aaron Courville and Marc G. Bellemare. We&#8217;d like to thank Tom Small for an animated figure used in this post.  We are also grateful for feedback by several members of the Google Research, Brain Team and DeepMind.</em>
</p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Posted by Rishabh Agarwal, Research Scientist and Pablo Samuel Castro, Staff Software Engineer, Google Research, Brain Team </span> 
           <p> <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement learning</a> (RL) is an area of <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> that focuses on learning from experiences to solve decision making tasks. While the field of RL has made great progress, resulting in impressive empirical results on complex tasks, such as <a href="https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html">playing video games</a>, <a href="http://rdcu.be/cbBRc">flying stratospheric balloons</a> and <a href="https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html">designing hardware chips</a>, it is <a href="https://arxiv.org/abs/1709.06560">becoming</a> <a href="https://arxiv.org/abs/1806.08295">increasingly</a> <a href="https://arxiv.org/abs/1803.07055">apparent</a> that the current standards for empirical evaluation might give a false sense of fast scientific progress while slowing it down. </p> 
           <p> To that end, in <a href="https://agarwl.github.io/rliable">Deep RL at the Edge of the Statistical Precipice</a>, accepted as an <a href="https://openreview.net/forum?id=uqv8-U4lKBe">oral presentation</a> at <a href="https://openreview.net/group?id=NeurIPS.cc/2021/Conference">NeurIPS 2021</a>, we discuss how <a href="https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty">statistical uncertainty</a> of results needs to be considered, especially when using only a few training runs, in order for evaluation in deep RL to be reliable. Specifically, the predominant practice of reporting <a href="https://en.wikipedia.org/wiki/Point_estimation">point estimates</a> ignores this uncertainty and hinders reproducibility of results. Related to this, tables with per-task scores, as are commonly reported, can be overwhelming beyond a few tasks and often omit standard deviations. Furthermore, simple performance metrics like the mean can be dominated by a few outlier tasks, while the median score would remain unaffected even if up to half of the tasks had performance scores of zero. Thus, to increase the field's confidence in reported results with a handful of runs, we propose various statistical tools, including stratified bootstrap confidence intervals, performance profiles, and better metrics, such as interquartile mean and probability of improvement. To help researchers incorporate these tools, we also release an easy-to-use Python library <a href="https://github.com/google-research/rliable">RLiable</a> with a <a href="https://colab.research.google.com/drive/1a0pSD-1tWhMmeJeeoyZM1A-HCW3yf1xR?usp=sharing">quickstart colab</a>. </p> 
           <p> <b>Statistical Uncertainty in RL Evaluation</b> <br> Empirical research in RL relies on evaluating performance on a diverse suite of tasks, such as <a href="https://www.google.com/search?q=Atari+2600+games">Atari 2600 video games</a>, to assess progress. Published results on deep RL benchmarks typically compare <a href="https://en.wikipedia.org/wiki/Point_estimation">point estimates</a> of the mean and median scores aggregated across tasks. These scores are typically relative to some defined baseline and optimal performance (e.g., random agent and average human performance on Atari games, respectively) so as to make scores comparable across different tasks. </p> 
           <p> In most RL experiments, there is randomness in the scores obtained from different training runs, so reporting only point estimates does not reveal whether similar results would be obtained with new independent runs. A small number of training runs, coupled with the <a href="https://arxiv.org/abs/1912.05663">high</a> <a href="https://arxiv.org/abs/1904.06312">variability</a> <a href="https://arxiv.org/abs/1909.03772">in</a> performance of deep RL algorithms, often leads to large statistical uncertainty in such point estimates. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEigMVfUgUJ35fOsr0HsN5J7mW6n18c3JuynEJFdxsvIHGm3zFmCtpTumI3PxlQHXF2t_zgoyUBnmQKmiS6xVtyhRwFLO1SynUVQiEN0t2KbYZuigRs42QMMcy1kRFc6HJGpft-HHS_sVUSG65R853eHvBy7wyTDddaRePGh-_YKgCR4yfnhmhEwYjjp8w=s1200" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="740" data-original-width="1200" height="394" src="https://blogger.googleusercontent.com/img/a/AVvXsEigMVfUgUJ35fOsr0HsN5J7mW6n18c3JuynEJFdxsvIHGm3zFmCtpTumI3PxlQHXF2t_zgoyUBnmQKmiS6xVtyhRwFLO1SynUVQiEN0t2KbYZuigRs42QMMcy1kRFc6HJGpft-HHS_sVUSG65R853eHvBy7wyTDddaRePGh-_YKgCR4yfnhmhEwYjjp8w=w640-h394" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">The distribution of median human normalized scores on the <a href="https://arxiv.org/abs/1903.00374">Atari 100k</a> benchmark, which contains 26 games, for five recently published algorithms, <a href="https://arxiv.org/abs/1906.05243">DER</a>, <a href="https://arxiv.org/abs/2003.10181">OTR</a>, <a href="https://arxiv.org/abs/2004.04136">CURL</a>, two variants of <a href="https://arxiv.org/abs/2004.13649">DrQ</a>, and <a href="https://arxiv.org/abs/2007.05929">SPR</a>. The reported point estimates of median scores based on a few runs in publications, as shown by dashed lines, do not provide information about the variability in median scores and typically overestimate (e.g., CURL, SPR, DrQ) or underestimate (e.g., DER) the expected median, which can result in erroneous conclusions.</td>
             </tr>
            </tbody>
           </table> 
           <p> As benchmarks become increasingly more complex, evaluating more than a few runs will be increasingly demanding due to the increased compute and data needed to solve such tasks. For example, five runs on 50 Atari games for 200 million frames takes 1000+ GPU days. Thus, evaluating more runs is not a feasible solution for reducing statistical uncertainty on computationally demanding benchmarks. While <a href="https://arxiv.org/abs/1709.06560">prior</a> <a href="https://arxiv.org/abs/1904.06979">work</a> has recommended <a href="https://en.wikipedia.org/wiki/Statistical_significance">statistical significance</a> tests as a solution, such tests are <a href="https://www.nature.com/articles/d41586-019-00857-9">dichotomous in nature</a> (either significant or not significant), so they often lack the granularity needed to yield meaningful insights and are <a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913">widely misinterpreted</a>. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhcuKWT01-AvyHEUoCc-_NGaeHtFnCnD1uoeU9Nn-NCrMW6jwIoZ4sBkSIHcK9GJy5wKeVmG4xJdkXv1vZg17_yIkc4VcrxzzJ1C7-SuqtaqOqcKktAPGpGBiMrTgKAWN_8nE61Sw9LgA71u_R8R9OG-NUb_bw6LxyaygyfK88mHpOEEuWrEQsoL06v3w=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="846" data-original-width="1999" height="270" src="https://blogger.googleusercontent.com/img/a/AVvXsEhcuKWT01-AvyHEUoCc-_NGaeHtFnCnD1uoeU9Nn-NCrMW6jwIoZ4sBkSIHcK9GJy5wKeVmG4xJdkXv1vZg17_yIkc4VcrxzzJ1C7-SuqtaqOqcKktAPGpGBiMrTgKAWN_8nE61Sw9LgA71u_R8R9OG-NUb_bw6LxyaygyfK88mHpOEEuWrEQsoL06v3w=w640-h270" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Number of runs in RL papers over the years. Beginning with the <a href="https://arxiv.org/abs/1207.4708">Arcade Learning Environment</a> (ALE), the shift toward computationally-demanding benchmarks has led to the practice of evaluating only a handful of runs per task, increasing the statistical uncertainty in point estimates. </td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Tools for Reliable Evaluation</b> <br> Any aggregate metric based on a finite number of runs is a <a href="https://en.wikipedia.org/wiki/Random_variable">random variable</a>, so to take this into account, we advocate for reporting <em>stratified <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrap</a> <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence intervals</a></em> (CIs), which predict the likely values of aggregate metrics if the same experiment were repeated with different runs. These CIs allow us to understand the statistical uncertainty and reproducibility of results. Such CIs use the scores on combined runs across tasks. For example, evaluating 3 runs each on <a href="https://arxiv.org/abs/1903.00374">Atari 100k</a>, which contains 26 tasks, results in 78 sample scores for uncertainty estimation. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgLuKXv3mpjh9fpzosblqurTrARuBshMYmX0SXPjvFUxSujQ6j_cm7ljLzRpwkPuwseysygp9f_bwtusH9Av-hs-32UP5VZmscajQOIV6wE_o4bDjzOpl0NLBTN2SN2fhxV1jHljDbFFhIIZx1z_e4uo-gir1BjlCv_5Jkk6cKCRr-70TppuoCfqiOKNg=s1450" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1000" data-original-width="1450" height="442" src="https://blogger.googleusercontent.com/img/a/AVvXsEgLuKXv3mpjh9fpzosblqurTrARuBshMYmX0SXPjvFUxSujQ6j_cm7ljLzRpwkPuwseysygp9f_bwtusH9Av-hs-32UP5VZmscajQOIV6wE_o4bDjzOpl0NLBTN2SN2fhxV1jHljDbFFhIIZx1z_e4uo-gir1BjlCv_5Jkk6cKCRr-70TppuoCfqiOKNg=w640-h442" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">In each task, colored balls denote scores on different runs. To compute statified bootstrap CIs using the <a href="https://www.uvm.edu/~statdhtx/StatPages/Randomization%20Tests/ResamplingWithR/BootstMeans/bootstrapping_means.html">percentile method</a>, bootstrap samples are created by randomly sampling scores <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)#Replacement_of_selected_units">with replacement</a> proportionately from each task. Then, the distribution of aggregate scores on these samples is the bootstrapping distribution, whose spread around the center gives us the confidence interval.</td>
             </tr>
            </tbody>
           </table> 
           <p> Most deep RL algorithms often perform better on some tasks and training runs, but aggregate performance metrics can conceal this variability, as shown below. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj5r_gWAU9Up1vXocAE8X9mVEsjQR42FEn-CCahp_6xy0Fx4ILyDKIm-kX7IZMQ9dsKmS4iptXJWAAR5dC3jlAVP3_7abzxaMg8Kgz9Jg-Y8l-Fhgz81pLz5uFdpFDwk7-YbOEhe9LPds1UaILP8Yg-79WjOEgQgspRuI46WaxHEY45W8tccFKc51ZnUA=s827" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="332" data-original-width="827" height="256" src="https://blogger.googleusercontent.com/img/a/AVvXsEj5r_gWAU9Up1vXocAE8X9mVEsjQR42FEn-CCahp_6xy0Fx4ILyDKIm-kX7IZMQ9dsKmS4iptXJWAAR5dC3jlAVP3_7abzxaMg8Kgz9Jg-Y8l-Fhgz81pLz5uFdpFDwk7-YbOEhe9LPds1UaILP8Yg-79WjOEgQgspRuI46WaxHEY45W8tccFKc51ZnUA=w640-h256" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Data with varied appearance but identical aggregate statistics. Source: <a href="https://dl.acm.org/doi/10.1145/3025453.3025912">Same Stats, Different Graphs.</a></td>
             </tr>
            </tbody>
           </table> 
           <p> Instead, we recommend <a href="http://www.argmin.net/2018/03/26/performance-profiles/">performance profiles</a>, which are typically used for <a href="https://arxiv.org/abs/cs/0102001">comparing solve times</a> of optimization software. These profiles plot the score distribution across all runs and tasks with uncertainty estimates using stratified bootstrap <a href="https://en.wikipedia.org/wiki/Confidence_and_prediction_bands">confidence bands</a>. These plots show the total runs across all tasks that obtain a score above a threshold (?) as a function of the threshold. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj5-ndlBjLrGM93AmJoygLeSSbYv0zzRxPYEva4-_Qtx9j5ylZ-elLgSV86MOFaZycuhhdhSAMfBtbhDiE05wygKFrIM-TqNiC-s6X_G_rbSmbIEQOFtBgDqHzVY4h3hv1hPZQZyNwxMH2HTOO6Jfk7qrJXHONvmM4qxUgzUg_G8c2B8nH79nCT3qMb6Q=s1275" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="754" data-original-width="1275" height="378" src="https://blogger.googleusercontent.com/img/a/AVvXsEj5-ndlBjLrGM93AmJoygLeSSbYv0zzRxPYEva4-_Qtx9j5ylZ-elLgSV86MOFaZycuhhdhSAMfBtbhDiE05wygKFrIM-TqNiC-s6X_G_rbSmbIEQOFtBgDqHzVY4h3hv1hPZQZyNwxMH2HTOO6Jfk7qrJXHONvmM4qxUgzUg_G8c2B8nH79nCT3qMb6Q=w640-h378" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Performance profiles correspond to the <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function#Complementary_cumulative_distribution_function_(tail_distribution)">empirical tail distribution</a> of scores on runs combined across all tasks. Shaded regions show 95% stratified bootstrap confidence bands.</td>
             </tr>
            </tbody>
           </table> 
           <p> Such profiles allow for qualitative comparisons at a glance. For example, the curve for one algorithm above another means that one algorithm <a href="https://en.wikipedia.org/wiki/Stochastic_dominance">is better than</a> the other. We can also read any score percentile, e.g., the profiles intersect y = 0.5 (dotted line above) at the median score. Furthermore, the area under the profile corresponds to the mean score. </p> 
           <p> While performance profiles are useful for qualitative comparisons, algorithms rarely outperform other algorithms on all tasks and thus their profiles often intersect, so finer quantitative comparisons require aggregate performance metrics. However, existing metrics have limitations: (1) a single high performing task may dominate the task mean score, while (2) the task median is unaffected by zero scores on nearly half of the tasks and requires a large number of training runs for small statistical uncertainty. To address the above limitations, we recommend two alternatives based on <a href="https://en.wikipedia.org/wiki/Robust_statistics">robust statistics</a>: the <a href="https://en.wikipedia.org/wiki/Interquartile_mean">interquartile mean</a> (IQM) and the optimality gap, both of which can be read as areas under the performance profile, below. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhub3TF6zXB8n95OVgQMFmgqpZGFOgelDYKroLohF_D6HylH7jQvIAJujREC62uuve0E72WTL3KVk96sRWV2qmL0dNH6ndbBEn36Rw9Ipqcw67R8lSQAgl0Dz1Ow4jForkYOd6OGvp5vaVIbrjF1AIZ27Zty3TytoHAry9A2lBGCEYzuObGnZxfxsd1fg=s823" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="589" data-original-width="823" height="286" src="https://blogger.googleusercontent.com/img/a/AVvXsEhub3TF6zXB8n95OVgQMFmgqpZGFOgelDYKroLohF_D6HylH7jQvIAJujREC62uuve0E72WTL3KVk96sRWV2qmL0dNH6ndbBEn36Rw9Ipqcw67R8lSQAgl0Dz1Ow4jForkYOd6OGvp5vaVIbrjF1AIZ27Zty3TytoHAry9A2lBGCEYzuObGnZxfxsd1fg=w400-h286" width="400"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">IQM (red) corresponds to the area under the performance profile, shown in blue, between the 25 and 75 percentile scores on the x-axis. Optimality gap (yellow) corresponds to the area between the profile and horizontal line at y = 1 (human performance), for scores less than 1.</td>
             </tr>
            </tbody>
           </table> 
           <p> As an alternative to median and mean, <a href="https://araffin.github.io/post/rliable/">IQM corresponds to</a> the mean score of the middle 50% of the runs combined across all tasks. It is more robust to outliers than mean, a better indicator of overall performance than median, and results in smaller CIs, and so, requires fewer runs to claim improvements. Another alternative to mean, optimality gap measures how far an algorithm is from optimal performance. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiQopOFqAXUMe7IK6MxF3UpJmcmCMxUjNI6IpTh9Y5tgw5MUm_8tQzUxCR2_L_QKfTSeH-zZ0T50xclA9dLbxGwQDA7UIMnOB2UXFucIPfiUszSQECvUbdjlhSw_87laU1vDSqB4A_GUn6gdEELIxhapJ7QMIoIjWZkleLe4Gd9ViO73KPX9nMlMayNpA=s1600" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="600" data-original-width="1600" height="240" src="https://blogger.googleusercontent.com/img/a/AVvXsEiQopOFqAXUMe7IK6MxF3UpJmcmCMxUjNI6IpTh9Y5tgw5MUm_8tQzUxCR2_L_QKfTSeH-zZ0T50xclA9dLbxGwQDA7UIMnOB2UXFucIPfiUszSQECvUbdjlhSw_87laU1vDSqB4A_GUn6gdEELIxhapJ7QMIoIjWZkleLe4Gd9ViO73KPX9nMlMayNpA=w640-h240" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">IQM discards the lowest 25% and highest 25% of the combined scores (colored balls) and computes the mean of the remaining 50% scores.</td>
             </tr>
            </tbody>
           </table> 
           <p> For directly comparing two algorithms, another metric to consider is the average probability of improvement, which describes how <a href="https://araffin.github.io/post/rliable/">likely an improvement over baseline is</a>, regardless of its size. This metric is computed using the <a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test">Mann-Whitney U-statistic</a>, averaged across tasks. </p> 
           <p> <b>Re-evaluating Evaluation</b> <br> Using the above tools for evaluation, we revisit performance evaluations of existing algorithms on widely used RL benchmarks, revealing inconsistencies in prior evaluation. For example, in the <a href="https://arxiv.org/abs/1207.4708">Arcade Learning Environment</a> (ALE), a widely recognized RL benchmark, the performance ranking of algorithms changes depending on the choice of aggregate metric. Since performance profiles capture the full picture, they often illustrate why such inconsistencies exist. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjYao7gqPWhvLrPHngLHa-ZQn3UEQY5oQf3rN5MmhhQS5uZV9olWr8Xwb8oAFvbX4rLMVcyn2Xbaw__pKohd3ZcWaERAN4VkZ-nTRZ2B5tAmYuaF853fNKPId3O_LYIKjinbHO8lxZOT2ZJYgx_JvnZ4PRIKEvKocCFBMSufh0o3QwEhUAH4UMYsmXWRg=s680" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="294" data-original-width="680" height="276" src="https://blogger.googleusercontent.com/img/a/AVvXsEjYao7gqPWhvLrPHngLHa-ZQn3UEQY5oQf3rN5MmhhQS5uZV9olWr8Xwb8oAFvbX4rLMVcyn2Xbaw__pKohd3ZcWaERAN4VkZ-nTRZ2B5tAmYuaF853fNKPId3O_LYIKjinbHO8lxZOT2ZJYgx_JvnZ4PRIKEvKocCFBMSufh0o3QwEhUAH4UMYsmXWRg=w640-h276" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Median (<b>left</b>) and IQM (<b>right</b>) human normalized scores on the ALE as a function of the number of environment frames seen during training. IQM results in significantly smaller CIs than median scores.</td>
             </tr>
            </tbody>
           </table> 
           <p> On <a href="https://github.com/deepmind/dm_control">DM Control</a>, a popular continuous control benchmark, there are large overlaps in 95% CIs of mean normalized scores for most algorithms. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgoDdiFCyskMHZZHrvhrx1Uwq1RZQtr4ImdLOcD6WJmTf0-yuo4r2eeHEo6ZpUzvGfRS0tyc3Rf1gmWK3GXzkMAyKJyZDjRZ2tYdRIQzKvZFcxsp-aUJtX47R2p9-g2wgLOFqNkR_6i-fxvpoUp4RHP3Entb4HlDLOMWjOj7lVA06_oQKlcKhhKPSrYYQ=s1159" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="430" data-original-width="1159" height="238" src="https://blogger.googleusercontent.com/img/a/AVvXsEgoDdiFCyskMHZZHrvhrx1Uwq1RZQtr4ImdLOcD6WJmTf0-yuo4r2eeHEo6ZpUzvGfRS0tyc3Rf1gmWK3GXzkMAyKJyZDjRZ2tYdRIQzKvZFcxsp-aUJtX47R2p9-g2wgLOFqNkR_6i-fxvpoUp4RHP3Entb4HlDLOMWjOj7lVA06_oQKlcKhhKPSrYYQ=w640-h238" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">DM Control Suite results, averaged across six tasks, on the 100k and 500k step benchmark. Since scores are normalized using maximum performance, mean scores correspond to one minus the optimality gap. The ordering of the algorithms is based on their claimed relative performance  all algorithms except Dreamer claimed improvement over at least one algorithm placed below them. Shaded regions show 95% CIs.</td>
             </tr>
            </tbody>
           </table> 
           <p> Finally, on <a href="https://openai.com/blog/procgen-benchmark/">Procgen</a>, a benchmark for evaluating <a href="https://ai.googleblog.com/2021/09/improving-generalization-in.html">generalization in RL</a>, the average probability of improvement shows that some claimed improvements are only 50-70% likely, suggesting that some reported improvements could be spurious. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEg5xeMyvQsANks82WbfH2LdD9TZsscjgNHiFN9FHimFXpSzdjuH-5ppOXwe9HW4rpoafVjdvU28VFb6VtVQqvjYT2credILikAoR4CFnRN8JdJNUihNDcSNTRPy8YwXukWi2BwoRX68ABK7rTW9J6LsN2i4hE5NwRPhgdUwtddeXEh6hyACOpV7vZHhug=s811" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="386" data-original-width="811" height="304" src="https://blogger.googleusercontent.com/img/a/AVvXsEg5xeMyvQsANks82WbfH2LdD9TZsscjgNHiFN9FHimFXpSzdjuH-5ppOXwe9HW4rpoafVjdvU28VFb6VtVQqvjYT2credILikAoR4CFnRN8JdJNUihNDcSNTRPy8YwXukWi2BwoRX68ABK7rTW9J6LsN2i4hE5NwRPhgdUwtddeXEh6hyACOpV7vZHhug=w640-h304" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Each row shows the probability that the algorithm X on the left outperforms algorithm Y on the right, given that X was claimed to be better than Y. Shaded region denotes 95% stratified bootstrap CIs.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Conclusion</b> <br> Our findings on widely-used deep RL benchmarks show that statistical issues can have a large influence on previously reported results. In this work, we take a fresh look at evaluation to improve the interpretation of reported results and standardize experimental reporting. Wed like to emphasize the importance of published papers providing results for all runs to allow for future statistical analyses. To build confidence in your results, please check out our open-source library <a href="https://github.com/google-research/rliable">RLiable</a> and the <a href="https://colab.research.google.com/drive/1a0pSD-1tWhMmeJeeoyZM1A-HCW3yf1xR?usp=sharing">quickstart colab</a>. </p> 
           <p> <b><em>Acknowledgments</em></b> <br> <em>This work was done in collaboration with Max Schwarzer, Aaron Courville and Marc G. Bellemare. Wed like to thank Tom Small for an animated figure used in this post. We are also grateful for feedback by several members of the Google Research, Brain Team and DeepMind.</em> </p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:RLiable: Towards Reliable Evaluation &amp; Reporting in Reinforcement Learning&amp;url=http://ai.googleblog.com/2021/11/rliable-towards-reliable-evaluation.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/11/rliable-towards-reliable-evaluation.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="post" data-id="1324022626586622227" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html" itemprop="url" title="MetNet-2: Deep Learning for 12-Hour Precipitation Forecasting"> MetNet-2: Deep Learning for 12-Hour Precipitation Forecasting </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Monday, November 15, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Posted by Nal Kalchbrenner and Lasse Espeholt, Google Research</span>


<p>
Deep learning has successfully been applied to a wide range of important challenges, such as <a href="http://ai.googleblog.com/2021/08/improved-detection-of-elusive-polyps.html">cancer prevention</a> and <a href="http://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html">increasing accessibility</a>. The application of <a href="https://arxiv.org/abs/2005.04988">deep learning models to weather forecasts</a> can be relevant to people on a day-to-day basis, from helping people plan their day to managing food production, transportation systems, or the energy grid. Weather forecasts typically rely on traditional physics-based techniques powered by the world&#8217;s largest supercomputers. Such methods are constrained by <a href="https://www.nature.com/articles/nature14956">high computational requirements</a> and are sensitive to approximations of the physical laws on which they are based. 
</p>

<p>
Deep learning <a href="https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0097">offers a new approach</a> to computing forecasts. Rather than incorporating explicit physical laws, deep learning models learn to predict weather patterns directly from observed data and are able to compute predictions faster than physics-based techniques. These approaches also have the potential to increase the frequency, scope, and accuracy of the predicted forecasts.
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhpJxLN_5CuSyz_gt-xrDIoLyi1HQ0PdAYHQgomGhbABA-qbDAcevBYsq0XcgpozNP3e_UWvwmgoUOC6pxv5vjKnDM4Wdn8zy6GYv3jVe3iccXNKh_-x4V0RAFugPfroW3FHmFcuQmLTzaNZGDultf0a72BrweNciUhWKSzYcgl1buu0gyg7BrV7k--WA=s1600" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="600" data-original-width="1600" height="240" src="https://blogger.googleusercontent.com/img/a/AVvXsEhpJxLN_5CuSyz_gt-xrDIoLyi1HQ0PdAYHQgomGhbABA-qbDAcevBYsq0XcgpozNP3e_UWvwmgoUOC6pxv5vjKnDM4Wdn8zy6GYv3jVe3iccXNKh_-x4V0RAFugPfroW3FHmFcuQmLTzaNZGDultf0a72BrweNciUhWKSzYcgl1buu0gyg7BrV7k--WA=w640-h240" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Illustration of the computation through MetNet-2. As the computation progresses, the network processes an ever larger context from the input and makes a probabilistic forecast of the likely future weather conditions.</td></tr></tbody></table>




<p>
Within weather forecasting, deep learning techniques have shown particular promise for <a href="https://en.wikipedia.org/wiki/Nowcasting_(meteorology)">nowcasting</a> &#8212; i.e., predicting weather up to 2-6 hours ahead. Previous work has focused on <a href="https://ai.googleblog.com/2020/01/using-machine-learning-to-nowcast.html">using direct neural network models for weather data</a>, extending neural forecasts from 0 to 8 hours with the <a href="https://ai.googleblog.com/2020/03/a-neural-weather-model-for-eight-hour.html">MetNet architecture</a>, generating <a href="https://deepmind.com/blog/article/nowcasting">continuations of radar data for up to 90 minutes ahead</a>, and <a href="http://raincheck.karyk.com/rain-check">interpreting the weather information learned</a> by these neural networks. Still, there is an opportunity for deep learning to extend improvements to longer-range forecasts. 
</p>

<p>
To that end, in &#8220;<a href="https://arxiv.org/abs/2111.07470">Skillful Twelve Hour Precipitation Forecasts Using Large Context Neural Networks</a>&#8221;, we push the forecasting boundaries of our neural precipitation model to 12 hour predictions while keeping a spatial resolution of 1 km and a time resolution of 2 minutes. By quadrupling the input context, adopting a richer weather input state, and extending the architecture to capture longer-range spatial dependencies, MetNet-2 substantially improves on the performance of its predecessor, MetNet. Compared to physics-based models, MetNet-2 outperforms the state-of-the-art <a href="https://www.essoar.org/pdfjs/10.1002/essoar.10501462.1">HREF ensemble model</a> for weather forecasts up to 12 hours ahead.
</p>

<p>
<b>MetNet-2 Features and Architecture</b>
<br>
Neural weather models like MetNet-2 map observations of the Earth to the probability of weather events, such as the likelihood of rain over a city in the afternoon, of wind gusts reaching 20 knots, or of a sunny day ahead. End-to-end deep learning has the potential to both streamline and increase quality by directly connecting a system's inputs and outputs. With this in mind, MetNet-2 aims to minimize both the complexity and the total number of steps involved in creating a forecast. 
</p>

<p>
The inputs to MetNet-2 include the radar and satellite images also used in MetNet. To capture a more comprehensive snapshot of the atmosphere with information such as temperature, humidity, and wind direction &#8212; critical for longer forecasts of up to 12 hours &#8212; MetNet-2 also uses the pre-processed starting state used in physical models as a proxy for this additional weather information. The radar-based measures of precipitation (<a href="https://www.nssl.noaa.gov/projects/mrms/">MRMS</a>) serve as the ground truth (i.e., what we are trying to predict) that we use in training to optimize MetNet-2&#8217;s parameters. 
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEidl9U_aE6zh6zn8niiFA1H2Cz-ftmPFXV6Bi8R8u2bo2vWMc1it0gUFZb-ru5lgKF8f_zMmfooQPnYDKfXX-0L8Vz4f-216vZV79_6aTJhuGmWH8i9edO9clpxa_X3e7wjDhNRrF6oAMhKMrcNgF10p7aRpI98CVhEqz8BJ8wbgsnIbYo8nrWbzFpE0Q=s512" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="360" data-original-width="512" height="450" src="https://blogger.googleusercontent.com/img/a/AVvXsEidl9U_aE6zh6zn8niiFA1H2Cz-ftmPFXV6Bi8R8u2bo2vWMc1it0gUFZb-ru5lgKF8f_zMmfooQPnYDKfXX-0L8Vz4f-216vZV79_6aTJhuGmWH8i9edO9clpxa_X3e7wjDhNRrF6oAMhKMrcNgF10p7aRpI98CVhEqz8BJ8wbgsnIbYo8nrWbzFpE0Q=w640-h450" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Example ground truth image: Instantaneous precipitation (mm/hr) based on radar (MRMS) capturing a 12 hours-long progression.</td></tr></tbody></table>


<p>
MetNet-2&#8217;s probabilistic forecasts can be viewed as averaging all possible future weather conditions weighted by how likely they are. Due to its probabilistic nature, MetNet-2 can be likened to physics-based ensemble models, which average some number of future weather conditions predicted by a variety of physics-based models. One notable difference between these two approaches is the duration of the core part of the computation: ensemble models take ~1 hour, whereas MetNet-2 takes ~1 second.
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjsWtg6q7Py4Ux0IrGxNMdI7alAnh-p9z8Nv4kXdS34o3uQef5qZ6KensNDGZTgzGAxhSxHVygTy9tMdsLSWTAjI3EwvDOubiYRBhDgYrk4_l07MdzugA7yzTjBnGp3oO33lo0PooPfu5MkajKBJQ0YxRMWuc-UvAYVQox78Pf33HCW4LpoLS8CEqErOQ=s793" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="127" data-original-width="793" height="102" src="https://blogger.googleusercontent.com/img/a/AVvXsEjsWtg6q7Py4Ux0IrGxNMdI7alAnh-p9z8Nv4kXdS34o3uQef5qZ6KensNDGZTgzGAxhSxHVygTy9tMdsLSWTAjI3EwvDOubiYRBhDgYrk4_l07MdzugA7yzTjBnGp3oO33lo0PooPfu5MkajKBJQ0YxRMWuc-UvAYVQox78Pf33HCW4LpoLS8CEqErOQ=w640-h102" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Steps in a MetNet-2 forecast and in a physics-based ensemble.</td></tr></tbody></table>


<p>
One of the main challenges that MetNet-2 must overcome to make 12 hour long forecasts is capturing a sufficient amount of spatial context in the input images. For each additional forecast hour we include 64 km of context in every direction at the input. This results in an input context of size 2048<sup>2</sup> km<sup>2</sup> &#8212; four times that used in MetNet. In order to process such a large context, MetNet-2 employs model parallelism whereby the model is distributed across 128 cores of a <a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">Cloud TPU v3-128</a>. Due to the size of the input context, MetNet-2 replaces the attentional layers of MetNet with computationally more efficient convolutional layers. But standard convolutional layers have local receptive fields that may fail to capture large spatial contexts, so MetNet-2 uses dilated receptive fields, whose size doubles layer after layer, in order to connect points in the input that are far apart one from the other.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiCoKS5L3j23hcJDcTssiSuKs0e4u6enaUJQmveAEjMRGt0ForeFOQx8hub0r_nlLZ7h4Q1rk3DUucM5vpR6ovAwNtHOYhySoyxzs54ahrBUlcp5tuPeUmLrwq3-M3aXwyVGwO5wfSEa95J_ioZMP5avMy_jf3oAECecMxOZkTR_ZSnvDklsT3aJBffDw=s1674" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="597" data-original-width="1674" height="228" src="https://blogger.googleusercontent.com/img/a/AVvXsEiCoKS5L3j23hcJDcTssiSuKs0e4u6enaUJQmveAEjMRGt0ForeFOQx8hub0r_nlLZ7h4Q1rk3DUucM5vpR6ovAwNtHOYhySoyxzs54ahrBUlcp5tuPeUmLrwq3-M3aXwyVGwO5wfSEa95J_ioZMP5avMy_jf3oAECecMxOZkTR_ZSnvDklsT3aJBffDw=w640-h228" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Example of input spatial context and target area for MetNet-2.</td></tr></tbody></table>


<p>
<b>Results</b>
<br>
Because MetNet-2&#8217;s predictions are probabilistic, the model&#8217;s output is naturally compared with the output of similarly probabilistic ensemble or post-processing models. <a href="https://nomads.ncep.noaa.gov/txt_descriptions/HREF_doc.shtml">HREF</a> is one such state-of-the-art ensemble model for precipitation in the United States, which aggregates ten predictions from five different models, twice a day. We evaluate the forecasts using established metrics, such as the <a href="https://journals.ametsoc.org/view/journals/wefo/15/5/1520-0434_2000_015_0559_dotcrp_2_0_co_2.xml">Continuous Ranked Probability Score</a>, which captures the magnitude of the probabilistic error of a model&#8217;s forecasts relative to the ground truth observations. Despite not performing any physics-based calculations, MetNet-2 is able to outperform HREF up to 12 hours into the future for both low and high levels of precipitation.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgQSVopbZHNl9Bk1HRR4ybO4yEGK8FPxjOr94erKqcc8WGs1yXthNZw6XOPau6UwcnKURAL9IuMg4pj8hyW3foE4sA_3r6mlGNCE1UYEBVag1BPjo-cKsiyT1CsWhobqlISO23PHttCcNvQsKBFqXrHuqbdx2AjdDSGxTFMaXFp_7LEDK9IaTQvMzKrmg=s1298" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="748" data-original-width="1298" height="368" src="https://blogger.googleusercontent.com/img/a/AVvXsEgQSVopbZHNl9Bk1HRR4ybO4yEGK8FPxjOr94erKqcc8WGs1yXthNZw6XOPau6UwcnKURAL9IuMg4pj8hyW3foE4sA_3r6mlGNCE1UYEBVag1BPjo-cKsiyT1CsWhobqlISO23PHttCcNvQsKBFqXrHuqbdx2AjdDSGxTFMaXFp_7LEDK9IaTQvMzKrmg=w640-h368" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Continuous Ranked Probability Score (CRPS; lower is better) for MetNet-2 vs HREF aggregated over a large number of test patches randomly located in the Continental United States.</td></tr></tbody></table>



<p>
<b>Examples of Forecasts</b>
<br>
The following figures provide a selection of forecasts from MetNet-2 compared with the physics-based ensemble HREF and the ground truth MRMS.
</p>





<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEh4deGJPLR0hIronPcWgrfZ37xxyId_5dOtzyLODJJBMS5-aCE2hWfn1CzSkWe8XBUv4pbquK1n9_34I6mphu89OTQn51pqdDgfTwinUwYaP_Wq4zBRYsN5wVsCheRDwFNJNYbHqbu0G8LqrJ4vug1edG4DMKdXFYyE5Nsudys1nARsC_4iBT3BCKFojw=s1984" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="848" data-original-width="1984" height="274" src="https://blogger.googleusercontent.com/img/a/AVvXsEh4deGJPLR0hIronPcWgrfZ37xxyId_5dOtzyLODJJBMS5-aCE2hWfn1CzSkWe8XBUv4pbquK1n9_34I6mphu89OTQn51pqdDgfTwinUwYaP_Wq4zBRYsN5wVsCheRDwFNJNYbHqbu0G8LqrJ4vug1edG4DMKdXFYyE5Nsudys1nARsC_4iBT3BCKFojw=w640-h274" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Probability maps for the cumulative precipitation rate of 1 mm/hr on January 3, 2019 over the Pacific NorthWest. The maps are shown for each hour of lead time from 1 to 12. <b>Left</b>: Ground truth, source MRMS. <b>Center</b>: Probability map as predicted by MetNet-2 . <b>Right</b>: Probability map as predicted by HREF.</td></tr></tbody></table>





<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjAXgO_kTmVS1gyhEXvz2x6gInspa5dpuDJNuYuMVeOJZZrdvzAdZ4ymHXyo0SdxVY-5LhFWglC9jTUf4X8dsihZFlDOxwQrX0ZzGOeUzZTmPSuYo3o99RRUJiWP1N5EjMhxhDdlTMmPCkIpcV859bxIsiqgi9BeOJHRFPfw82IXyJETn9lfGlXcwKWHA=s1984" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="848" data-original-width="1984" height="274" src="https://blogger.googleusercontent.com/img/a/AVvXsEjAXgO_kTmVS1gyhEXvz2x6gInspa5dpuDJNuYuMVeOJZZrdvzAdZ4ymHXyo0SdxVY-5LhFWglC9jTUf4X8dsihZFlDOxwQrX0ZzGOeUzZTmPSuYo3o99RRUJiWP1N5EjMhxhDdlTMmPCkIpcV859bxIsiqgi9BeOJHRFPfw82IXyJETn9lfGlXcwKWHA=w640-h274" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparison of 0.2 mm/hr precipitation on March 30, 2020 over Denver, Colorado. <b>Left</b>: Ground truth, source MRMS. <b>Center:</b> Probability map as predicted by MetNet-2 . <b>Right</b>: Probability map as predicted by HREF.MetNet-2 is able to predict the onset of the storm (called <a href="https://learnweather.com/basic-weather/convective-initiation-mk/">convective initiation</a>) earlier in the forecast than HREF as well as the storm&#8217;s starting location, whereas HREF misses the initiation location, but captures its growth phase well.</td></tr></tbody></table>






<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiK5MfQbEYHpOtdbNYT3d61pwQif1ukIAvrdpIoZM7tA_eXxCmUdbEv02o-gU2JwOT0oIKizB_kvCb6sJw_-qklND6SaaLOXDLVJnlfnMBKY0EWNmowUJNHr8qEirLeHrC1jb4WxSJ0ViqLaEXWqlW58iaMNWQeNi-8yYY8WeksGGpA69g-YPWkT_DGmA=s1984" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="848" data-original-width="1984" height="274" src="https://blogger.googleusercontent.com/img/a/AVvXsEiK5MfQbEYHpOtdbNYT3d61pwQif1ukIAvrdpIoZM7tA_eXxCmUdbEv02o-gU2JwOT0oIKizB_kvCb6sJw_-qklND6SaaLOXDLVJnlfnMBKY0EWNmowUJNHr8qEirLeHrC1jb4WxSJ0ViqLaEXWqlW58iaMNWQeNi-8yYY8WeksGGpA69g-YPWkT_DGmA=w640-h274" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparison of 2 mm/hr precipitation stemming from Hurricane Isaias, an extreme weather event that occurred on August 4, 2020 over the North East coast of the US. <b>Left</b>: Ground truth, source MRMS. <b>Center</b>: Probability map as predicted by MetNet-2. <b>Right</b>: Probability map as predicted by HREF.</td></tr></tbody></table>




<p>
<b>Interpreting What MetNet-2 Learns About Weather</b>
<br>
Because MetNet-2 does not use hand-crafted physical equations, its performance inspires a natural question: What kind of physical relations about the weather does it learn from the data during training? Using <a href="https://arxiv.org/abs/1703.01365">advanced interpretability tools</a>, we further trace the impact of various input features on MetNet-2&#8217;s performance at different forecast timelines. Perhaps the most surprising finding is that MetNet-2 appears to emulate the physics described by <a href="http://www.dca.iag.usp.br/material/ritaynoue/agm5706/METR4424qgtheory.pdf">Quasi-Geostrophic Theory</a>, which is used as an effective approximation of large-scale weather phenomena. MetNet-2 was able to pick up on changes in the atmospheric forces, at the scale of a typical high- or low-pressure system (i.e., the <a href="https://en.wikipedia.org/wiki/Synoptic_scale_meteorology">synoptic scale</a>), that bring about favorable conditions for precipitation, a key tenet of the theory.
</p>

<p>
<b>Conclusion</b>
<br>
MetNet-2 represents a step toward enabling a new modeling paradigm for weather forecasting that does not rely on hand-coding the physics of weather phenomena, but rather embraces end-to-end learning from observations to weather targets and parallel forecasting on low-precision hardware. Yet many challenges remain on the path to fully achieving this goal, including incorporating more raw data about the atmosphere directly (rather than using the pre-processed starting state from physical models), broadening the set of weather phenomena, increasing the lead time horizon to days and weeks, and widening the geographic coverage beyond the United States. 
</p>


<p>
<b>Acknowledgements</b>
<br>
<em>Shreya Agrawal, Casper Snderby, Manoj Kumar, Jonathan Heek, Carla Bromberg, Cenk Gazen, Jason Hickey, Aaron Bell, Marcin Andrychowicz, Amy McGovern, Rob Carver, Stephan Hoyer, Zack Ontiveros, Lak Lakshmanan, David McPeek, Ian Gonzalez, Claudio Martella, Samier Merchant, Fred Zyda, Daniel Furrer and Tom Small.</em>
</p><p><em><br /></em></p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Posted by Nal Kalchbrenner and Lasse Espeholt, Google Research</span> 
           <p> Deep learning has successfully been applied to a wide range of important challenges, such as <a href="http://ai.googleblog.com/2021/08/improved-detection-of-elusive-polyps.html">cancer prevention</a> and <a href="http://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html">increasing accessibility</a>. The application of <a href="https://arxiv.org/abs/2005.04988">deep learning models to weather forecasts</a> can be relevant to people on a day-to-day basis, from helping people plan their day to managing food production, transportation systems, or the energy grid. Weather forecasts typically rely on traditional physics-based techniques powered by the worlds largest supercomputers. Such methods are constrained by <a href="https://www.nature.com/articles/nature14956">high computational requirements</a> and are sensitive to approximations of the physical laws on which they are based. </p> 
           <p> Deep learning <a href="https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0097">offers a new approach</a> to computing forecasts. Rather than incorporating explicit physical laws, deep learning models learn to predict weather patterns directly from observed data and are able to compute predictions faster than physics-based techniques. These approaches also have the potential to increase the frequency, scope, and accuracy of the predicted forecasts. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhpJxLN_5CuSyz_gt-xrDIoLyi1HQ0PdAYHQgomGhbABA-qbDAcevBYsq0XcgpozNP3e_UWvwmgoUOC6pxv5vjKnDM4Wdn8zy6GYv3jVe3iccXNKh_-x4V0RAFugPfroW3FHmFcuQmLTzaNZGDultf0a72BrweNciUhWKSzYcgl1buu0gyg7BrV7k--WA=s1600" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="600" data-original-width="1600" height="240" src="https://blogger.googleusercontent.com/img/a/AVvXsEhpJxLN_5CuSyz_gt-xrDIoLyi1HQ0PdAYHQgomGhbABA-qbDAcevBYsq0XcgpozNP3e_UWvwmgoUOC6pxv5vjKnDM4Wdn8zy6GYv3jVe3iccXNKh_-x4V0RAFugPfroW3FHmFcuQmLTzaNZGDultf0a72BrweNciUhWKSzYcgl1buu0gyg7BrV7k--WA=w640-h240" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Illustration of the computation through MetNet-2. As the computation progresses, the network processes an ever larger context from the input and makes a probabilistic forecast of the likely future weather conditions.</td>
             </tr>
            </tbody>
           </table> 
           <p> Within weather forecasting, deep learning techniques have shown particular promise for <a href="https://en.wikipedia.org/wiki/Nowcasting_(meteorology)">nowcasting</a>  i.e., predicting weather up to 2-6 hours ahead. Previous work has focused on <a href="https://ai.googleblog.com/2020/01/using-machine-learning-to-nowcast.html">using direct neural network models for weather data</a>, extending neural forecasts from 0 to 8 hours with the <a href="https://ai.googleblog.com/2020/03/a-neural-weather-model-for-eight-hour.html">MetNet architecture</a>, generating <a href="https://deepmind.com/blog/article/nowcasting">continuations of radar data for up to 90 minutes ahead</a>, and <a href="http://raincheck.karyk.com/rain-check">interpreting the weather information learned</a> by these neural networks. Still, there is an opportunity for deep learning to extend improvements to longer-range forecasts. </p> 
           <p> To that end, in <a href="https://arxiv.org/abs/2111.07470">Skillful Twelve Hour Precipitation Forecasts Using Large Context Neural Networks</a>, we push the forecasting boundaries of our neural precipitation model to 12 hour predictions while keeping a spatial resolution of 1 km and a time resolution of 2 minutes. By quadrupling the input context, adopting a richer weather input state, and extending the architecture to capture longer-range spatial dependencies, MetNet-2 substantially improves on the performance of its predecessor, MetNet. Compared to physics-based models, MetNet-2 outperforms the state-of-the-art <a href="https://www.essoar.org/pdfjs/10.1002/essoar.10501462.1">HREF ensemble model</a> for weather forecasts up to 12 hours ahead. </p> 
           <p> <b>MetNet-2 Features and Architecture</b> <br> Neural weather models like MetNet-2 map observations of the Earth to the probability of weather events, such as the likelihood of rain over a city in the afternoon, of wind gusts reaching 20 knots, or of a sunny day ahead. End-to-end deep learning has the potential to both streamline and increase quality by directly connecting a system's inputs and outputs. With this in mind, MetNet-2 aims to minimize both the complexity and the total number of steps involved in creating a forecast. </p> 
           <p> The inputs to MetNet-2 include the radar and satellite images also used in MetNet. To capture a more comprehensive snapshot of the atmosphere with information such as temperature, humidity, and wind direction  critical for longer forecasts of up to 12 hours  MetNet-2 also uses the pre-processed starting state used in physical models as a proxy for this additional weather information. The radar-based measures of precipitation (<a href="https://www.nssl.noaa.gov/projects/mrms/">MRMS</a>) serve as the ground truth (i.e., what we are trying to predict) that we use in training to optimize MetNet-2s parameters. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEidl9U_aE6zh6zn8niiFA1H2Cz-ftmPFXV6Bi8R8u2bo2vWMc1it0gUFZb-ru5lgKF8f_zMmfooQPnYDKfXX-0L8Vz4f-216vZV79_6aTJhuGmWH8i9edO9clpxa_X3e7wjDhNRrF6oAMhKMrcNgF10p7aRpI98CVhEqz8BJ8wbgsnIbYo8nrWbzFpE0Q=s512" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="360" data-original-width="512" height="450" src="https://blogger.googleusercontent.com/img/a/AVvXsEidl9U_aE6zh6zn8niiFA1H2Cz-ftmPFXV6Bi8R8u2bo2vWMc1it0gUFZb-ru5lgKF8f_zMmfooQPnYDKfXX-0L8Vz4f-216vZV79_6aTJhuGmWH8i9edO9clpxa_X3e7wjDhNRrF6oAMhKMrcNgF10p7aRpI98CVhEqz8BJ8wbgsnIbYo8nrWbzFpE0Q=w640-h450" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Example ground truth image: Instantaneous precipitation (mm/hr) based on radar (MRMS) capturing a 12 hours-long progression.</td>
             </tr>
            </tbody>
           </table> 
           <p> MetNet-2s probabilistic forecasts can be viewed as averaging all possible future weather conditions weighted by how likely they are. Due to its probabilistic nature, MetNet-2 can be likened to physics-based ensemble models, which average some number of future weather conditions predicted by a variety of physics-based models. One notable difference between these two approaches is the duration of the core part of the computation: ensemble models take ~1 hour, whereas MetNet-2 takes ~1 second. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjsWtg6q7Py4Ux0IrGxNMdI7alAnh-p9z8Nv4kXdS34o3uQef5qZ6KensNDGZTgzGAxhSxHVygTy9tMdsLSWTAjI3EwvDOubiYRBhDgYrk4_l07MdzugA7yzTjBnGp3oO33lo0PooPfu5MkajKBJQ0YxRMWuc-UvAYVQox78Pf33HCW4LpoLS8CEqErOQ=s793" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="127" data-original-width="793" height="102" src="https://blogger.googleusercontent.com/img/a/AVvXsEjsWtg6q7Py4Ux0IrGxNMdI7alAnh-p9z8Nv4kXdS34o3uQef5qZ6KensNDGZTgzGAxhSxHVygTy9tMdsLSWTAjI3EwvDOubiYRBhDgYrk4_l07MdzugA7yzTjBnGp3oO33lo0PooPfu5MkajKBJQ0YxRMWuc-UvAYVQox78Pf33HCW4LpoLS8CEqErOQ=w640-h102" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Steps in a MetNet-2 forecast and in a physics-based ensemble.</td>
             </tr>
            </tbody>
           </table> 
           <p> One of the main challenges that MetNet-2 must overcome to make 12 hour long forecasts is capturing a sufficient amount of spatial context in the input images. For each additional forecast hour we include 64 km of context in every direction at the input. This results in an input context of size 2048<sup>2</sup> km<sup>2</sup>  four times that used in MetNet. In order to process such a large context, MetNet-2 employs model parallelism whereby the model is distributed across 128 cores of a <a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">Cloud TPU v3-128</a>. Due to the size of the input context, MetNet-2 replaces the attentional layers of MetNet with computationally more efficient convolutional layers. But standard convolutional layers have local receptive fields that may fail to capture large spatial contexts, so MetNet-2 uses dilated receptive fields, whose size doubles layer after layer, in order to connect points in the input that are far apart one from the other. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiCoKS5L3j23hcJDcTssiSuKs0e4u6enaUJQmveAEjMRGt0ForeFOQx8hub0r_nlLZ7h4Q1rk3DUucM5vpR6ovAwNtHOYhySoyxzs54ahrBUlcp5tuPeUmLrwq3-M3aXwyVGwO5wfSEa95J_ioZMP5avMy_jf3oAECecMxOZkTR_ZSnvDklsT3aJBffDw=s1674" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="597" data-original-width="1674" height="228" src="https://blogger.googleusercontent.com/img/a/AVvXsEiCoKS5L3j23hcJDcTssiSuKs0e4u6enaUJQmveAEjMRGt0ForeFOQx8hub0r_nlLZ7h4Q1rk3DUucM5vpR6ovAwNtHOYhySoyxzs54ahrBUlcp5tuPeUmLrwq3-M3aXwyVGwO5wfSEa95J_ioZMP5avMy_jf3oAECecMxOZkTR_ZSnvDklsT3aJBffDw=w640-h228" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Example of input spatial context and target area for MetNet-2.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Results</b> <br> Because MetNet-2s predictions are probabilistic, the models output is naturally compared with the output of similarly probabilistic ensemble or post-processing models. <a href="https://nomads.ncep.noaa.gov/txt_descriptions/HREF_doc.shtml">HREF</a> is one such state-of-the-art ensemble model for precipitation in the United States, which aggregates ten predictions from five different models, twice a day. We evaluate the forecasts using established metrics, such as the <a href="https://journals.ametsoc.org/view/journals/wefo/15/5/1520-0434_2000_015_0559_dotcrp_2_0_co_2.xml">Continuous Ranked Probability Score</a>, which captures the magnitude of the probabilistic error of a models forecasts relative to the ground truth observations. Despite not performing any physics-based calculations, MetNet-2 is able to outperform HREF up to 12 hours into the future for both low and high levels of precipitation. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgQSVopbZHNl9Bk1HRR4ybO4yEGK8FPxjOr94erKqcc8WGs1yXthNZw6XOPau6UwcnKURAL9IuMg4pj8hyW3foE4sA_3r6mlGNCE1UYEBVag1BPjo-cKsiyT1CsWhobqlISO23PHttCcNvQsKBFqXrHuqbdx2AjdDSGxTFMaXFp_7LEDK9IaTQvMzKrmg=s1298" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="748" data-original-width="1298" height="368" src="https://blogger.googleusercontent.com/img/a/AVvXsEgQSVopbZHNl9Bk1HRR4ybO4yEGK8FPxjOr94erKqcc8WGs1yXthNZw6XOPau6UwcnKURAL9IuMg4pj8hyW3foE4sA_3r6mlGNCE1UYEBVag1BPjo-cKsiyT1CsWhobqlISO23PHttCcNvQsKBFqXrHuqbdx2AjdDSGxTFMaXFp_7LEDK9IaTQvMzKrmg=w640-h368" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Continuous Ranked Probability Score (CRPS; lower is better) for MetNet-2 vs HREF aggregated over a large number of test patches randomly located in the Continental United States.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Examples of Forecasts</b> <br> The following figures provide a selection of forecasts from MetNet-2 compared with the physics-based ensemble HREF and the ground truth MRMS. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEh4deGJPLR0hIronPcWgrfZ37xxyId_5dOtzyLODJJBMS5-aCE2hWfn1CzSkWe8XBUv4pbquK1n9_34I6mphu89OTQn51pqdDgfTwinUwYaP_Wq4zBRYsN5wVsCheRDwFNJNYbHqbu0G8LqrJ4vug1edG4DMKdXFYyE5Nsudys1nARsC_4iBT3BCKFojw=s1984" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="848" data-original-width="1984" height="274" src="https://blogger.googleusercontent.com/img/a/AVvXsEh4deGJPLR0hIronPcWgrfZ37xxyId_5dOtzyLODJJBMS5-aCE2hWfn1CzSkWe8XBUv4pbquK1n9_34I6mphu89OTQn51pqdDgfTwinUwYaP_Wq4zBRYsN5wVsCheRDwFNJNYbHqbu0G8LqrJ4vug1edG4DMKdXFYyE5Nsudys1nARsC_4iBT3BCKFojw=w640-h274" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Probability maps for the cumulative precipitation rate of 1 mm/hr on January 3, 2019 over the Pacific NorthWest. The maps are shown for each hour of lead time from 1 to 12. <b>Left</b>: Ground truth, source MRMS. <b>Center</b>: Probability map as predicted by MetNet-2 . <b>Right</b>: Probability map as predicted by HREF.</td>
             </tr>
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjAXgO_kTmVS1gyhEXvz2x6gInspa5dpuDJNuYuMVeOJZZrdvzAdZ4ymHXyo0SdxVY-5LhFWglC9jTUf4X8dsihZFlDOxwQrX0ZzGOeUzZTmPSuYo3o99RRUJiWP1N5EjMhxhDdlTMmPCkIpcV859bxIsiqgi9BeOJHRFPfw82IXyJETn9lfGlXcwKWHA=s1984" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="848" data-original-width="1984" height="274" src="https://blogger.googleusercontent.com/img/a/AVvXsEjAXgO_kTmVS1gyhEXvz2x6gInspa5dpuDJNuYuMVeOJZZrdvzAdZ4ymHXyo0SdxVY-5LhFWglC9jTUf4X8dsihZFlDOxwQrX0ZzGOeUzZTmPSuYo3o99RRUJiWP1N5EjMhxhDdlTMmPCkIpcV859bxIsiqgi9BeOJHRFPfw82IXyJETn9lfGlXcwKWHA=w640-h274" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Comparison of 0.2 mm/hr precipitation on March 30, 2020 over Denver, Colorado. <b>Left</b>: Ground truth, source MRMS. <b>Center:</b> Probability map as predicted by MetNet-2 . <b>Right</b>: Probability map as predicted by HREF.MetNet-2 is able to predict the onset of the storm (called <a href="https://learnweather.com/basic-weather/convective-initiation-mk/">convective initiation</a>) earlier in the forecast than HREF as well as the storms starting location, whereas HREF misses the initiation location, but captures its growth phase well.</td>
             </tr>
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiK5MfQbEYHpOtdbNYT3d61pwQif1ukIAvrdpIoZM7tA_eXxCmUdbEv02o-gU2JwOT0oIKizB_kvCb6sJw_-qklND6SaaLOXDLVJnlfnMBKY0EWNmowUJNHr8qEirLeHrC1jb4WxSJ0ViqLaEXWqlW58iaMNWQeNi-8yYY8WeksGGpA69g-YPWkT_DGmA=s1984" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="848" data-original-width="1984" height="274" src="https://blogger.googleusercontent.com/img/a/AVvXsEiK5MfQbEYHpOtdbNYT3d61pwQif1ukIAvrdpIoZM7tA_eXxCmUdbEv02o-gU2JwOT0oIKizB_kvCb6sJw_-qklND6SaaLOXDLVJnlfnMBKY0EWNmowUJNHr8qEirLeHrC1jb4WxSJ0ViqLaEXWqlW58iaMNWQeNi-8yYY8WeksGGpA69g-YPWkT_DGmA=w640-h274" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Comparison of 2 mm/hr precipitation stemming from Hurricane Isaias, an extreme weather event that occurred on August 4, 2020 over the North East coast of the US. <b>Left</b>: Ground truth, source MRMS. <b>Center</b>: Probability map as predicted by MetNet-2. <b>Right</b>: Probability map as predicted by HREF.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Interpreting What MetNet-2 Learns About Weather</b> <br> Because MetNet-2 does not use hand-crafted physical equations, its performance inspires a natural question: What kind of physical relations about the weather does it learn from the data during training? Using <a href="https://arxiv.org/abs/1703.01365">advanced interpretability tools</a>, we further trace the impact of various input features on MetNet-2s performance at different forecast timelines. Perhaps the most surprising finding is that MetNet-2 appears to emulate the physics described by <a href="http://www.dca.iag.usp.br/material/ritaynoue/agm5706/METR4424qgtheory.pdf">Quasi-Geostrophic Theory</a>, which is used as an effective approximation of large-scale weather phenomena. MetNet-2 was able to pick up on changes in the atmospheric forces, at the scale of a typical high- or low-pressure system (i.e., the <a href="https://en.wikipedia.org/wiki/Synoptic_scale_meteorology">synoptic scale</a>), that bring about favorable conditions for precipitation, a key tenet of the theory. </p> 
           <p> <b>Conclusion</b> <br> MetNet-2 represents a step toward enabling a new modeling paradigm for weather forecasting that does not rely on hand-coding the physics of weather phenomena, but rather embraces end-to-end learning from observations to weather targets and parallel forecasting on low-precision hardware. Yet many challenges remain on the path to fully achieving this goal, including incorporating more raw data about the atmosphere directly (rather than using the pre-processed starting state from physical models), broadening the set of weather phenomena, increasing the lead time horizon to days and weeks, and widening the geographic coverage beyond the United States. </p> 
           <p> <b>Acknowledgements</b> <br> <em>Shreya Agrawal, Casper Snderby, Manoj Kumar, Jonathan Heek, Carla Bromberg, Cenk Gazen, Jason Hickey, Aaron Bell, Marcin Andrychowicz, Amy McGovern, Rob Carver, Stephan Hoyer, Zack Ontiveros, Lak Lakshmanan, David McPeek, Ian Gonzalez, Claudio Martella, Samier Merchant, Fred Zyda, Daniel Furrer and Tom Small.</em> </p>
           <p><em><br></em></p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:MetNet-2: Deep Learning for 12-Hour Precipitation Forecasting&amp;url=http://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="post" data-id="8291129565142711697" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/11/an-open-source-vibrotactile-haptics.html" itemprop="url" title="An Open Source Vibrotactile Haptics Platform for On-Body Applications"> An Open Source Vibrotactile Haptics Platform for On-Body Applications </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Friday, November 12, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Posted by Artem Dementyev, Hardware Engineer, Google Research</span>

<p>
Most wearable smart devices and mobile phones have the means to communicate with the user through tactile feedback, enabling applications from simple notifications to sensory substitution for accessibility. Typically, they accomplish this using <a href="https://en.wikipedia.org/wiki/Haptic_technology#Vibration">vibrotactile actuators</a>, which are small electric vibration motors. However, designing a <a href="https://en.wikipedia.org/wiki/Haptic_technology">haptic system</a> that is well-targeted and effective for a given task requires experimentation with the number of actuators and their locations in the device, yet most practical applications require standalone on-body devices and integration into small form factors. This combination of factors can be difficult to address outside of a laboratory as integrating these systems can be quite time-consuming and often requires a high level of expertise.</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjVpluZLm3bnV5MzzyTfj0tU9wCsyWiGWgjsJILWhHPlsAJqTZymcgCBqe46WN0GhbWySgZhdmffvJqnAh49K_b1JQ0Kt0SnpFBosVSKxNRSspIbCojNriAuCHSXQtG2BgE2DOvU75se4LqmlOmJE72qXlimpi2M-kE20ViHSMQXovqczyQ9fvagidKwA=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="488" data-original-width="1999" height="156" src="https://blogger.googleusercontent.com/img/a/AVvXsEjVpluZLm3bnV5MzzyTfj0tU9wCsyWiGWgjsJILWhHPlsAJqTZymcgCBqe46WN0GhbWySgZhdmffvJqnAh49K_b1JQ0Kt0SnpFBosVSKxNRSspIbCojNriAuCHSXQtG2BgE2DOvU75se4LqmlOmJE72qXlimpi2M-kE20ViHSMQXovqczyQ9fvagidKwA=w640-h156" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">A typical lab setup on the left and the VHP board on the right.</td></tr></tbody></table>


<p>
In &#8220;<a href="https://dl.acm.org/doi/pdf/10.1145/3472749.3474772">VHP: Vibrotactile Haptics Platform for On-body Applications</a>&#8221;, presented at <a href="https://uist.acm.org/">ACM UIST 2021</a>, we develop a low-power miniature electronics board that can drive up to 12 independent channels of haptic signals with arbitrary waveforms. The VHP electronics board can be battery-powered, and integrated into wearable devices and small gadgets. It allows all-day wear, has low latency, battery life between 3 and 25 hours, and can run 12 actuators simultaneously. We show that VHP can be used in bracelet, sleeve, and phone-case form factors. The bracelet was programmed with an audio-to-tactile interface to aid lipreading and remained functional when worn for multiple months by developers. To facilitate greater progress in the field of wearable multi-channel haptics with the necessary tools for their design, implementation, and experimentation, we are releasing the hardware design and software for the VHP system via <a href="https://github.com/google/audio-to-tactile">GitHub</a>. 
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjt_AcLYHbXiUl-1XKGlYG5OliJpRn75eXGQxjSgQSaftpL5YT5ru5jwzcZHLXSLtEHy5TSntBYpNMsM72nniN9j0FEZr_Nm6CMEo8EdFS2JCP4VecFUfQezqrKHVn3vhjUzPrfCqKMJrosVhqKhZO8evFj0oB7A2RrZ9PFDuTYixc4UlxnK0S6u5oDJg=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="971" data-original-width="1999" height="310" src="https://blogger.googleusercontent.com/img/a/AVvXsEjt_AcLYHbXiUl-1XKGlYG5OliJpRn75eXGQxjSgQSaftpL5YT5ru5jwzcZHLXSLtEHy5TSntBYpNMsM72nniN9j0FEZr_Nm6CMEo8EdFS2JCP4VecFUfQezqrKHVn3vhjUzPrfCqKMJrosVhqKhZO8evFj0oB7A2RrZ9PFDuTYixc4UlxnK0S6u5oDJg=w640-h310" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Front and back sides of the VHP circuit board. </td></tr></tbody></table>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj94QjnzXgdZH640qCMMAbviwMQ0Q-7B886SxYSAU5-x-bliGJhRzrNi1tbGB-bkK8Jn7YK1OV3YTqFku6A89fP0kbUeN-pCYRiKwzGWDn54imVqwsK3vK7nYDkDuyPAJCOgss0-Q8WWg7CDE72fuugZPbhdOOwSGr5x5b-2fpQVNFVUwiNUR7cnXo4Pg=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1266" data-original-width="1999" height="406" src="https://blogger.googleusercontent.com/img/a/AVvXsEj94QjnzXgdZH640qCMMAbviwMQ0Q-7B886SxYSAU5-x-bliGJhRzrNi1tbGB-bkK8Jn7YK1OV3YTqFku6A89fP0kbUeN-pCYRiKwzGWDn54imVqwsK3vK7nYDkDuyPAJCOgss0-Q8WWg7CDE72fuugZPbhdOOwSGr5x5b-2fpQVNFVUwiNUR7cnXo4Pg=w640-h406" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Block diagram of the system.</td></tr></tbody></table>


<p>
<b>Platform Specifications.</b>
<br />
VHP consists of a custom designed circuit board, where the main components are the microcontroller and haptic amplifier, which converts microcontroller&#8217;s digital output into signals that drive the actuators. The haptic actuators can be controlled by signals arriving via serial, USB, and <a href="https://en.wikipedia.org/wiki/Bluetooth_Low_Energy">Bluetooth Low Energy</a> (BLE), as well as onboard microphones, using an <a href="https://www.nordicsemi.com/Products/nRF52840">nRF52840</a> microcontroller, which was chosen because it offers many input and output options and BLE, all in a small package. We added several sensors into the board to provide more experimental flexibility: an on-board digital microphone, an analog microphone amplifier, and an accelerometer. The firmware is a portable C/C++ library that works in the <a href="https://www.arduino.cc/">Arduino</a> ecosystem. 
</p>

<p>
To allow for rapid iteration during development, the interface between the board and actuators is critical. The 12 tactile signals&#8217; wiring have to be quick to set up in order to allow for such development, while being flexible and robust to stand up to prolonged use. For the interface, we use a 24-pin <a href="https://en.wikipedia.org/wiki/Flexible_electronics">FPC</a> (flexible printed circuit) connector on the VHP. We support interfacing to the actuators in two ways: with a custom flexible circuit board and with a rigid breakout board.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgGaJA_gO_Ag_vBL7liRlwHVUy3jZ72Hf6pFdepi7IDXy8zTeYyE0eAzy1j4uJQNv_1CY5x_jJzlKIpyEzvjthvnZx1BHnlK_xY2BbvZm_pvXCOeKV5lNT3GgeXvsOCmfSe1biCI_u0MWcvo7g7dk5fvhp0HtMTQZKPDtVtAbgEi_cWnMD57GuvcjeYkQ=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1475" data-original-width="1999" height="472" src="https://blogger.googleusercontent.com/img/a/AVvXsEgGaJA_gO_Ag_vBL7liRlwHVUy3jZ72Hf6pFdepi7IDXy8zTeYyE0eAzy1j4uJQNv_1CY5x_jJzlKIpyEzvjthvnZx1BHnlK_xY2BbvZm_pvXCOeKV5lNT3GgeXvsOCmfSe1biCI_u0MWcvo7g7dk5fvhp0HtMTQZKPDtVtAbgEi_cWnMD57GuvcjeYkQ=w640-h472" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">VHP board (small board on the <b>right</b>) connected to three different types of tactile actuators via rigid breakout board (large board on the <b>left</b>).</td></tr></tbody></table>



<p>
<b>Using Haptic Actuators as Sensors </b>
<br />
In our previous <a href="https://ai.googleblog.com/2020/11/haptics-with-input-using-linear.html">blog post</a>, we explored how back-EMF in a haptic actuator could be used for sensing and demonstrated a variety of useful applications. Instead of using back-EMF sensing in the VHP system, we measure the electrical current that drives each vibrotactile actuator and use the current load as the sensing mechanism. Unlike back-EMF sensing, this current-sensing approach allows simultaneous sensing and actuation, while minimizing the additional space needed on the board. 
</p>

<p>
One challenge with the current-sensing approach is that there is a wide variety of vibrotactile actuators, each of which may behave differently and need different presets. In addition, because different actuators can be added and removed during prototyping with the adapter board, it would be useful if the VHP were able to identify the actuator automatically. This would improve the speed of prototyping and make the system more novice-friendly. 
</p>

<p>
To explore this possibility, we collected current-load data from three off-the-shelf haptic actuators and trained a simple support vector machine classifier to recognize the difference in the signal pattern between actuators. The test accuracy was 100% for classifying the three actuators, indicating that each actuator has a very distinct response. 
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhNOYIBIiKxo66xrH0yK2qcwQ0_ah3XIJER3gfqCCTniFWa5W44GBZAv0Xumkpk65LxGw0GzHIM_yKd5adCveDFlaxJ_LFIn49s3SkJixPsi9yeYIljyo5mLgC8QOGiJDGZ6aNlTzp3Ua0UxePOoXiJsckoBck1VGQJu4pSVARWTPO69aYM23NSo5J2pw=s432" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="288" data-original-width="432" height="426" src="https://blogger.googleusercontent.com/img/a/AVvXsEhNOYIBIiKxo66xrH0yK2qcwQ0_ah3XIJER3gfqCCTniFWa5W44GBZAv0Xumkpk65LxGw0GzHIM_yKd5adCveDFlaxJ_LFIn49s3SkJixPsi9yeYIljyo5mLgC8QOGiJDGZ6aNlTzp3Ua0UxePOoXiJsckoBck1VGQJu4pSVARWTPO69aYM23NSo5J2pw=w640-h426" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Different actuators have a different current signature during a frequency sweep, thus allowing for automatic identification.</td></tr></tbody></table>



<p>
Additionally, vibrotactile actuators require proper contact with the skin for consistent control over stimulation. Thus, the device should measure skin contact and either provide an alert or self-adjust if it is not loaded correctly. To test whether a skin contact measuring technique works in practice, we measured the current load on actuators in a bracelet as it was tightened and loosened around the wrist. As the bracelet strap is tightened, the contact pressure between the skin and the actuator increases and the current required to drive the actuator signal increases commensurately. 
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEizoM-Cd1SBq3MnOPEYGS6_OApqRncer_EgY12rP4kpCTVYFt1PYnhS2qmavi0nLFwL0FiKR42NDvQPilKnNHn2H3hDE21Q4Y5qUrxkcdJvmQOmEIWb4fIPj8N0PFTOSorUi5gqaDCsoJCbTZWDWixB3ZyIcliSaCRhOJFSpRXcWZBA6zgbDzpcQQ4seg=s480" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="270" data-original-width="480" height="360" src="https://blogger.googleusercontent.com/img/a/AVvXsEizoM-Cd1SBq3MnOPEYGS6_OApqRncer_EgY12rP4kpCTVYFt1PYnhS2qmavi0nLFwL0FiKR42NDvQPilKnNHn2H3hDE21Q4Y5qUrxkcdJvmQOmEIWb4fIPj8N0PFTOSorUi5gqaDCsoJCbTZWDWixB3ZyIcliSaCRhOJFSpRXcWZBA6zgbDzpcQQ4seg=w640-h360" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Current load sensing is responding to touch, while the actuator is driven at 250 Hz frequency.<br /><br /></td></tr></tbody></table>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhXNb4PZEsIAdjL8XClJ_farl8QJpRzOFfcUMi6JLAejpFZ_j4a3kdiki7ImEI8dBXi5t2tBlFq9NjiDKsq-KH0IGds1M5jhiVkOzw9VNU5FBL8RbAvKwCS1QDlDPFzMGR-0S1B-zyciWzsirPi7rGCbC5XOl9aS_6r9T_pQhUQ311_AMf0STwBy1ColA=s480" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="270" data-original-width="480" height="360" src="https://blogger.googleusercontent.com/img/a/AVvXsEhXNb4PZEsIAdjL8XClJ_farl8QJpRzOFfcUMi6JLAejpFZ_j4a3kdiki7ImEI8dBXi5t2tBlFq9NjiDKsq-KH0IGds1M5jhiVkOzw9VNU5FBL8RbAvKwCS1QDlDPFzMGR-0S1B-zyciWzsirPi7rGCbC5XOl9aS_6r9T_pQhUQ311_AMf0STwBy1ColA=w640-h360" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Quality of the fit of the bracelet is measured.</td></tr></tbody></table>

<p>  
  <b>Audio-to-Tactile Feedback </b>
<br />
To demonstrate the utility of the VHP platform, we used it to develop an audio-to-tactile feedback device to help with lipreading. Lipreading can be difficult for many speech sounds that look similar (<a href="https://en.wikipedia.org/wiki/Viseme">visemes</a>), such as &#8220;pin&#8221; and &#8220;min&#8221;. In order to help the user differentiate visemes like these, we attach a microphone to the VHP system, which can then pick up the speech sounds and translate the audio to vibrations on the wrist. For audio-to-tactile translation, we used our previously developed algorithms for real-time audio-to-tactile conversion, available via <a href="https://github.com/google/audio-to-tactile">GitHub</a>. Briefly, audio filters are paired with neural networks to recognize certain viesemes (e.g., picking up the hard consonant &#8220;p&#8221; in &#8220;pin&#8221;), and are then translated to vibrations in different parts of the bracelet. Our approach is inspired by <a href="https://ieeexplore.ieee.org/document/8423203">tactile phonemic sleeve</a> (TAPS), however the major difference is that in our approach the tactile signal is presented continuously and in real-time. 
</p>

<p>
One of the developers who employs lipreading in daily life wore the bracelet daily for several months and found it to give better information to facilitate lipreading than previous devices, allowing improved understanding of lipreading visemes with the bracelet versus lipreading alone. In the future, we plan to conduct full-scale experiments with multiple users wearing the device for an extended time.
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhtsbQUuAWwAtoAqzlM6lOevz0Et8FpMPgKMSnoDTsdaWag0-BlOhMw4iddEC_5TTN8h5pQ9LdxpXYCL-B9EK482S2MN4hZaIz6kK6-l5KAMLslFdPlu_ED6qoUQIDkCoj8SVnVSqlTsQbZqICoEFI7YHp4wWTI31IZk4tgcP4-tr1I8GGhuvP9-4ilfg=s950" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="320" data-original-width="950" height="216" src="https://blogger.googleusercontent.com/img/a/AVvXsEhtsbQUuAWwAtoAqzlM6lOevz0Et8FpMPgKMSnoDTsdaWag0-BlOhMw4iddEC_5TTN8h5pQ9LdxpXYCL-B9EK482S2MN4hZaIz6kK6-l5KAMLslFdPlu_ED6qoUQIDkCoj8SVnVSqlTsQbZqICoEFI7YHp4wWTI31IZk4tgcP4-tr1I8GGhuvP9-4ilfg=w640-h216" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><b>Left</b>: Audio-to-tactile sleeve. <b>Middle</b>: Audio-to-tactile bracelet. <b>Right</b>: One of our developers tests out the bracelets, which are worn on both arms.</td></tr></tbody></table>



<p>
<b>Potential Applications</b>
<br />
The VHP platform enables rapid experimentation and prototyping that can be used to develop techniques for a variety of applications. For example:
</p>

<ul>

<li><em>Rich haptics on small devices:</em> Expanding the number of actuators on mobile phones, which typically only have one or two, could be useful to provide additional tactile information. This is especially useful as fingers are sensitive to vibrations. We demonstrated a prototype mobile phone case with eight vibrotactile actuators. This could be used to provide rich notifications and enhance effects in a mobile game or when watching a video. 

</li><li><em>Lab psychophysical experiments:<strong> </strong></em>Because VHP can be easily set up to send and receive haptic signals in real time, e.g., from a <a href="https://jupyter.org/">Jupyter notebook</a>, it could be used to perform real-time haptic experiments. 

</li><li><em>Notifications and alerts:<strong> </strong></em>The wearable VHP could be used to provide haptic notifications from other devices, e.g., alerting if someone is at the door, and could even communicate distinguishable alerts through use of multiple actuators.

</li><li><em>Sensory substitution:<strong> </strong></em>Besides the lipreading assistance example above, there are many other potential applications for accessibility using sensory substitution, such as visual-to-tactile sensing or even sensing magnetic fields. 

</li><li><em>Loading sensing:</em> The ability to sense from the haptic actuator current load is unique to our platform, and enables a variety of features, such as pressure sensing or automatically adjusting actuator output. 
</li>
</ul>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgChU2NcIS1N9ocnj3TNzDbDkyjOjd2PPk_xoWAJjVltWjgk0a5aTDvmSPKgqmTL5-n1bnv2rG9HfJf6foeRTApStcwYmuTXuFtomdKATpGQ2axD4j0PU_aymbobd3tmm4fbGWai1LClppW7IzIi_LxCycS4zli5-3-VlkjK8uQ_P53Pa4v7UIoL5Z4KQ=s1491" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="344" data-original-width="1491" height="148" src="https://blogger.googleusercontent.com/img/a/AVvXsEgChU2NcIS1N9ocnj3TNzDbDkyjOjd2PPk_xoWAJjVltWjgk0a5aTDvmSPKgqmTL5-n1bnv2rG9HfJf6foeRTApStcwYmuTXuFtomdKATpGQ2axD4j0PU_aymbobd3tmm4fbGWai1LClppW7IzIi_LxCycS4zli5-3-VlkjK8uQ_P53Pa4v7UIoL5Z4KQ=w640-h148" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Integrating eight voice coils into a phone case. We used loading sensing to understand which voice coils are being touched. <br /></td></tr></tbody></table>


<p>
<b>What's next?</b>
<br />
We hope that others can utilize the platform to build a diverse set of applications. If you are interested and have ideas about using our platform or want to receive updates, please fill out this <a href="https://sites.google.com/view/vhp-collaborations/home">form</a>. We hope that with this platform, we can help democratize the use of haptics and inspire a more widespread use of tactile devices.
</p>

<p>
<b>Acknowledgments</b>
<br />
<em>This work was done by Artem Dementyev, Pascal Getreuer, Dimitri Kanevsky, Malcolm Slaney and Richard Lyon. We thank Alex Olwal, Thad Starner, Hong Tan, Charlotte Reed, Sarah Sterman for valuable feedback and discussion on the paper.  Yuhui Zhao, Dmitrii Votintcev, Chet Gnegy, Whitney Bai and Sagar Savla for feedback on the design and engineering. </em>
</p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Posted by Artem Dementyev, Hardware Engineer, Google Research</span> 
           <p> Most wearable smart devices and mobile phones have the means to communicate with the user through tactile feedback, enabling applications from simple notifications to sensory substitution for accessibility. Typically, they accomplish this using <a href="https://en.wikipedia.org/wiki/Haptic_technology#Vibration">vibrotactile actuators</a>, which are small electric vibration motors. However, designing a <a href="https://en.wikipedia.org/wiki/Haptic_technology">haptic system</a> that is well-targeted and effective for a given task requires experimentation with the number of actuators and their locations in the device, yet most practical applications require standalone on-body devices and integration into small form factors. This combination of factors can be difficult to address outside of a laboratory as integrating these systems can be quite time-consuming and often requires a high level of expertise.</p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjVpluZLm3bnV5MzzyTfj0tU9wCsyWiGWgjsJILWhHPlsAJqTZymcgCBqe46WN0GhbWySgZhdmffvJqnAh49K_b1JQ0Kt0SnpFBosVSKxNRSspIbCojNriAuCHSXQtG2BgE2DOvU75se4LqmlOmJE72qXlimpi2M-kE20ViHSMQXovqczyQ9fvagidKwA=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="488" data-original-width="1999" height="156" src="https://blogger.googleusercontent.com/img/a/AVvXsEjVpluZLm3bnV5MzzyTfj0tU9wCsyWiGWgjsJILWhHPlsAJqTZymcgCBqe46WN0GhbWySgZhdmffvJqnAh49K_b1JQ0Kt0SnpFBosVSKxNRSspIbCojNriAuCHSXQtG2BgE2DOvU75se4LqmlOmJE72qXlimpi2M-kE20ViHSMQXovqczyQ9fvagidKwA=w640-h156" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">A typical lab setup on the left and the VHP board on the right.</td>
             </tr>
            </tbody>
           </table> 
           <p> In <a href="https://dl.acm.org/doi/pdf/10.1145/3472749.3474772">VHP: Vibrotactile Haptics Platform for On-body Applications</a>, presented at <a href="https://uist.acm.org/">ACM UIST 2021</a>, we develop a low-power miniature electronics board that can drive up to 12 independent channels of haptic signals with arbitrary waveforms. The VHP electronics board can be battery-powered, and integrated into wearable devices and small gadgets. It allows all-day wear, has low latency, battery life between 3 and 25 hours, and can run 12 actuators simultaneously. We show that VHP can be used in bracelet, sleeve, and phone-case form factors. The bracelet was programmed with an audio-to-tactile interface to aid lipreading and remained functional when worn for multiple months by developers. To facilitate greater progress in the field of wearable multi-channel haptics with the necessary tools for their design, implementation, and experimentation, we are releasing the hardware design and software for the VHP system via <a href="https://github.com/google/audio-to-tactile">GitHub</a>. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjt_AcLYHbXiUl-1XKGlYG5OliJpRn75eXGQxjSgQSaftpL5YT5ru5jwzcZHLXSLtEHy5TSntBYpNMsM72nniN9j0FEZr_Nm6CMEo8EdFS2JCP4VecFUfQezqrKHVn3vhjUzPrfCqKMJrosVhqKhZO8evFj0oB7A2RrZ9PFDuTYixc4UlxnK0S6u5oDJg=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="971" data-original-width="1999" height="310" src="https://blogger.googleusercontent.com/img/a/AVvXsEjt_AcLYHbXiUl-1XKGlYG5OliJpRn75eXGQxjSgQSaftpL5YT5ru5jwzcZHLXSLtEHy5TSntBYpNMsM72nniN9j0FEZr_Nm6CMEo8EdFS2JCP4VecFUfQezqrKHVn3vhjUzPrfCqKMJrosVhqKhZO8evFj0oB7A2RrZ9PFDuTYixc4UlxnK0S6u5oDJg=w640-h310" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Front and back sides of the VHP circuit board. </td>
             </tr>
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj94QjnzXgdZH640qCMMAbviwMQ0Q-7B886SxYSAU5-x-bliGJhRzrNi1tbGB-bkK8Jn7YK1OV3YTqFku6A89fP0kbUeN-pCYRiKwzGWDn54imVqwsK3vK7nYDkDuyPAJCOgss0-Q8WWg7CDE72fuugZPbhdOOwSGr5x5b-2fpQVNFVUwiNUR7cnXo4Pg=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1266" data-original-width="1999" height="406" src="https://blogger.googleusercontent.com/img/a/AVvXsEj94QjnzXgdZH640qCMMAbviwMQ0Q-7B886SxYSAU5-x-bliGJhRzrNi1tbGB-bkK8Jn7YK1OV3YTqFku6A89fP0kbUeN-pCYRiKwzGWDn54imVqwsK3vK7nYDkDuyPAJCOgss0-Q8WWg7CDE72fuugZPbhdOOwSGr5x5b-2fpQVNFVUwiNUR7cnXo4Pg=w640-h406" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Block diagram of the system.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Platform Specifications.</b> <br> VHP consists of a custom designed circuit board, where the main components are the microcontroller and haptic amplifier, which converts microcontrollers digital output into signals that drive the actuators. The haptic actuators can be controlled by signals arriving via serial, USB, and <a href="https://en.wikipedia.org/wiki/Bluetooth_Low_Energy">Bluetooth Low Energy</a> (BLE), as well as onboard microphones, using an <a href="https://www.nordicsemi.com/Products/nRF52840">nRF52840</a> microcontroller, which was chosen because it offers many input and output options and BLE, all in a small package. We added several sensors into the board to provide more experimental flexibility: an on-board digital microphone, an analog microphone amplifier, and an accelerometer. The firmware is a portable C/C++ library that works in the <a href="https://www.arduino.cc/">Arduino</a> ecosystem. </p> 
           <p> To allow for rapid iteration during development, the interface between the board and actuators is critical. The 12 tactile signals wiring have to be quick to set up in order to allow for such development, while being flexible and robust to stand up to prolonged use. For the interface, we use a 24-pin <a href="https://en.wikipedia.org/wiki/Flexible_electronics">FPC</a> (flexible printed circuit) connector on the VHP. We support interfacing to the actuators in two ways: with a custom flexible circuit board and with a rigid breakout board. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgGaJA_gO_Ag_vBL7liRlwHVUy3jZ72Hf6pFdepi7IDXy8zTeYyE0eAzy1j4uJQNv_1CY5x_jJzlKIpyEzvjthvnZx1BHnlK_xY2BbvZm_pvXCOeKV5lNT3GgeXvsOCmfSe1biCI_u0MWcvo7g7dk5fvhp0HtMTQZKPDtVtAbgEi_cWnMD57GuvcjeYkQ=s1999" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1475" data-original-width="1999" height="472" src="https://blogger.googleusercontent.com/img/a/AVvXsEgGaJA_gO_Ag_vBL7liRlwHVUy3jZ72Hf6pFdepi7IDXy8zTeYyE0eAzy1j4uJQNv_1CY5x_jJzlKIpyEzvjthvnZx1BHnlK_xY2BbvZm_pvXCOeKV5lNT3GgeXvsOCmfSe1biCI_u0MWcvo7g7dk5fvhp0HtMTQZKPDtVtAbgEi_cWnMD57GuvcjeYkQ=w640-h472" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">VHP board (small board on the <b>right</b>) connected to three different types of tactile actuators via rigid breakout board (large board on the <b>left</b>).</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Using Haptic Actuators as Sensors </b> <br> In our previous <a href="https://ai.googleblog.com/2020/11/haptics-with-input-using-linear.html">blog post</a>, we explored how back-EMF in a haptic actuator could be used for sensing and demonstrated a variety of useful applications. Instead of using back-EMF sensing in the VHP system, we measure the electrical current that drives each vibrotactile actuator and use the current load as the sensing mechanism. Unlike back-EMF sensing, this current-sensing approach allows simultaneous sensing and actuation, while minimizing the additional space needed on the board. </p> 
           <p> One challenge with the current-sensing approach is that there is a wide variety of vibrotactile actuators, each of which may behave differently and need different presets. In addition, because different actuators can be added and removed during prototyping with the adapter board, it would be useful if the VHP were able to identify the actuator automatically. This would improve the speed of prototyping and make the system more novice-friendly. </p> 
           <p> To explore this possibility, we collected current-load data from three off-the-shelf haptic actuators and trained a simple support vector machine classifier to recognize the difference in the signal pattern between actuators. The test accuracy was 100% for classifying the three actuators, indicating that each actuator has a very distinct response. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhNOYIBIiKxo66xrH0yK2qcwQ0_ah3XIJER3gfqCCTniFWa5W44GBZAv0Xumkpk65LxGw0GzHIM_yKd5adCveDFlaxJ_LFIn49s3SkJixPsi9yeYIljyo5mLgC8QOGiJDGZ6aNlTzp3Ua0UxePOoXiJsckoBck1VGQJu4pSVARWTPO69aYM23NSo5J2pw=s432" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="288" data-original-width="432" height="426" src="https://blogger.googleusercontent.com/img/a/AVvXsEhNOYIBIiKxo66xrH0yK2qcwQ0_ah3XIJER3gfqCCTniFWa5W44GBZAv0Xumkpk65LxGw0GzHIM_yKd5adCveDFlaxJ_LFIn49s3SkJixPsi9yeYIljyo5mLgC8QOGiJDGZ6aNlTzp3Ua0UxePOoXiJsckoBck1VGQJu4pSVARWTPO69aYM23NSo5J2pw=w640-h426" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Different actuators have a different current signature during a frequency sweep, thus allowing for automatic identification.</td>
             </tr>
            </tbody>
           </table> 
           <p> Additionally, vibrotactile actuators require proper contact with the skin for consistent control over stimulation. Thus, the device should measure skin contact and either provide an alert or self-adjust if it is not loaded correctly. To test whether a skin contact measuring technique works in practice, we measured the current load on actuators in a bracelet as it was tightened and loosened around the wrist. As the bracelet strap is tightened, the contact pressure between the skin and the actuator increases and the current required to drive the actuator signal increases commensurately. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEizoM-Cd1SBq3MnOPEYGS6_OApqRncer_EgY12rP4kpCTVYFt1PYnhS2qmavi0nLFwL0FiKR42NDvQPilKnNHn2H3hDE21Q4Y5qUrxkcdJvmQOmEIWb4fIPj8N0PFTOSorUi5gqaDCsoJCbTZWDWixB3ZyIcliSaCRhOJFSpRXcWZBA6zgbDzpcQQ4seg=s480" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="270" data-original-width="480" height="360" src="https://blogger.googleusercontent.com/img/a/AVvXsEizoM-Cd1SBq3MnOPEYGS6_OApqRncer_EgY12rP4kpCTVYFt1PYnhS2qmavi0nLFwL0FiKR42NDvQPilKnNHn2H3hDE21Q4Y5qUrxkcdJvmQOmEIWb4fIPj8N0PFTOSorUi5gqaDCsoJCbTZWDWixB3ZyIcliSaCRhOJFSpRXcWZBA6zgbDzpcQQ4seg=w640-h360" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Current load sensing is responding to touch, while the actuator is driven at 250 Hz frequency.<br><br></td>
             </tr>
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhXNb4PZEsIAdjL8XClJ_farl8QJpRzOFfcUMi6JLAejpFZ_j4a3kdiki7ImEI8dBXi5t2tBlFq9NjiDKsq-KH0IGds1M5jhiVkOzw9VNU5FBL8RbAvKwCS1QDlDPFzMGR-0S1B-zyciWzsirPi7rGCbC5XOl9aS_6r9T_pQhUQ311_AMf0STwBy1ColA=s480" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="270" data-original-width="480" height="360" src="https://blogger.googleusercontent.com/img/a/AVvXsEhXNb4PZEsIAdjL8XClJ_farl8QJpRzOFfcUMi6JLAejpFZ_j4a3kdiki7ImEI8dBXi5t2tBlFq9NjiDKsq-KH0IGds1M5jhiVkOzw9VNU5FBL8RbAvKwCS1QDlDPFzMGR-0S1B-zyciWzsirPi7rGCbC5XOl9aS_6r9T_pQhUQ311_AMf0STwBy1ColA=w640-h360" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Quality of the fit of the bracelet is measured.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Audio-to-Tactile Feedback </b> <br> To demonstrate the utility of the VHP platform, we used it to develop an audio-to-tactile feedback device to help with lipreading. Lipreading can be difficult for many speech sounds that look similar (<a href="https://en.wikipedia.org/wiki/Viseme">visemes</a>), such as pin and min. In order to help the user differentiate visemes like these, we attach a microphone to the VHP system, which can then pick up the speech sounds and translate the audio to vibrations on the wrist. For audio-to-tactile translation, we used our previously developed algorithms for real-time audio-to-tactile conversion, available via <a href="https://github.com/google/audio-to-tactile">GitHub</a>. Briefly, audio filters are paired with neural networks to recognize certain viesemes (e.g., picking up the hard consonant p in pin), and are then translated to vibrations in different parts of the bracelet. Our approach is inspired by <a href="https://ieeexplore.ieee.org/document/8423203">tactile phonemic sleeve</a> (TAPS), however the major difference is that in our approach the tactile signal is presented continuously and in real-time. </p> 
           <p> One of the developers who employs lipreading in daily life wore the bracelet daily for several months and found it to give better information to facilitate lipreading than previous devices, allowing improved understanding of lipreading visemes with the bracelet versus lipreading alone. In the future, we plan to conduct full-scale experiments with multiple users wearing the device for an extended time. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhtsbQUuAWwAtoAqzlM6lOevz0Et8FpMPgKMSnoDTsdaWag0-BlOhMw4iddEC_5TTN8h5pQ9LdxpXYCL-B9EK482S2MN4hZaIz6kK6-l5KAMLslFdPlu_ED6qoUQIDkCoj8SVnVSqlTsQbZqICoEFI7YHp4wWTI31IZk4tgcP4-tr1I8GGhuvP9-4ilfg=s950" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="320" data-original-width="950" height="216" src="https://blogger.googleusercontent.com/img/a/AVvXsEhtsbQUuAWwAtoAqzlM6lOevz0Et8FpMPgKMSnoDTsdaWag0-BlOhMw4iddEC_5TTN8h5pQ9LdxpXYCL-B9EK482S2MN4hZaIz6kK6-l5KAMLslFdPlu_ED6qoUQIDkCoj8SVnVSqlTsQbZqICoEFI7YHp4wWTI31IZk4tgcP4-tr1I8GGhuvP9-4ilfg=w640-h216" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;"><b>Left</b>: Audio-to-tactile sleeve. <b>Middle</b>: Audio-to-tactile bracelet. <b>Right</b>: One of our developers tests out the bracelets, which are worn on both arms.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Potential Applications</b> <br> The VHP platform enables rapid experimentation and prototyping that can be used to develop techniques for a variety of applications. For example: </p> 
           <ul> 
            <li><em>Rich haptics on small devices:</em> Expanding the number of actuators on mobile phones, which typically only have one or two, could be useful to provide additional tactile information. This is especially useful as fingers are sensitive to vibrations. We demonstrated a prototype mobile phone case with eight vibrotactile actuators. This could be used to provide rich notifications and enhance effects in a mobile game or when watching a video. </li>
            <li><em>Lab psychophysical experiments:<strong> </strong></em>Because VHP can be easily set up to send and receive haptic signals in real time, e.g., from a <a href="https://jupyter.org/">Jupyter notebook</a>, it could be used to perform real-time haptic experiments. </li>
            <li><em>Notifications and alerts:<strong> </strong></em>The wearable VHP could be used to provide haptic notifications from other devices, e.g., alerting if someone is at the door, and could even communicate distinguishable alerts through use of multiple actuators. </li>
            <li><em>Sensory substitution:<strong> </strong></em>Besides the lipreading assistance example above, there are many other potential applications for accessibility using sensory substitution, such as visual-to-tactile sensing or even sensing magnetic fields. </li>
            <li><em>Loading sensing:</em> The ability to sense from the haptic actuator current load is unique to our platform, and enables a variety of features, such as pressure sensing or automatically adjusting actuator output. </li> 
           </ul> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgChU2NcIS1N9ocnj3TNzDbDkyjOjd2PPk_xoWAJjVltWjgk0a5aTDvmSPKgqmTL5-n1bnv2rG9HfJf6foeRTApStcwYmuTXuFtomdKATpGQ2axD4j0PU_aymbobd3tmm4fbGWai1LClppW7IzIi_LxCycS4zli5-3-VlkjK8uQ_P53Pa4v7UIoL5Z4KQ=s1491" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="344" data-original-width="1491" height="148" src="https://blogger.googleusercontent.com/img/a/AVvXsEgChU2NcIS1N9ocnj3TNzDbDkyjOjd2PPk_xoWAJjVltWjgk0a5aTDvmSPKgqmTL5-n1bnv2rG9HfJf6foeRTApStcwYmuTXuFtomdKATpGQ2axD4j0PU_aymbobd3tmm4fbGWai1LClppW7IzIi_LxCycS4zli5-3-VlkjK8uQ_P53Pa4v7UIoL5Z4KQ=w640-h148" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Integrating eight voice coils into a phone case. We used loading sensing to understand which voice coils are being touched. <br></td>
             </tr>
            </tbody>
           </table> 
           <p> <b>What's next?</b> <br> We hope that others can utilize the platform to build a diverse set of applications. If you are interested and have ideas about using our platform or want to receive updates, please fill out this <a href="https://sites.google.com/view/vhp-collaborations/home">form</a>. We hope that with this platform, we can help democratize the use of haptics and inspire a more widespread use of tactile devices. </p> 
           <p> <b>Acknowledgments</b> <br> <em>This work was done by Artem Dementyev, Pascal Getreuer, Dimitri Kanevsky, Malcolm Slaney and Richard Lyon. We thank Alex Olwal, Thad Starner, Hong Tan, Charlotte Reed, Sarah Sterman for valuable feedback and discussion on the paper. Yuhui Zhao, Dmitrii Votintcev, Chet Gnegy, Whitney Bai and Sagar Savla for feedback on the design and engineering. </em> </p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:An Open Source Vibrotactile Haptics Platform for On-Body Applications&amp;url=http://ai.googleblog.com/2021/11/an-open-source-vibrotactile-haptics.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/11/an-open-source-vibrotactile-haptics.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="post" data-id="7905810780558994646" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/11/making-better-future-predictions-by.html" itemprop="url" title="Making Better Future Predictions by Watching Unlabeled Videos"> Making Better Future Predictions by Watching Unlabeled Videos </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Thursday, November 11, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Posted by Dave Epstein, Student Researcher and Chen Sun, Staff Research Scientist, Google Research</span>



<p>
Machine learning (ML) agents are increasingly deployed in the real world to make decisions and assist people in their daily lives. Making reasonable predictions about the future at varying timescales is one of the most important capabilities for such agents because it enables them to predict changes in the world around them,<a href="https://blog.waymo.com/2020/05/vectornet.html"> including other agents&#8217; behaviors</a>, and<a href="https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html"> plan how to act next</a>. Importantly, successful future prediction requires both capturing meaningful transitions in the environment (e.g., dough transforming into bread) and adapting to how transitions unfold over time in order to make decisions. 
</p>


<p>
Previous work in future prediction from visual observations has largely been constrained by the format of its output (e.g., pixels that represent an image) or<a href="https://research.google.com/ava/"> a manually-defined set of human activities</a> (e.g., predicting if someone will keep walking, sit down, or jump). These are either too detailed and hard to predict or lack important information about the richness of the real world. For example, predicting &#8220;person jumping&#8221; does not capture why they&#8217;re jumping, what they&#8217;re jumping onto, etc. Also, with<a href="https://arxiv.org/abs/1808.07784"> very few exceptions</a>, previous models were designed to make predictions at a <em>fixed offset</em> into the future, which is a limiting assumption because we rarely know when <em>meaningful</em> future states will happen. 
</p>


<p>
For example, in a video about making ice cream (depicted below), the meaningful transition from &#8220;cream&#8221; to &#8220;ice cream&#8221; occurs over 35 seconds, so models predicting such transitions would need to look 35 seconds ahead. But this time interval varies a large amount across different activities and videos &#8212; meaningful transitions occur at any distance into the future. Learning to make such predictions at flexible intervals is hard because the desired ground truth may be relatively ambiguous. For example, the correct prediction could be the just-churned ice cream in the machine, or scoops of the ice cream in a bowl. In addition, collecting such annotations at scale (i.e., frame-by-frame for millions of videos) is infeasible. However, many existing <a href="https://ai.googleblog.com/2019/09/learning-cross-modal-temporal.html">instructional videos come with speech transcripts</a>, which often offer concise, general descriptions throughout entire videos. This source of data can guide a model&#8217;s attention toward important parts of the video, obviating the need for manual labeling and allowing a flexible, data-driven definition of the future.
</p>


<p>
In &#8220;<a href="https://arxiv.org/abs/2101.02337">Learning Temporal Dynamics from Cycles in Narrated Video</a>&#8221;, published at <a href="https://iccv2021.thecvf.com/home">ICCV 2021</a>, we propose an approach that is <a href="https://en.wikipedia.org/wiki/Self-supervised_learning">self-supervised</a>, using a recent <a href="https://www.di.ens.fr/willow/research/howto100m/">large unlabeled dataset of diverse human action</a>. The resulting model operates at a high level of abstraction, can make predictions arbitrarily far into the future, and chooses how far into the future to predict based on context. Called <a href="https://arxiv.org/abs/2101.02337">Multi-Modal Cycle Consistency</a> (MMCC), it leverages narrated instructional video to learn a strong predictive model of the future. We demonstrate how MMCC can be applied, without fine-tuning, to a variety of challenging tasks, and qualitatively examine its predictions. In the example below, MMCC predicts the future (d) from present frame (a), rather than less relevant potential futures (b) or (c).
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgw8vIpMPXOxDZD-gNBfsI_fVW_wT0Ws1cdD11zOgBdIFfSi2SBwnB5PaKfVCjIlErdyWEMejEF0Nb7L2M1QwyXu0tiV-pLRs7j2zyqYXpocVsaL5U7PPlEKjI6qcU6fEIzpUmc_fh1-sajZFz6ps9GEaDbLtcwfjyM8O9tjoGm1fx_dlX_uqQ3jd5zag=s3000" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="958" data-original-width="3000" height="204" src="https://blogger.googleusercontent.com/img/a/AVvXsEgw8vIpMPXOxDZD-gNBfsI_fVW_wT0Ws1cdD11zOgBdIFfSi2SBwnB5PaKfVCjIlErdyWEMejEF0Nb7L2M1QwyXu0tiV-pLRs7j2zyqYXpocVsaL5U7PPlEKjI6qcU6fEIzpUmc_fh1-sajZFz6ps9GEaDbLtcwfjyM8O9tjoGm1fx_dlX_uqQ3jd5zag=w640-h204" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">This work uses cues from vision and language to predict high-level changes (such as cream becoming ice cream) in video (video from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td></tr></tbody></table>


<p>
<b>Viewing Videos as Graphs</b>
<br />
The foundation of our method is to represent narrated videos as <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">graphs</a>. We view videos as a collection of nodes, where nodes are either video frames (sampled at 1 frame per second) or segments of narrated text (extracted with <a href="https://en.wikipedia.org/wiki/Speech_recognition">automatic speech recognition</a> systems), <a href="https://en.wikipedia.org/wiki/Feature_learning">encoded by neural networks</a>. During training, MMCC constructs a graph from the nodes, using <em>cross-modal</em> edges to connect video frames and text segments that refer to the same state, and <em>temporal</em> edges to connect the present (e.g., strawberry-flavored cream) and the future (e.g., soft-serve ice cream). The temporal edges operate on both modalities equally &#8212; they can start from either a video frame, some text, or both, and can connect to a future (or past) state in either modality. MMCC achieves this by learning a <em><a href="https://en.wikipedia.org/wiki/Latent_space">latent representation</a> </em>shared by frames and text and then making predictions in this representation space. 
</p>

<p>
<b>Multi-modal Cycle Consistency</b>
<br />
To learn the cross-modal and temporal edge functions without supervision, we apply the idea of <a href="https://ai.googleblog.com/2019/08/video-understanding-using-temporal.html">cycle consistency</a>. Here, cycle consistency refers to the construction of <a href="https://en.wikipedia.org/wiki/Cycle_graph">cycle graphs</a>, in which the model constructs a series of edges from an initial node to other nodes and back again: Given a start node (e.g., a sample video frame), the model is expected to find its cross-modal counterpart (i.e., text describing the frame) and combine them as the <em>present state</em>. To do this, at the start of training, the model assumes that frames and text with the same timestamps are counterparts, but then relaxes this assumption later. The model then predicts a <em>future state, </em>and the node most similar to this prediction is selected. Finally, the model attempts to invert the above steps by predicting the present state <em>backward</em> from the future node, and thus connecting the future node back with the start node. 
</p>

<p>
The discrepancy between the model&#8217;s <em>prediction of the present</em> from the future and the <em>actual present</em> is the <em>cycle-consistency loss</em>. Intuitively, this training objective requires the predicted future to contain enough information about its past to be invertible, leading to predictions that correspond to meaningful changes to the same entities (e.g., tomato becoming marinara sauce, or flour and eggs in a bowl becoming dough). Moreover, the inclusion of cross-modal edges ensures future predictions are meaningful in either modality. 
</p>

<p>
To learn the temporal and cross-modal edge functions end-to-end, we use the <a href="https://arxiv.org/abs/1706.03762">soft attention</a> technique, which first outputs how likely each node is to be the target node of the edge, and then &#8220;picks&#8221; a node by taking the weighted average among all possible candidates. Importantly, this cyclic graph constraint makes few assumptions for the kind of temporal edges the model should learn, as long as they end up forming a consistent cycle. This enables the emergence of long-term temporal dynamics critical for future prediction without requiring manual labels of meaningful changes. 
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiMMjGh-S7Zv1xoGg8Y-vuGEXez0Zo-egRNlFTizDtyQHfbDA0xDJUizf4TQrYq-jJmx6qcSz8wRVIO8yVaYOLu3fITqYzqMmnCwCrW8xRWQarZUscnH2vBwSC7vJ8pD-A24PnoxpShH7uc9aFewKRL1vYHhiX-BmwM-qxEttRA-bxJUAODNfi5sWUMyg=s1991" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1540" data-original-width="1991" height="496" src="https://blogger.googleusercontent.com/img/a/AVvXsEiMMjGh-S7Zv1xoGg8Y-vuGEXez0Zo-egRNlFTizDtyQHfbDA0xDJUizf4TQrYq-jJmx6qcSz8wRVIO8yVaYOLu3fITqYzqMmnCwCrW8xRWQarZUscnH2vBwSC7vJ8pD-A24PnoxpShH7uc9aFewKRL1vYHhiX-BmwM-qxEttRA-bxJUAODNfi5sWUMyg=w640-h496" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">An example of the training objective: A cycle graph is expected to be constructed between the chicken with soy sauce and the chicken in chili oil because they are two adjacent steps in the chicken&#8217;s preparation (video from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td></tr></tbody></table>


<p>
<b>Discovering Cycles in Real-World Video</b>
<br />
MMCC is trained without any explicit ground truth, using only long video sequences and randomly sampled starting conditions (a frame or text excerpt) and asking the model to find temporal cycles. After training, MMCC can identify meaningful cycles that capture complex changes in video.</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEg7n6Sj9wjqRYDhmE9wFMTHJO9dao7v3aQWM0ALrZyBr7EerkEOcVVG0vAiqtob-99TTO0KdxgRKPs0q43ygzTQz623cce8_acRMxWN2WEp-j9UzswtfASLN95GF49AKxUjIUAgRmAumgpYTek4CDxKAyzXrM2d8CGv9iDE5IfyowQ0gaMcoSY_u7fnjQ=s1741" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="898" data-original-width="1741" height="330" src="https://blogger.googleusercontent.com/img/a/AVvXsEg7n6Sj9wjqRYDhmE9wFMTHJO9dao7v3aQWM0ALrZyBr7EerkEOcVVG0vAiqtob-99TTO0KdxgRKPs0q43ygzTQz623cce8_acRMxWN2WEp-j9UzswtfASLN95GF49AKxUjIUAgRmAumgpYTek4CDxKAyzXrM2d8CGv9iDE5IfyowQ0gaMcoSY_u7fnjQ=w640-h330" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Given frames as input (<b>left</b>), MMCC selects relevant text from video narrations and uses both modalities to predict a future frame (<b>middle</b>). It then finds text relevant to this future and uses it to predict the past (<b>right</b>). Using its knowledge of how objects and scenes change over time, MMCC &#8220;closes the cycle&#8221; and ends up where it started (videos from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td></tr></tbody></table>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhIZOd1Xl4TJFdohFOUGCR0qXGTiCVXVmESDFi1wf0YP0efTgDVZJpWrgoUa0tenwIGwa7S1I2c2209uO4_yrrTlv7xqgFQTNVuqTQ_D-V6F0lLW09LsuBF7qU3NpvHDEfz5q30uYl-pVSw35VFn-rm4xL8GgpcfZ9A-nRKZ5Nm8XTWBCV2748NAZY52g=s1745" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="912" data-original-width="1745" height="334" src="https://blogger.googleusercontent.com/img/a/AVvXsEhIZOd1Xl4TJFdohFOUGCR0qXGTiCVXVmESDFi1wf0YP0efTgDVZJpWrgoUa0tenwIGwa7S1I2c2209uO4_yrrTlv7xqgFQTNVuqTQ_D-V6F0lLW09LsuBF7qU3NpvHDEfz5q30uYl-pVSw35VFn-rm4xL8GgpcfZ9A-nRKZ5Nm8XTWBCV2748NAZY52g=w640-h334" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The model can also start from narrated text rather than frames and still find relevant transitions (videos from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td></tr></tbody></table>



<p>
<b>Zero-Shot Applications</b>
<br />
For MMCC to identify meaningful transitions over time in an entire video, we define a &#8220;likely transition score&#8221; for each pair (A, B) of frames in a video, according to the model's predictions &#8212; the closer B is to our model&#8217;s prediction of the future of A, the higher the score assigned. We then rank all pairs according to this score and show the highest-scoring pairs of present and future frames detected in previously unseen videos (examples below).</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgThz086aBJ2pX3SY07njOnCKImRYeOEDz85Q9rnbPYSAWmqEBm9zWVUOqyGkEhy_42VqJQwJDOZUbxu8IPfL-cYRXUHGuEUbz9l818VRqQ3Xol3Fb61GG3ZIR-XE2R-E0C9Yo0izAVyR4-gTdlKzQy0F4QvwZGxylUt5PhaQWxkAuJs-i7C7RHRnAlEA=s1331" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1050" data-original-width="1331" height="504" src="https://blogger.googleusercontent.com/img/a/AVvXsEgThz086aBJ2pX3SY07njOnCKImRYeOEDz85Q9rnbPYSAWmqEBm9zWVUOqyGkEhy_42VqJQwJDOZUbxu8IPfL-cYRXUHGuEUbz9l818VRqQ3Xol3Fb61GG3ZIR-XE2R-E0C9Yo0izAVyR4-gTdlKzQy0F4QvwZGxylUt5PhaQWxkAuJs-i7C7RHRnAlEA=w640-h504" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The highest-scoring pairs from eight random videos, which showcase the versatility of the model across a wide range of tasks (videos from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td></tr></tbody></table>




<p>
We can use this same approach to temporally sort an unordered collection of video frames without any fine-tuning by finding an ordering that maximizes the overall confidence scores between all adjacent frames in the sorted sequence.
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjx3aLYLFYfJzrdC7S6D3Wf5b3gGl40q28No-Qc9Csw7GzQjV8FNXxi3z4ZdJ8iKeOKf4Zjs6C0YsAvI17VfjGFUA4ooyiAf1LbxePIT9GgEaGNq8fU4A5yV683Lx2LHI2ZM-R_aQjKjWLWuFJMagu_4EUS6pjyvd6SdnbOkaJEGtG3p-94xGPDNNX4fA=s1390" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="804" data-original-width="1390" height="370" src="https://blogger.googleusercontent.com/img/a/AVvXsEjx3aLYLFYfJzrdC7S6D3Wf5b3gGl40q28No-Qc9Csw7GzQjV8FNXxi3z4ZdJ8iKeOKf4Zjs6C0YsAvI17VfjGFUA4ooyiAf1LbxePIT9GgEaGNq8fU4A5yV683Lx2LHI2ZM-R_aQjKjWLWuFJMagu_4EUS6pjyvd6SdnbOkaJEGtG3p-94xGPDNNX4fA=w640-h370" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><b>Left</b>: Shuffled frames from three videos. <b>Right</b>: MMCC unshuffles the frames. The true order is shown under each frame. Even when MMCC does not predict the ground truth, its predictions often appear reasonable, and so, it can present an alternate ordering (videos from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td></tr></tbody></table>



<p>
<b>Evaluating Future Prediction</b>
<br />
We evaluate the model&#8217;s ability to anticipate action, potentially minutes in advance, using the <a href="https://en.wikipedia.org/wiki/Precision_and_recall">top-k recall metric</a>, which here measures a model&#8217;s ability to retrieve the correct future (higher is better). On <a href="https://arxiv.org/abs/1903.08225">CrossTask</a>, a dataset of instruction videos with labels describing key steps, MMCC outperforms the previous self-supervised state-of-the-art models in inferring possible future actions. 
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container">
  <tbody><tr>
   <td>
   </td>
   <td colspan="7"><b>Recall</b>
   </td>
  </tr>
  <tr>
   <td style="text-align: left;"><b>Model</b>
   </td>
   <td style="text-align: center;">&nbsp;&nbsp;&nbsp;<b>Top-1</b>&nbsp;&nbsp;&nbsp;
   </td>
   <td style="text-align: center;">&nbsp;&nbsp;&nbsp;<b>Top-5</b>&nbsp;&nbsp;&nbsp;
   </td>
   <td style="text-align: center;">&nbsp;&nbsp;&nbsp;<b>Top-10</b>&nbsp;&nbsp;&nbsp;
   </td>
  </tr>
  <tr>
   <td style="text-align: left;"><a href="https://arxiv.org/abs/1912.06430">Cross-modal</a>&nbsp;&nbsp;&nbsp;
   </td>
   <td style="text-align: center;">2.9
   </td>
   <td style="text-align: center;">14.2
   </td>
   <td style="text-align: center;">24.3
   </td>
  </tr>
  <tr>
   <td style="text-align: left;"><a href="https://arxiv.org/abs/1504.08023">Repr. Ant.</a>
   </td>
   <td style="text-align: center;">3.0
   </td>
   <td style="text-align: center;">13.3
   </td>
   <td style="text-align: center;">26.0
   </td>
  </tr>
  <tr>
   <td style="text-align: left;"><a href="https://arxiv.org/abs/2008.01065">MemDPC</a>
   </td>
   <td style="text-align: center;">2.9
   </td>
   <td style="text-align: center;">15.8
   </td>
   <td style="text-align: center;">27.4
   </td>
  </tr>
  <tr>
   <td style="text-align: left;"><a href="https://arxiv.org/abs/1808.07784">TAP</a>
   </td>
   <td style="text-align: center;">4.5
   </td>
   <td style="text-align: center;">17.1
   </td>
   <td style="text-align: center;">27.9
   </td>
  </tr>
  <tr>
   <td style="text-align: left;"><b>MMCC</b>
   </td>
   <td style="text-align: center;"><b>5.4</b>
   </td>
   <td style="text-align: center;"><b>19.9</b>
   </td>
   <td style="text-align: center;"><b>33.8</b>
   </td>
  </tr>
</tbody></table>


<p>
<b>Conclusions</b>
<br />
We have introduced a self-supervised method to learn temporal dynamics by cycling through narrated instructional videos. Despite the simplicity of the model&#8217;s architecture, it can discover meaningful long-term transitions in vision and language, and can be applied without further training to challenging downstream tasks, such as anticipating far-away action and ordering collections of images. An interesting future direction is transferring the model to agents so they can use it to conduct long-term planning. 
</p>

<p>
<b>Acknowledgements</b>
<br />
<em>The core team includes Dave Epstein, Jiajun Wu, Cordelia Schmid, and Chen Sun. We thank Alexei Efros, Mia Chiquier, and Shiry Ginosar for their feedback, and Allan Jabri for inspiration in figure design. Dave would like to thank Ddac Surs and Carl Vondrick for insightful early discussions on cycling through time in video.</em>
</p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Posted by Dave Epstein, Student Researcher and Chen Sun, Staff Research Scientist, Google Research</span> 
           <p> Machine learning (ML) agents are increasingly deployed in the real world to make decisions and assist people in their daily lives. Making reasonable predictions about the future at varying timescales is one of the most important capabilities for such agents because it enables them to predict changes in the world around them,<a href="https://blog.waymo.com/2020/05/vectornet.html"> including other agents behaviors</a>, and<a href="https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html"> plan how to act next</a>. Importantly, successful future prediction requires both capturing meaningful transitions in the environment (e.g., dough transforming into bread) and adapting to how transitions unfold over time in order to make decisions. </p> 
           <p> Previous work in future prediction from visual observations has largely been constrained by the format of its output (e.g., pixels that represent an image) or<a href="https://research.google.com/ava/"> a manually-defined set of human activities</a> (e.g., predicting if someone will keep walking, sit down, or jump). These are either too detailed and hard to predict or lack important information about the richness of the real world. For example, predicting person jumping does not capture why theyre jumping, what theyre jumping onto, etc. Also, with<a href="https://arxiv.org/abs/1808.07784"> very few exceptions</a>, previous models were designed to make predictions at a <em>fixed offset</em> into the future, which is a limiting assumption because we rarely know when <em>meaningful</em> future states will happen. </p> 
           <p> For example, in a video about making ice cream (depicted below), the meaningful transition from cream to ice cream occurs over 35 seconds, so models predicting such transitions would need to look 35 seconds ahead. But this time interval varies a large amount across different activities and videos  meaningful transitions occur at any distance into the future. Learning to make such predictions at flexible intervals is hard because the desired ground truth may be relatively ambiguous. For example, the correct prediction could be the just-churned ice cream in the machine, or scoops of the ice cream in a bowl. In addition, collecting such annotations at scale (i.e., frame-by-frame for millions of videos) is infeasible. However, many existing <a href="https://ai.googleblog.com/2019/09/learning-cross-modal-temporal.html">instructional videos come with speech transcripts</a>, which often offer concise, general descriptions throughout entire videos. This source of data can guide a models attention toward important parts of the video, obviating the need for manual labeling and allowing a flexible, data-driven definition of the future. </p> 
           <p> In <a href="https://arxiv.org/abs/2101.02337">Learning Temporal Dynamics from Cycles in Narrated Video</a>, published at <a href="https://iccv2021.thecvf.com/home">ICCV 2021</a>, we propose an approach that is <a href="https://en.wikipedia.org/wiki/Self-supervised_learning">self-supervised</a>, using a recent <a href="https://www.di.ens.fr/willow/research/howto100m/">large unlabeled dataset of diverse human action</a>. The resulting model operates at a high level of abstraction, can make predictions arbitrarily far into the future, and chooses how far into the future to predict based on context. Called <a href="https://arxiv.org/abs/2101.02337">Multi-Modal Cycle Consistency</a> (MMCC), it leverages narrated instructional video to learn a strong predictive model of the future. We demonstrate how MMCC can be applied, without fine-tuning, to a variety of challenging tasks, and qualitatively examine its predictions. In the example below, MMCC predicts the future (d) from present frame (a), rather than less relevant potential futures (b) or (c). </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgw8vIpMPXOxDZD-gNBfsI_fVW_wT0Ws1cdD11zOgBdIFfSi2SBwnB5PaKfVCjIlErdyWEMejEF0Nb7L2M1QwyXu0tiV-pLRs7j2zyqYXpocVsaL5U7PPlEKjI6qcU6fEIzpUmc_fh1-sajZFz6ps9GEaDbLtcwfjyM8O9tjoGm1fx_dlX_uqQ3jd5zag=s3000" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="958" data-original-width="3000" height="204" src="https://blogger.googleusercontent.com/img/a/AVvXsEgw8vIpMPXOxDZD-gNBfsI_fVW_wT0Ws1cdD11zOgBdIFfSi2SBwnB5PaKfVCjIlErdyWEMejEF0Nb7L2M1QwyXu0tiV-pLRs7j2zyqYXpocVsaL5U7PPlEKjI6qcU6fEIzpUmc_fh1-sajZFz6ps9GEaDbLtcwfjyM8O9tjoGm1fx_dlX_uqQ3jd5zag=w640-h204" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">This work uses cues from vision and language to predict high-level changes (such as cream becoming ice cream) in video (video from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Viewing Videos as Graphs</b> <br> The foundation of our method is to represent narrated videos as <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">graphs</a>. We view videos as a collection of nodes, where nodes are either video frames (sampled at 1 frame per second) or segments of narrated text (extracted with <a href="https://en.wikipedia.org/wiki/Speech_recognition">automatic speech recognition</a> systems), <a href="https://en.wikipedia.org/wiki/Feature_learning">encoded by neural networks</a>. During training, MMCC constructs a graph from the nodes, using <em>cross-modal</em> edges to connect video frames and text segments that refer to the same state, and <em>temporal</em> edges to connect the present (e.g., strawberry-flavored cream) and the future (e.g., soft-serve ice cream). The temporal edges operate on both modalities equally  they can start from either a video frame, some text, or both, and can connect to a future (or past) state in either modality. MMCC achieves this by learning a <em><a href="https://en.wikipedia.org/wiki/Latent_space">latent representation</a> </em>shared by frames and text and then making predictions in this representation space. </p> 
           <p> <b>Multi-modal Cycle Consistency</b> <br> To learn the cross-modal and temporal edge functions without supervision, we apply the idea of <a href="https://ai.googleblog.com/2019/08/video-understanding-using-temporal.html">cycle consistency</a>. Here, cycle consistency refers to the construction of <a href="https://en.wikipedia.org/wiki/Cycle_graph">cycle graphs</a>, in which the model constructs a series of edges from an initial node to other nodes and back again: Given a start node (e.g., a sample video frame), the model is expected to find its cross-modal counterpart (i.e., text describing the frame) and combine them as the <em>present state</em>. To do this, at the start of training, the model assumes that frames and text with the same timestamps are counterparts, but then relaxes this assumption later. The model then predicts a <em>future state, </em>and the node most similar to this prediction is selected. Finally, the model attempts to invert the above steps by predicting the present state <em>backward</em> from the future node, and thus connecting the future node back with the start node. </p> 
           <p> The discrepancy between the models <em>prediction of the present</em> from the future and the <em>actual present</em> is the <em>cycle-consistency loss</em>. Intuitively, this training objective requires the predicted future to contain enough information about its past to be invertible, leading to predictions that correspond to meaningful changes to the same entities (e.g., tomato becoming marinara sauce, or flour and eggs in a bowl becoming dough). Moreover, the inclusion of cross-modal edges ensures future predictions are meaningful in either modality. </p> 
           <p> To learn the temporal and cross-modal edge functions end-to-end, we use the <a href="https://arxiv.org/abs/1706.03762">soft attention</a> technique, which first outputs how likely each node is to be the target node of the edge, and then picks a node by taking the weighted average among all possible candidates. Importantly, this cyclic graph constraint makes few assumptions for the kind of temporal edges the model should learn, as long as they end up forming a consistent cycle. This enables the emergence of long-term temporal dynamics critical for future prediction without requiring manual labels of meaningful changes. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiMMjGh-S7Zv1xoGg8Y-vuGEXez0Zo-egRNlFTizDtyQHfbDA0xDJUizf4TQrYq-jJmx6qcSz8wRVIO8yVaYOLu3fITqYzqMmnCwCrW8xRWQarZUscnH2vBwSC7vJ8pD-A24PnoxpShH7uc9aFewKRL1vYHhiX-BmwM-qxEttRA-bxJUAODNfi5sWUMyg=s1991" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1540" data-original-width="1991" height="496" src="https://blogger.googleusercontent.com/img/a/AVvXsEiMMjGh-S7Zv1xoGg8Y-vuGEXez0Zo-egRNlFTizDtyQHfbDA0xDJUizf4TQrYq-jJmx6qcSz8wRVIO8yVaYOLu3fITqYzqMmnCwCrW8xRWQarZUscnH2vBwSC7vJ8pD-A24PnoxpShH7uc9aFewKRL1vYHhiX-BmwM-qxEttRA-bxJUAODNfi5sWUMyg=w640-h496" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">An example of the training objective: A cycle graph is expected to be constructed between the chicken with soy sauce and the chicken in chili oil because they are two adjacent steps in the chickens preparation (video from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Discovering Cycles in Real-World Video</b> <br> MMCC is trained without any explicit ground truth, using only long video sequences and randomly sampled starting conditions (a frame or text excerpt) and asking the model to find temporal cycles. After training, MMCC can identify meaningful cycles that capture complex changes in video.</p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEg7n6Sj9wjqRYDhmE9wFMTHJO9dao7v3aQWM0ALrZyBr7EerkEOcVVG0vAiqtob-99TTO0KdxgRKPs0q43ygzTQz623cce8_acRMxWN2WEp-j9UzswtfASLN95GF49AKxUjIUAgRmAumgpYTek4CDxKAyzXrM2d8CGv9iDE5IfyowQ0gaMcoSY_u7fnjQ=s1741" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="898" data-original-width="1741" height="330" src="https://blogger.googleusercontent.com/img/a/AVvXsEg7n6Sj9wjqRYDhmE9wFMTHJO9dao7v3aQWM0ALrZyBr7EerkEOcVVG0vAiqtob-99TTO0KdxgRKPs0q43ygzTQz623cce8_acRMxWN2WEp-j9UzswtfASLN95GF49AKxUjIUAgRmAumgpYTek4CDxKAyzXrM2d8CGv9iDE5IfyowQ0gaMcoSY_u7fnjQ=w640-h330" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Given frames as input (<b>left</b>), MMCC selects relevant text from video narrations and uses both modalities to predict a future frame (<b>middle</b>). It then finds text relevant to this future and uses it to predict the past (<b>right</b>). Using its knowledge of how objects and scenes change over time, MMCC closes the cycle and ends up where it started (videos from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td>
             </tr>
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhIZOd1Xl4TJFdohFOUGCR0qXGTiCVXVmESDFi1wf0YP0efTgDVZJpWrgoUa0tenwIGwa7S1I2c2209uO4_yrrTlv7xqgFQTNVuqTQ_D-V6F0lLW09LsuBF7qU3NpvHDEfz5q30uYl-pVSw35VFn-rm4xL8GgpcfZ9A-nRKZ5Nm8XTWBCV2748NAZY52g=s1745" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="912" data-original-width="1745" height="334" src="https://blogger.googleusercontent.com/img/a/AVvXsEhIZOd1Xl4TJFdohFOUGCR0qXGTiCVXVmESDFi1wf0YP0efTgDVZJpWrgoUa0tenwIGwa7S1I2c2209uO4_yrrTlv7xqgFQTNVuqTQ_D-V6F0lLW09LsuBF7qU3NpvHDEfz5q30uYl-pVSw35VFn-rm4xL8GgpcfZ9A-nRKZ5Nm8XTWBCV2748NAZY52g=w640-h334" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">The model can also start from narrated text rather than frames and still find relevant transitions (videos from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Zero-Shot Applications</b> <br> For MMCC to identify meaningful transitions over time in an entire video, we define a likely transition score for each pair (A, B) of frames in a video, according to the model's predictions  the closer B is to our models prediction of the future of A, the higher the score assigned. We then rank all pairs according to this score and show the highest-scoring pairs of present and future frames detected in previously unseen videos (examples below).</p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgThz086aBJ2pX3SY07njOnCKImRYeOEDz85Q9rnbPYSAWmqEBm9zWVUOqyGkEhy_42VqJQwJDOZUbxu8IPfL-cYRXUHGuEUbz9l818VRqQ3Xol3Fb61GG3ZIR-XE2R-E0C9Yo0izAVyR4-gTdlKzQy0F4QvwZGxylUt5PhaQWxkAuJs-i7C7RHRnAlEA=s1331" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1050" data-original-width="1331" height="504" src="https://blogger.googleusercontent.com/img/a/AVvXsEgThz086aBJ2pX3SY07njOnCKImRYeOEDz85Q9rnbPYSAWmqEBm9zWVUOqyGkEhy_42VqJQwJDOZUbxu8IPfL-cYRXUHGuEUbz9l818VRqQ3Xol3Fb61GG3ZIR-XE2R-E0C9Yo0izAVyR4-gTdlKzQy0F4QvwZGxylUt5PhaQWxkAuJs-i7C7RHRnAlEA=w640-h504" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">The highest-scoring pairs from eight random videos, which showcase the versatility of the model across a wide range of tasks (videos from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td>
             </tr>
            </tbody>
           </table> 
           <p> We can use this same approach to temporally sort an unordered collection of video frames without any fine-tuning by finding an ordering that maximizes the overall confidence scores between all adjacent frames in the sorted sequence. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjx3aLYLFYfJzrdC7S6D3Wf5b3gGl40q28No-Qc9Csw7GzQjV8FNXxi3z4ZdJ8iKeOKf4Zjs6C0YsAvI17VfjGFUA4ooyiAf1LbxePIT9GgEaGNq8fU4A5yV683Lx2LHI2ZM-R_aQjKjWLWuFJMagu_4EUS6pjyvd6SdnbOkaJEGtG3p-94xGPDNNX4fA=s1390" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="804" data-original-width="1390" height="370" src="https://blogger.googleusercontent.com/img/a/AVvXsEjx3aLYLFYfJzrdC7S6D3Wf5b3gGl40q28No-Qc9Csw7GzQjV8FNXxi3z4ZdJ8iKeOKf4Zjs6C0YsAvI17VfjGFUA4ooyiAf1LbxePIT9GgEaGNq8fU4A5yV683Lx2LHI2ZM-R_aQjKjWLWuFJMagu_4EUS6pjyvd6SdnbOkaJEGtG3p-94xGPDNNX4fA=w640-h370" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;"><b>Left</b>: Shuffled frames from three videos. <b>Right</b>: MMCC unshuffles the frames. The true order is shown under each frame. Even when MMCC does not predict the ground truth, its predictions often appear reasonable, and so, it can present an alternate ordering (videos from <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>).</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Evaluating Future Prediction</b> <br> We evaluate the models ability to anticipate action, potentially minutes in advance, using the <a href="https://en.wikipedia.org/wiki/Precision_and_recall">top-k recall metric</a>, which here measures a models ability to retrieve the correct future (higher is better). On <a href="https://arxiv.org/abs/1903.08225">CrossTask</a>, a dataset of instruction videos with labels describing key steps, MMCC outperforms the previous self-supervised state-of-the-art models in inferring possible future actions. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container"> 
            <tbody>
             <tr> 
              <td> </td> 
              <td colspan="7"><b>Recall</b> </td> 
             </tr> 
             <tr> 
              <td style="text-align: left;"><b>Model</b> </td> 
              <td style="text-align: center;">&nbsp;&nbsp;&nbsp;<b>Top-1</b>&nbsp;&nbsp;&nbsp; </td> 
              <td style="text-align: center;">&nbsp;&nbsp;&nbsp;<b>Top-5</b>&nbsp;&nbsp;&nbsp; </td> 
              <td style="text-align: center;">&nbsp;&nbsp;&nbsp;<b>Top-10</b>&nbsp;&nbsp;&nbsp; </td> 
             </tr> 
             <tr> 
              <td style="text-align: left;"><a href="https://arxiv.org/abs/1912.06430">Cross-modal</a>&nbsp;&nbsp;&nbsp; </td> 
              <td style="text-align: center;">2.9 </td> 
              <td style="text-align: center;">14.2 </td> 
              <td style="text-align: center;">24.3 </td> 
             </tr> 
             <tr> 
              <td style="text-align: left;"><a href="https://arxiv.org/abs/1504.08023">Repr. Ant.</a> </td> 
              <td style="text-align: center;">3.0 </td> 
              <td style="text-align: center;">13.3 </td> 
              <td style="text-align: center;">26.0 </td> 
             </tr> 
             <tr> 
              <td style="text-align: left;"><a href="https://arxiv.org/abs/2008.01065">MemDPC</a> </td> 
              <td style="text-align: center;">2.9 </td> 
              <td style="text-align: center;">15.8 </td> 
              <td style="text-align: center;">27.4 </td> 
             </tr> 
             <tr> 
              <td style="text-align: left;"><a href="https://arxiv.org/abs/1808.07784">TAP</a> </td> 
              <td style="text-align: center;">4.5 </td> 
              <td style="text-align: center;">17.1 </td> 
              <td style="text-align: center;">27.9 </td> 
             </tr> 
             <tr> 
              <td style="text-align: left;"><b>MMCC</b> </td> 
              <td style="text-align: center;"><b>5.4</b> </td> 
              <td style="text-align: center;"><b>19.9</b> </td> 
              <td style="text-align: center;"><b>33.8</b> </td> 
             </tr> 
            </tbody>
           </table> 
           <p> <b>Conclusions</b> <br> We have introduced a self-supervised method to learn temporal dynamics by cycling through narrated instructional videos. Despite the simplicity of the models architecture, it can discover meaningful long-term transitions in vision and language, and can be applied without further training to challenging downstream tasks, such as anticipating far-away action and ordering collections of images. An interesting future direction is transferring the model to agents so they can use it to conduct long-term planning. </p> 
           <p> <b>Acknowledgements</b> <br> <em>The core team includes Dave Epstein, Jiajun Wu, Cordelia Schmid, and Chen Sun. We thank Alexei Efros, Mia Chiquier, and Shiry Ginosar for their feedback, and Allan Jabri for inspiration in figure design. Dave would like to thank Ddac Surs and Carl Vondrick for insightful early discussions on cycling through time in video.</em> </p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Making Better Future Predictions by Watching Unlabeled Videos&amp;url=http://ai.googleblog.com/2021/11/making-better-future-predictions-by.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/11/making-better-future-predictions-by.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="blog-pager" id="blog-pager"> <a class="home-link" href="http://ai.googleblog.com/"> <i class="material-icons"> ? </i> </a> <i class="material-icons disabled"> ? </i> <span id="blog-pager-older-link"> <a class="blog-pager-older-link" href="http://ai.googleblog.com/search?updated-max=2021-11-11T12:26:00-08:00&amp;max-results=10" id="Blog1_blog-pager-older-link" title="Older Posts"> <i class="material-icons"> ? </i> </a> </span> 
       </div> 
       <div class="clear"></div> 
      </div>
     </div> 
    </div> 
   </div> 
   <div class="col-right"> 
    <div class="section" id="sidebar-top">
     <div class="widget HTML" data-version="1" id="HTML8"> 
      <div class="widget-content"> 
       <div class="searchBox"> 
        <input type="text" title="Search This Blog" placeholder="Search blog ..."> 
       </div> 
      </div> 
      <div class="clear"></div> 
     </div>
    </div> 
    <div id="aside"> 
     <div class="section" id="sidebar">
      <div class="widget Label" data-version="1" id="Label1"> 
       <div class="tab"> 
        <img class="sidebar-icon" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAYpJREFUeNrs2aFuwzAQBmAvKRkMKRjZA4QMDJaWFgyMjuzFRg37DIUlA3uFkoGQSaWzJU+tpri5O9+l/zSfdFJlpe59yTmyVedq1PjfcZMZ70NuQnaF8w8htyE/rABtpviXkLcK88c5HhLkMBfgVan43zfFBNGMjHVGT/s55KP2pAvidbGHd+nzKt1RKSLG3rKF1iPFv6UWiPke8i7kEqGdGsI1O+LYVdqJAjgirwkKYD0ytkJBUNbAMvX8V3q9PhUsYvU1sWD8SO/sQvx2ahxOiNoJCSBCoAHYCEQAC4EKICOQASQEOmAS8RcAFxFN5hiIiugpgC3wk9hQAHH/70EBHXUN7IER5EWMiBgo2+nzOKQv9SCAeEM/OQAkhE/ncccFICB87qzQMia5FsJfOui0zMnmRvipU1ormHQuxGTxUsAcCFLxJQBLBLn4UoAFglW8BkATwS5eC6CBEBWvCShBiIvXBkgQRcVbADiI4uKtABSESvGWgB9EzHt3+tNwyO0qa9SoIYtvAQYAqDJhaWWeMecAAAAASUVORK5CYII="> 
        <h2> Labels </h2> <i class="material-icons arrow"> ? </i> 
       </div> 
       <div class="widget-content list-label-widget-content"> 
        <ul> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/accessibility"> accessibility </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ACL"> ACL </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ACM"> ACM </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Acoustic%20Modeling"> Acoustic Modeling </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Adaptive%20Data%20Analysis"> Adaptive Data Analysis </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ads"> ads </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/adsense"> adsense </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/adwords"> adwords </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Africa"> Africa </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/AI"> AI </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/AI%20for%20Social%20Good"> AI for Social Good </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Algorithms"> Algorithms </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Android"> Android </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Android%20Wear"> Android Wear </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/API"> API </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/App%20Engine"> App Engine </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/App%20Inventor"> App Inventor </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/April%20Fools"> April Fools </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Art"> Art </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Audio"> Audio </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Augmented%20Reality"> Augmented Reality </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Australia"> Australia </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Automatic%20Speech%20Recognition"> Automatic Speech Recognition </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/AutoML"> AutoML </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Awards"> Awards </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/BigQuery"> BigQuery </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Cantonese"> Cantonese </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Chemistry"> Chemistry </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/China"> China </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Chrome"> Chrome </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Cloud%20Computing"> Cloud Computing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Collaboration"> Collaboration </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Compression"> Compression </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Computational%20Imaging"> Computational Imaging </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Computational%20Photography"> Computational Photography </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Computer%20Science"> Computer Science </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Computer%20Vision"> Computer Vision </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/conference"> conference </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/conferences"> conferences </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Conservation"> Conservation </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/correlate"> correlate </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Course%20Builder"> Course Builder </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/crowd-sourcing"> crowd-sourcing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/CVPR"> CVPR </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Data%20Center"> Data Center </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Data%20Discovery"> Data Discovery </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/data%20science"> data science </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/datasets"> datasets </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Deep%20Learning"> Deep Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/DeepDream"> DeepDream </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/DeepMind"> DeepMind </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/distributed%20systems"> distributed systems </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Diversity"> Diversity </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Earth%20Engine"> Earth Engine </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/economics"> economics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Education"> Education </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Electronic%20Commerce%20and%20Algorithms"> Electronic Commerce and Algorithms </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/electronics"> electronics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/EMEA"> EMEA </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/EMNLP"> EMNLP </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Encryption"> Encryption </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/entities"> entities </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Entity%20Salience"> Entity Salience </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Environment"> Environment </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Europe"> Europe </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Exacycle"> Exacycle </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Expander"> Expander </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Faculty%20Institute"> Faculty Institute </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Faculty%20Summit"> Faculty Summit </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Flu%20Trends"> Flu Trends </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Fusion%20Tables"> Fusion Tables </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/gamification"> gamification </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Gboard"> Gboard </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Gmail"> Gmail </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Accelerated%20Science"> Google Accelerated Science </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Books"> Google Books </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Brain"> Google Brain </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Cloud%20Platform"> Google Cloud Platform </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Docs"> Google Docs </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Drive"> Google Drive </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Genomics"> Google Genomics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Maps"> Google Maps </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Photos"> Google Photos </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Play%20Apps"> Google Play Apps </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Science%20Fair"> Google Science Fair </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Sheets"> Google Sheets </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Translate"> Google Translate </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Trips"> Google Trips </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Voice%20Search"> Google Voice Search </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%2B"> Google+ </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Government"> Government </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/grants"> grants </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Graph"> Graph </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Graph%20Mining"> Graph Mining </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Hardware"> Hardware </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/HCI"> HCI </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Health"> Health </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/High%20Dynamic%20Range%20Imaging"> High Dynamic Range Imaging </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ICCV"> ICCV </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ICLR"> ICLR </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ICML"> ICML </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ICSE"> ICSE </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Image%20Annotation"> Image Annotation </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Image%20Classification"> Image Classification </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Image%20Processing"> Image Processing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Inbox"> Inbox </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/India"> India </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Information%20Retrieval"> Information Retrieval </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/internationalization"> internationalization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Internet%20of%20Things"> Internet of Things </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Interspeech"> Interspeech </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/IPython"> IPython </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Journalism"> Journalism </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/jsm"> jsm </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/jsm2011"> jsm2011 </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/K-12"> K-12 </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Kaggle"> Kaggle </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/KDD"> KDD </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Keyboard%20Input"> Keyboard Input </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Klingon"> Klingon </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Korean"> Korean </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Labs"> Labs </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Linear%20Optimization"> Linear Optimization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/localization"> localization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Low-Light%20Photography"> Low-Light Photography </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Machine%20Hearing"> Machine Hearing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Machine%20Intelligence"> Machine Intelligence </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Machine%20Learning"> Machine Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Machine%20Perception"> Machine Perception </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Machine%20Translation"> Machine Translation </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Magenta"> Magenta </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/MapReduce"> MapReduce </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/market%20algorithms"> market algorithms </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Market%20Research"> Market Research </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/materials%20science"> materials science </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Mixed%20Reality"> Mixed Reality </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ML"> ML </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ML%20Fairness"> ML Fairness </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/MOOC"> MOOC </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Moore%27s%20Law"> Moore's Law </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Multimodal%20Learning"> Multimodal Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/NAACL"> NAACL </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Natural%20Language%20Processing"> Natural Language Processing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Natural%20Language%20Understanding"> Natural Language Understanding </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Network%20Management"> Network Management </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Networks"> Networks </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Neural%20Networks"> Neural Networks </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/NeurIPS"> NeurIPS </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Nexus"> Nexus </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Ngram"> Ngram </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/NIPS"> NIPS </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/NLP"> NLP </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/On-device%20Learning"> On-device Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/open%20source"> open source </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/operating%20systems"> operating systems </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Optical%20Character%20Recognition"> Optical Character Recognition </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/optimization"> optimization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/osdi"> osdi </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/osdi10"> osdi10 </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/patents"> patents </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Peer%20Review"> Peer Review </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ph.d.%20fellowship"> ph.d. fellowship </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/PhD%20Fellowship"> PhD Fellowship </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/PhotoScan"> PhotoScan </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Physics"> Physics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/PiLab"> PiLab </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Pixel"> Pixel </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Policy"> Policy </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Professional%20Development"> Professional Development </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Proposals"> Proposals </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Public%20Data%20Explorer"> Public Data Explorer </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/publication"> publication </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Publications"> Publications </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Quantum%20AI"> Quantum AI </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Quantum%20Computing"> Quantum Computing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Recommender%20Systems"> Recommender Systems </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Reinforcement%20Learning"> Reinforcement Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/renewable%20energy"> renewable energy </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Research"> Research </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Research%20Awards"> Research Awards </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/resource%20optimization"> resource optimization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Responsible%20AI"> Responsible AI </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Robotics"> Robotics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/schema.org"> schema.org </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Search"> Search </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/search%20ads"> search ads </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Security%20and%20Privacy"> Security and Privacy </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Self-Supervised%20Learning"> Self-Supervised Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Semantic%20Models"> Semantic Models </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Semi-supervised%20Learning"> Semi-supervised Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/SIGCOMM"> SIGCOMM </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/SIGMOD"> SIGMOD </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Site%20Reliability%20Engineering"> Site Reliability Engineering </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Social%20Networks"> Social Networks </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Software"> Software </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Sound%20Search"> Sound Search </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Speech"> Speech </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Speech%20Recognition"> Speech Recognition </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/statistics"> statistics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Structured%20Data"> Structured Data </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Style%20Transfer"> Style Transfer </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Supervised%20Learning"> Supervised Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Systems"> Systems </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/TensorBoard"> TensorBoard </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/TensorFlow"> TensorFlow </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/TPU"> TPU </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Translate"> Translate </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/trends"> trends </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/TTS"> TTS </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/TV"> TV </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/UI"> UI </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/University%20Relations"> University Relations </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/UNIX"> UNIX </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Unsupervised%20Learning"> Unsupervised Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/User%20Experience"> User Experience </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/video"> video </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Video%20Analysis"> Video Analysis </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Virtual%20Reality"> Virtual Reality </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Vision%20Research"> Vision Research </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Visiting%20Faculty"> Visiting Faculty </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Visualization"> Visualization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/VLDB"> VLDB </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Voice%20Search"> Voice Search </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Wiki"> Wiki </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/wikipedia"> wikipedia </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/WWW"> WWW </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Year%20in%20Review"> Year in Review </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/YouTube"> YouTube </a> </li> 
        </ul> 
        <div class="clear"></div> 
       </div> 
      </div>
      <div class="widget BlogArchive" data-version="1" id="BlogArchive1"> 
       <div class="tab"> <i class="material-icons icon"> ? </i> 
        <h2> Archive </h2> <i class="material-icons arrow"> ? </i> 
       </div> 
       <div class="widget-content"> 
        <div id="ArchiveList"> 
         <div id="BlogArchive1_ArchiveList"> 
          <ul class="hierarchy"> 
           <li class="archivedate expanded"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy toggle-open"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2021/"> 2021 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate expanded"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2020/"> 2020 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2019/"> 2019 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2018/"> 2018 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2017/"> 2017 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2016/"> 2016 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2015/"> 2015 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2014/"> 2014 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2013/"> 2013 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2012/"> 2012 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2011/"> 2011 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2010/"> 2010 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2009/"> 2009 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2008/"> 2008 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2007/"> 2007 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2006/"> 2006 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
         </div> 
        </div> 
        <div class="clear"></div> 
       </div> 
      </div>
      <div class="widget HTML" data-version="1" id="HTML6"> 
       <div class="widget-content"> <a href="http://googleaiblog.blogspot.com/atom.xml"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAihJREFUeNrsWa9Pw0AU7viRMDFRBAkzJDMIBIhJJhCzk7NILIqMv4AEhdz+BCY3OYssAlGBoAJREpZwAlHEBO8lr8nSvNeVbu1dyX3JlzTrXfa+u/e9d7c5joWFhYVO1Fa8PwH2gK6m+BRwAvSlAdsrgr8E1jUuMH73GTAEzrkBWymTewZlihhLmgDXIAFuHgGVQOUF7OSYM1p6PgTuA1vAZlUEvAnPdapcMY0VICECekQ0XRfYrqoHsAGNgXfAoMomRiFDEhOZkkL3S88hMaB2LwXp0bj+ps2edpToZpjfoIDQtBeU+xjoDzP2G/gCPKZ5f8WsCAFJoJgOCcFdWSTeL9YQMSvTA1h9BkI5jaiXhLpSCL/8mVZY0UpyJ9ZdOkniu1dmJ96BpzQu9w6s28gcOq9j6pwLdR8/36NK5CQKwJSMrb2MhhSglBpt4UjsrdsnNu0B3J0HCozbCc4TjyY2srEgos/4RQljCzNxl4ireQD8FOq+T+W0mTB2g7njhlR+Sy2jsXFvU658U8YTbeaGpdIu7mWkEAq5ZtIjIhFZdtfX7QHckSvB2B6zC3VdAkZk0kAQwaXTk/CzTXK3wjIExCs6ZJpTnE4uY1KV+KzFzA3KTiFPENHJkOPcsfpLhwe4btoSuvUqAR+6TOxlCE6ZfKUsJLgsqGW8OpqAGx2X+sLxrwUog+JUeQRMDBIwyXOcnlPtPnL0/UsT/8LnOxYWFhZG4leAAQAAQHEaYuzHbAAAAABJRU5ErkJggg==" class="sidebar-icon"> <h2>Feed</h2> </a> 
       </div> 
       <div class="clear"></div> 
      </div>
     </div> 
     <div class="section" id="sidebar-bottom">
      <div class="widget HTML" data-version="1" id="HTML5"> 
       <div class="widget-content"> 
        <div class="share followgooglewrapper"> <button data-href="https://twitter.com/intent/follow?original_referer=http://ai.googleblog.com/&amp;screen_name=googleai" onclick="sharingPopup(this);" id="twitter-share"><span class="twitter-follow">Follow @googleai</span></button> 
         <script>
      function sharingPopup (button) {
      var url = button.getAttribute("data-href");
	    window.open(
 url,'popUpWindow','height=500,width=500,left=10,top=10,resizable=yes,scrollbars=yes,toolbar=yes,menubar=no,location=no,directories=no,status=yes');
          }
    </script> 
        </div> 
       </div> 
       <div class="clear"></div> 
      </div>
      <div class="widget HTML" data-version="1" id="HTML1"> 
       <div class="widget-content">
         Give us feedback in our <a href="http://support.google.com/bin/static.py?hl=en&amp;page=portal_groups.cs">Product Forums</a>. 
       </div> 
       <div class="clear"></div> 
      </div>
     </div> 
    </div> 
   </div> 
   <div style="clear:both;"></div> 
  </div> <!-- Footer --> 
  <div class="google-footer-outer loading"> 
   <div id="google-footer"> <a href="//www.google.com/"> <img class="google-logo-dark" height="36" src="data:image/png;base64,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" style="margin-top: -16px;" width="92"> </a> 
    <ul> 
     <li> <a href="//www.google.com/"> Google </a> </li> 
     <li> <a href="//www.google.com/policies/privacy/"> Privacy </a> </li> 
     <li> <a href="//www.google.com/policies/terms/"> Terms </a> </li> 
    </ul> 
   </div> 
  </div> 
  <script type="text/javascript">
      //<![CDATA[
      // Social sharing popups.
      var postEl = document.getElementsByClassName('social-wrapper');
      var postCount = postEl.length;
      for(i=0; i<postCount;i++){
        postEl[i].addEventListener("click", function(event){
          var postUrl = this.getAttribute("data-href");
          window.open(
            postUrl,'popUpWindow','height=500,width=500,left=10,top=10,resizable=yes,scrollbars=yes,toolbar=yes,menubar=no,location=no,directories=no,status=yes');
        });}
      //]]>
    </script> 
  <script type="text/javascript">
      //<![CDATA[
      var BreakpointHandler = function() {
        this.initted = false;
        this.isHomePage = false;
        this.isMobile = false;
      };
      BreakpointHandler.prototype.finalizeSummary = function(summaryHtml, lastNode) {
        // Use $.trim for IE8 compatibility
        summaryHtml = $.trim(summaryHtml).replace(/(<br>|\s)+$/,'');
        if (lastNode.nodeType == 3) {
          var lastChar = summaryHtml.slice(-1);
          if (!lastChar.match(/[."?]/)) {
            if (!lastChar.match(/[A-Za-z]/)) {
              summaryHtml = summaryHtml.slice(0, -1);
            }
            summaryHtml += ' ...';
          }
        } else if (lastNode.nodeType == 1 && (lastNode.nodeName == 'I' || lastNode.nodeName == 'A')) {
          summaryHtml += ' ...';
        }
        return summaryHtml;
      };
      BreakpointHandler.prototype.generateSummaryFromContent = function(content, numWords) {
        var seenWords = 0;
        var summaryHtml = '';
        for (var i=0; i < content.childNodes.length; i++) {
          var node = content.childNodes[i];
          var nodeText;
          if (node.nodeType == 1) {
            if (node.hasAttribute('data-about-pullquote')) {
              continue;
            }
            nodeText = node.textContent;
            if (nodeText === undefined) {
              // innerText for IE8
              nodeText = node.innerText;
            }
            if (node.nodeName == 'DIV' || node.nodeName == 'B') {
              // Don't end early if we haven't seen enough words.
              if (seenWords < 10) {
                continue;
              }
              if (i > 0) {
                summaryHtml = this.finalizeSummary(summaryHtml, content.childNodes[i-1]);
              }
              break;
            }
            summaryHtml += node.outerHTML;
          } else if (node.nodeType == 3) {
            nodeText = node.nodeValue;
            summaryHtml += nodeText + ' ';
          }
          var words = nodeText.match(/\S+\s*/g);
          if (!words) {
            continue;
          }
          var remain = numWords - seenWords;
          if (words.length >= remain) {
            summaryHtml = this.finalizeSummary(summaryHtml, node);
            break;
          }
          seenWords += words.length;
        }
        return summaryHtml;
      };
      BreakpointHandler.prototype.detect = function() {
        var match,
            pl     = /\+/g,
            search = /([^&=]+)=?([^&]*)/g,
            decode = function (s) { return decodeURIComponent(s.replace(pl, " ")); },
            query  = window.location.search.substring(1);
        var urlParams = {};
        while (match = search.exec(query))
          urlParams[decode(match[1])] = decode(match[2]);
        this.isListPage = $('html').hasClass('list-page');
        this.isMobile = urlParams['m'] === '1';
        this.isHomePage = window.location.pathname == '/';
      };
      BreakpointHandler.prototype.initContent = function() {
        var self = this;
        $('.post').each(function(index) {
          var body = $(this).children('.post-body')[0];
          var content = $(body).children('.post-content')[0];
          $(content).addClass('post-original');
          var data = $(content).children('script').html();
          data = self.rewriteForSSL(data);

          if (document.body.className.indexOf('is-preview') !== -1) {
            // If exists, extract specified editor's preview.
            var match = data.match(/([\s\S]+?)<div data-is-preview.+?>([\s\S]+)<\/div>/m);
            if (match) {
              data = match[1];
            }
          }

          // Prevent big images from loading when they aren't needed.
          // This must be done as a pre-injection step, since image loading can't be
          // canceled once embedded into the DOM.
          if (self.isListPage && self.isMobile) {
            data = data.replace(/<(img|iframe) .+?>/g, '');
          }
          // Insert template to be rendered as nodes.
          content.innerHTML = data;
          if (self.isListPage) {
            var summary = document.createElement('div');
            $(summary).addClass('post-content');
            $(summary).addClass('post-summary');
            body.insertBefore(summary, content);
            if (match) {
              // Use provided summary.
              summary.innerHTML = match[2];
            } else {
              // Generate a summary.
              // Summary generation relies on DOM, so it must occur after content is
              // inserted into the page.
              summary.innerHTML = self.generateSummaryFromContent(content, 30);
            }
            // Add read more link to summary.
            var titleAnchor = $(this).find('.title a')[0];
            var link = titleAnchor.cloneNode(true);
            link.innerHTML = 'Read More';
            $(link).addClass('read-more');
            summary.appendChild(link);
          }
        });
        // Firefox does not allow for proper styling of BR.
        if (navigator.userAgent.indexOf('Firefox') > -1) {
          $('.post-content br').replaceWith('<span class="space"></span>');
        }
        $('.loading').removeClass('loading');
      };
      BreakpointHandler.prototype.process = function() {
        if (!this.initted) {
          var makeInsecureImageRegex = function(hosts) {
            var whitelist = hosts.join('|').replace(/\./g,'\\.');
            // Normal image tags, plus input images (yes, this is possible!)
            return new RegExp('(<(img|input)[^>]+?src=("|\'))http:\/\/(' + whitelist +')', 'g');
          };
          this.sslImageRegex = makeInsecureImageRegex(BreakpointHandler.KNOWN_HTTPS_HOSTS);
          this.sslImageCurrentDomainRegex = makeInsecureImageRegex([window.location.hostname]);
          this.detect();
          this.initContent();
          this.initted = true;
        }
      };
      BreakpointHandler.KNOWN_HTTPS_HOSTS = [
        "www.google.org",
        "www.google.com",
        "services.google.com",
        "blogger.com",
        "draft.blogger.com",
        "www.blogger.com",
        "photos1.blogger.com",
        "photos2.blogger.com",
        "photos3.blogger.com",
        "blogblog.com",
        "img1.blogblog.com",
        "img2.blogblog.com",
        "www.blogblog.com",
        "www1.blogblog.com",
        "www2.blogblog.com",
        "0.bp.blogspot.com",
        "1.bp.blogspot.com",
        "2.bp.blogspot.com",
        "3.bp.blogspot.com",
        "4.bp.blogspot.com",
        "lh3.googleusercontent.com",
        "lh4.googleusercontent.com",
        "lh5.googleusercontent.com",
        "lh6.googleusercontent.com",
        "themes.googleusercontent.com",
      ];
        BreakpointHandler.prototype.rewriteForSSL = function(html) {
        // Handle HTTP -> HTTPS source replacement of images, movies, and other embedded content.
        return html.replace(this.sslImageRegex, '$1https://$4')
        .replace(this.sslImageCurrentDomainRegex, '$1//$4')
        .replace(/(<(embed|iframe)[^>]+?src=("|'))http:\/\/([^"']*?(youtube|picasaweb\.google)\.com)/g, '$1https://$4')
        // Slideshow SWF takes a image host, so we need to rewrite that parameter.
        .replace(/(<embed[^>]+?feed=http(?=[^s]))/g, '$1s');
        };
        $(document).ready(function() {
        var handler = new BreakpointHandler();
        handler.process();
        // Top-level navigation.
        $(".BlogArchive .tab").click(function(ev) {
        ev.preventDefault();
        $(this).parent().toggleClass('active');
        $(this).siblings().slideToggle(300);
        });
        $(".Label .tab").click(function(ev) {
        ev.preventDefault();
        $(this).parent().toggleClass('active');
        $(this).siblings().slideToggle(300);
        });
        // Blog archive year expansion.
        $('.BlogArchive .intervalToggle').click(function(ev) {
        ev.preventDefault();
        if ($(this).parent().hasClass('collapsed')) {
        $(this).parent().removeClass('collapsed');
        $(this).parent().addClass('expanded');
        } else {
        $(this).parent().removeClass('expanded');
        $(this).parent().addClass('collapsed');
        }
        });
        // Reverse order of months.
        $('.BlogArchive .intervalToggle + div').each(function(_, items) {
        var year = $(this);
        year.children().each(function(_, month) {
        year.prepend(month);
        });
        });
        // Set anchors to open in new tab.
        $('.post-content img').parent().each(function(_, node) {
        if (node.nodeName == 'A') {
        $(this).attr('target', '_blank');
        }
        });
        // Process search requests.
        $('.searchBox input').on("keypress", function(ev) {
        if (ev.which == 13) {
        window.location.href = 'https://www.google.com/search?q=site%3A' + window.location.hostname + '%20' + encodeURIComponent ($(this).val());
        }
        });
        });
        //]]>
    </script> 
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/prettify.min.css" rel="stylesheet" type="text/css"> 
  <script language="javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/prettify.min.js" type="text/javascript"></script> 
  <script language="javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/lang-css.min.js" type="text/javascript"></script> 
  <script type="text/javascript">
        document.addEventListener('DOMContentLoaded',function() {
            prettyPrint();
        });
    </script> 
  <script type="text/javascript" src="https://www.blogger.com/static/v1/widgets/1896816315-widgets.js"></script> 
  <script type="text/javascript">
window['__wavt'] = 'AOuZoY7pW-nLhG4msH1O2UOJg3obvkOocg:1637267593484';_WidgetManager._Init('//www.blogger.com/rearrange?blogID\x3d8474926331452026626','//ai.googleblog.com/','8474926331452026626');
_WidgetManager._SetDataContext([{'name': 'blog', 'data': {'blogId': '8474926331452026626', 'title': 'Google AI Blog', 'url': 'http://ai.googleblog.com/', 'canonicalUrl': 'http://ai.googleblog.com/', 'homepageUrl': 'http://ai.googleblog.com/', 'searchUrl': 'http://ai.googleblog.com/search', 'canonicalHomepageUrl': 'http://ai.googleblog.com/', 'blogspotFaviconUrl': 'http://ai.googleblog.com/favicon.ico', 'bloggerUrl': 'https://www.blogger.com', 'hasCustomDomain': true, 'httpsEnabled': false, 'enabledCommentProfileImages': true, 'gPlusViewType': 'FILTERED_POSTMOD', 'adultContent': false, 'analyticsAccountNumber': 'UA-961555-69', 'encoding': 'UTF-8', 'locale': 'en', 'localeUnderscoreDelimited': 'en', 'languageDirection': 'ltr', 'isPrivate': false, 'isMobile': false, 'isMobileRequest': false, 'mobileClass': '', 'isPrivateBlog': false, 'isDynamicViewsAvailable': true, 'feedLinks': '\x3clink rel\x3d\x22alternate\x22 type\x3d\x22application/atom+xml\x22 title\x3d\x22Google AI Blog - Atom\x22 href\x3d\x22http://ai.googleblog.com/feeds/posts/default\x22 /\x3e\n\x3clink rel\x3d\x22alternate\x22 type\x3d\x22application/rss+xml\x22 title\x3d\x22Google AI Blog - RSS\x22 href\x3d\x22http://ai.googleblog.com/feeds/posts/default?alt\x3drss\x22 /\x3e\n\x3clink rel\x3d\x22service.post\x22 type\x3d\x22application/atom+xml\x22 title\x3d\x22Google AI Blog - Atom\x22 href\x3d\x22https://www.blogger.com/feeds/8474926331452026626/posts/default\x22 /\x3e\n', 'meTag': '', 'adsenseHostId': 'ca-host-pub-1556223355139109', 'adsenseHasAds': false, 'adsenseAutoAds': false, 'ieCssRetrofitLinks': '\x3c!--[if IE]\x3e\x3cscript type\x3d\x22text/javascript\x22 src\x3d\x22https://www.blogger.com/static/v1/jsbin/403901366-ieretrofit.js\x22\x3e\x3c/script\x3e\n\x3c![endif]--\x3e', 'view': '', 'dynamicViewsCommentsSrc': '//www.blogblog.com/dynamicviews/4224c15c4e7c9321/js/comments.js', 'dynamicViewsScriptSrc': '//www.blogblog.com/dynamicviews/cdbdb1903ee0053e', 'plusOneApiSrc': 'https://apis.google.com/js/plusone.js', 'disableGComments': true, 'sharing': {'platforms': [{'name': 'Get link', 'key': 'link', 'shareMessage': 'Get link', 'target': ''}, {'name': 'Facebook', 'key': 'facebook', 'shareMessage': 'Share to Facebook', 'target': 'facebook'}, {'name': 'BlogThis!', 'key': 'blogThis', 'shareMessage': 'BlogThis!', 'target': 'blog'}, {'name': 'Twitter', 'key': 'twitter', 'shareMessage': 'Share to Twitter', 'target': 'twitter'}, {'name': 'Pinterest', 'key': 'pinterest', 'shareMessage': 'Share to Pinterest', 'target': 'pinterest'}, {'name': 'Email', 'key': 'email', 'shareMessage': 'Email', 'target': 'email'}], 'disableGooglePlus': true, 'googlePlusShareButtonWidth': 0, 'googlePlusBootstrap': '\x3cscript type\x3d\x22text/javascript\x22\x3ewindow.___gcfg \x3d {\x27lang\x27: \x27en\x27};\x3c/script\x3e'}, 'hasCustomJumpLinkMessage': false, 'jumpLinkMessage': 'Read more', 'pageType': 'index', 'pageName': '', 'pageTitle': 'Google AI Blog'}}, {'name': 'features', 'data': {'sharing_get_link_dialog': 'true', 'sharing_native': 'false'}}, {'name': 'messages', 'data': {'edit': 'Edit', 'linkCopiedToClipboard': 'Link copied to clipboard!', 'ok': 'Ok', 'postLink': 'Post Link'}}, {'name': 'template', 'data': {'name': 'custom', 'localizedName': 'Custom', 'isResponsive': false, 'isAlternateRendering': false, 'isCustom': true}}, {'name': 'view', 'data': {'classic': {'name': 'classic', 'url': '?view\x3dclassic'}, 'flipcard': {'name': 'flipcard', 'url': '?view\x3dflipcard'}, 'magazine': {'name': 'magazine', 'url': '?view\x3dmagazine'}, 'mosaic': {'name': 'mosaic', 'url': '?view\x3dmosaic'}, 'sidebar': {'name': 'sidebar', 'url': '?view\x3dsidebar'}, 'snapshot': {'name': 'snapshot', 'url': '?view\x3dsnapshot'}, 'timeslide': {'name': 'timeslide', 'url': '?view\x3dtimeslide'}, 'isMobile': false, 'title': 'Google AI Blog', 'description': 'The latest from Google Research', 'url': 'http://ai.googleblog.com/', 'type': 'feed', 'isSingleItem': false, 'isMultipleItems': true, 'isError': false, 'isPage': false, 'isPost': false, 'isHomepage': true, 'isArchive': false, 'isLabelSearch': false}}]);
_WidgetManager._RegisterWidget('_HeaderView', new _WidgetInfo('Header1', 'header', document.getElementById('Header1'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_BlogView', new _WidgetInfo('Blog1', 'main', document.getElementById('Blog1'), {'cmtInteractionsEnabled': false}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_HTMLView', new _WidgetInfo('HTML8', 'sidebar-top', document.getElementById('HTML8'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_LabelView', new _WidgetInfo('Label1', 'sidebar', document.getElementById('Label1'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_BlogArchiveView', new _WidgetInfo('BlogArchive1', 'sidebar', document.getElementById('BlogArchive1'), {'languageDirection': 'ltr', 'loadingMessage': 'Loading\x26hellip;'}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_HTMLView', new _WidgetInfo('HTML6', 'sidebar', document.getElementById('HTML6'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_HTMLView', new _WidgetInfo('HTML5', 'sidebar-bottom', document.getElementById('HTML5'), {}, 'displayModeFull'));
_WidgetManager._RegisterWidget('_HTMLView', new _WidgetInfo('HTML1', 'sidebar-bottom', document.getElementById('HTML1'), {}, 'displayModeFull'));
</script>  
 </body>
</html>